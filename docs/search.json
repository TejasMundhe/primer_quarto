[
  {
    "path": "index.html",
    "id": "welcome",
    "chapter": "Welcome",
    "heading": "Welcome",
    "text": " isn’t book ’re looking .First, book students classes. Everything book designed make experience students better. hope material may useful people outside class.Second, book changes time. --date possible.Third, highly opinionated matters . might share views.",
    "code": ""
  },
  {
    "path": "preamble.html",
    "id": "preamble",
    "chapter": "Preamble",
    "heading": "Preamble",
    "text": "",
    "code": ""
  },
  {
    "path": "preamble.html",
    "id": "dedication",
    "chapter": "Preamble",
    "heading": "Dedication",
    "text": "romantic, Kay —\nlove?\nNeed ask anyone tell us things?",
    "code": ""
  },
  {
    "path": "preamble.html",
    "id": "acknowledgements",
    "chapter": "Preamble",
    "heading": "Acknowledgements",
    "text": "work builds contributions many people R Open Source communities. particular, like acknowledge extensive material taken Diez, Barr, Çetinkaya-Rundel (2014), Grolemund Wickham (2017), Irizarry (2019), Kim Ismay (2019), Jenny Bryan (2019), Diez, Barr, Çetinkaya-Rundel (2014), Downey (2012), Grolemund Wickham (2017), Kuhn Silge (2020), Timbers, Campbell, Lee (2021), Legler Roback (2019).Alboukadel Kassambara, Andrew Tran, Thomas Mock others kindly allowed re-use /modification work.Thanks contributions Harvard students, colleagues random people met internet: Albert Rivero, Nicholas Dow, Celine Vendler, Sophia Zheng, Maria Burzillo, Robert McKenzie, Deborah Gonzalez, Beau Meche, Evelyn Cai, Miro Bergam, Jessica Edwards, Emma Freeman, Cassidy Bargell, Yao Yu, Vivian Zhang, Ishan Bhatt, Mak Famulari, Tahmid Ahmed, Eliot Min, Hannah Valencia, Asmer Safi, Erin Guetzloe, Shea Jenkins, Thomas Weiss, Diego Martinez, Andy Wang, Tyler Simko, Jake Berg, Connor Rust, Liam Rust, Alla Baranovsky, Carine Hajjar, Diego Arias, Stephanie Yao Tyler Simko.Also, Becca Gill, Ajay Malik, Heather Li, Nosa Lawani, Stephanie Saab, Nuo Wen Lei, Anmay Gupta Dario Anaya.Also, Kevin Xu, Anmay Gupta, Sophia Zhu, Arghayan Jeiyasarangkan, Yuhan Wu, Ryan Southward, George Pentchev, Ahmet Atilla Colak, Mahima Malhotra, Shreeram Patkar.like gratefully acknowledge funding Derek Bok Center Teaching Learning Harvard University, via Digital Teaching Fellows Learning Lab Undergraduate Fellows programs.David Kane(former) Preceptor Statistical Methods MathematicsDepartment GovernmentHarvard University",
    "code": ""
  },
  {
    "path": "preamble.html",
    "id": "license",
    "chapter": "Preamble",
    "heading": "License",
    "text": "work licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.",
    "code": ""
  },
  {
    "path": "getting-started.html",
    "id": "getting-started",
    "chapter": "Getting Started",
    "heading": "Getting Started",
    "text": "world confronts us. Make decisions must.",
    "code": ""
  },
  {
    "path": "getting-started.html",
    "id": "installing-r-and-rstudio",
    "chapter": "Getting Started",
    "heading": "Installing R and RStudio",
    "text": "\nFIGURE 0.1: Analogy difference R RStudio.\nuse R via RStudio. R RStudio car’s engine dashboard.precisely, R programming language runs computations, RStudio integrated development environment (IDE) provides interface many convenient features. Just access speedometer, rearview mirrors, navigation system makes driving much easier, using RStudio’s interface makes using R much easier.Download install R RStudio (Desktop version) computer.Download install R.Download install R.Download install RStudio Desktop (free version).Download install RStudio Desktop (free version).using Windows, download install R Tools. , Windows users. Rtools42 installer want. link fifth paragraph page.want slower introduction, check short (free) book, Getting Used R, RStudio, R Markdown. Ismay Kennedy (2016) include screencast recordings can follow along pause learn.",
    "code": ""
  },
  {
    "path": "getting-started.html",
    "id": "using-r-via-rstudio",
    "chapter": "Getting Started",
    "heading": "Using R via RStudio",
    "text": "\nFIGURE 0.2: Icons R versus RStudio computer.\nMuch don’t drive car interacting directly engine rather interacting elements car’s dashboard, won’t using R directly. Instead, use RStudio’s interface. install R RStudio computer, ’ll two new programs (also called applications) can open. Always work RStudio directly R application.Open RStudio. see three panes, panels, dividing screen: Console pane, Environment pane, Files pane.workspace. Start big pane left:three panels (tabs) window, ’ll focusing Console Terminal. first start R, Console gives information version R. Console can type run R code. example, type 1 + 1 hit return, Console returns 2.Look top right:main two tabs ’ll using Environment Git (yet visible). Environment tab shows datasets variables currently loaded R. case, loaded dataset 3407 rows 5 columns variable x equal 5. , Environment empty. Let’s change . Go Console type:assigned value 5 object, x. <- operator used assign values objects R. Now, hit return/enter see variable x equal 5 Environment tab. must always hit return/enter typing command, otherwise RStudio realize want R execute command.Look bottom right window:Files tab displays computer’s file system. create project later, tab automatically show contents project’s folder. Plots tab display plots make RStudio.",
    "code": "\nx <- 5"
  },
  {
    "path": "getting-started.html",
    "id": "package-installation",
    "chapter": "Getting Started",
    "heading": "Package installation",
    "text": "R packages, also known libraries, extend power R providing additional functions data.\nFIGURE 0.3: Analogy R versus R packages.\nR like new mobile phone: certain amount features use first time, doesn’t everything. R packages like apps can download onto phone.Consider analogy Instagram. new phone want share photo friends. need :Install app: Since phone new include Instagram app, need download app. . (might need future update app.)Open app: ’ve installed Instagram, need open . need every time use app.process similar R package. need :\nFIGURE 0.4: Installing versus loading R package\nInstall package: like installing app phone. packages installed default install R RStudio. Thus want use package first time, need install . ’ve installed package, likely won’t install unless want update newer version.“Load” package: “Loading” package like opening app phone. Packages “loaded” default start RStudio. need “load” package want use every time restart RStudio.installing packages, issue command:just makes various errors less likely.Install package need Primer. Console pane within RStudio, type:press return/enter keyboard. Note must include quotation marks around name package. package can depend packages, automatically installed needed.One tricky aspect process R occasionally ask :Unless good reason , always answer “” question.R packages generally live one two places:CRAN (rhymes “clan”) mature, popular packages. Use install.packages(), .CRAN (rhymes “clan”) mature, popular packages. Use install.packages(), .Github experimental, less stable packages. Use remotes::install_github(). end section, install one package Github.Github experimental, less stable packages. Use remotes::install_github(). end section, install one package Github.",
    "code": "\noptions(pkgType = \"binary\")\ninstall.packages(\"remotes\")Do you want to install from sources the packages which \nneed compilation? (Yes/no/cancel)"
  },
  {
    "path": "getting-started.html",
    "id": "package-loading",
    "chapter": "Getting Started",
    "heading": "Package loading",
    "text": "Recall ’ve installed package, need “load” . using library() command.example, load remotes package, run following code Console. mean “run following code”? Type copy--paste code Console hit enter/return key.running code, blinking cursor appear next > symbol. (> “prompt.”) means successful remotes package now loaded ready use. , however, might get red “error message” reads:error message means haven’t successfully installed package. get error message, make sure install remotes package proceeding.historical reasons packages also known libraries, relevant command loading library().Note R occasionally ask want install packages. almost always want , otherwise R asking .",
    "code": "\nlibrary(remotes)Error in library(remotes) : there is no package called ‘remotes’"
  },
  {
    "path": "getting-started.html",
    "id": "package-use",
    "chapter": "Getting Started",
    "heading": "Package use",
    "text": "load package want use every time start RStudio. don’t load package attempting use one features, see error message like:different error message one just saw package installed yet. R telling trying use function package yet loaded. R doesn’t know “find” function want use.Let’s install package available CRAN: primer.tutorials. Copy paste following R Console:Depending computer/browser/locale, might fail, especially quotation marks paste turn “curly.” case, type commands scratch.Many new packages installed, including primer.data, provides data sets use Primer. may take minutes. something gets messed , often useful use read error messages see specific package fault. , use remove.packages() command remove problematic package install .",
    "code": "Error: could not find function\nlibrary(remotes)\ninstall_github(\"PPBDS/primer.tutorials\")"
  },
  {
    "path": "getting-started.html",
    "id": "tutorials",
    "chapter": "Getting Started",
    "heading": "Tutorials",
    "text": "chapter textbook, one tutorials available primer.tutorials package. order access tutorials, run library(primer.tutorials) R Console.can access tutorials via Tutorial tab top right pane RStudio. don’t see tutorials, try clicking “Home” button – little house symbol thin red roof upper right. may need restart R session. Click “Session” menu select “Restart R”.order expand window, can drag enlarge tutorial pane inside RStudio. order open pop-window, click “Show New Window” icon next home icon.may notice Jobs tab lower left pane create output tutorial starting . RStudio running code create tutorial.work automatically saved RStudio sessions. can complete tutorial multiple sittings.Now ? ways can close tutorial safely can quit RStudio session.clicked “Show new window” working tutorial pop-window, simply X pop-window.clicked “Show new window” working tutorial pop-window, simply X pop-window.working tutorial inside Tutorial pane RStudio, simply press red stop sign icon.working tutorial inside Tutorial pane RStudio, simply press red stop sign icon.",
    "code": ""
  },
  {
    "path": "getting-started.html",
    "id": "basic-commands",
    "chapter": "Getting Started",
    "heading": "Basic Commands",
    "text": "terms, tips, tricks know getting started R.Functions: perform tasks taking input called argument returning output.sqrt() function gives us square root argument. 64 argument. Therefore, output 8. Try Console!Help files: provide documentation packages, functions datasets. can bring help files adding ? name object Console. Run ?sqrt.Help files: provide documentation packages, functions datasets. can bring help files adding ? name object Console. Run ?sqrt.Errors, warnings, messages: important communications R . error, code run. Read (/Google) message try fix . Warnings don’t prevent code completing. example, create scatterplot based data two missing values, see warning:Errors, warnings, messages: important communications R . error, code run. Read (/Google) message try fix . Warnings don’t prevent code completing. example, create scatterplot based data two missing values, see warning:Messages similar. cases, fix underlying issue warning/message goes away.",
    "code": "\nsqrt(64)## [1] 8Warning message:\nRemoved 2 rows containing missing values (geom_point).  "
  },
  {
    "path": "getting-started.html",
    "id": "summary",
    "chapter": "Getting Started",
    "heading": "Summary",
    "text": "done following:Installed latest versions R RStudio.Installed latest versions R RStudio.Installed, CRAN, remotes package:Installed, CRAN, remotes package:Installed, Github, primer.tutorials package:Learned basic terminology help read rest Primer.Let’s get started.",
    "code": "\ninstall.packages(\"remotes\")\nremotes::install_github(\"PPBDS/primer.tutorials\")"
  },
  {
    "path": "visualization.html",
    "id": "visualization",
    "chapter": "1 Visualization",
    "heading": "1 Visualization",
    "text": "Everyone loves visualizations.read chapter, completed associated tutorials, able create graphics like one data. Join us journey.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "looking-at-data",
    "chapter": "1 Visualization",
    "heading": "1.1 Looking at data",
    "text": "chapter focuses ggplot2, one core packages tidyverse. tidyverse package contains 8 individual packages, importantly ggplot2, also packages help us view data within R. access datasets, help pages, functions use chapter, load tidyverse:one line code loads packages associated tidyverse, packages use almost every data analysis. first time load tidyverse, R report functions tidyverse conflict functions base R packages may loaded. (hide messages book ugly.)might get error message:happens, need install package:, run library(tidyverse) .",
    "code": "\nlibrary(tidyverse)Error in library(tidyverse) : there is no package called ‘tidyverse’\ninstall.packages(\"tidyverse\")"
  },
  {
    "path": "visualization.html",
    "id": "examining-trains",
    "chapter": "1 Visualization",
    "heading": "1.1.1 Examining trains",
    "text": "data comes us “spreadsheet”-type format. datasets called data frames tibbles R. Let’s explore trains tibble primer.data package. data comes Enos (2014), investigated attitudes toward immigration among Boston commuters.Let’s unpack output:tibble specific kind data frame. particular data frame 115 rows corresponding different units, meaning people case.tibble specific kind data frame. particular data frame 115 rows corresponding different units, meaning people case.tibble also 14 columns corresponding variables describe unit observation.tibble also 14 columns corresponding variables describe unit observation.see, default, top 10 rows columns. can see (fewer) rows columns using print() command:see, default, top 10 rows columns. can see (fewer) rows columns using print() command:n argument print() tells R number rows want see. width refers number characters print across screen. Want see every row every column? Try:Inf R object means infinity.",
    "code": "\nlibrary(primer.data)\ntrains## # A tibble: 115 × 14\n##    treatment att_start att_end gender race  liberal party       age income line \n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>     <int>  <dbl> <chr>\n##  1 Treated          11      11 Female White FALSE   Democrat     31 135000 Fram…\n##  2 Treated           9      10 Female White FALSE   Republic…    34 105000 Fram…\n##  3 Treated           3       5 Male   White TRUE    Democrat     63 135000 Fram…\n##  4 Treated          11      11 Male   White FALSE   Democrat     45 300000 Fram…\n##  5 Control           8       5 Male   White TRUE    Democrat     55 135000 Fram…\n##  6 Treated          13      13 Female White FALSE   Democrat     37  87500 Fram…\n##  7 Control          13      13 Female White FALSE   Republic…    53  87500 Fram…\n##  8 Treated          10      11 Male   White FALSE   Democrat     36 135000 Fram…\n##  9 Control          12      12 Female White FALSE   Democrat     54 105000 Fram…\n## 10 Treated           9      10 Male   White FALSE   Republic…    42 135000 Fram…\n## # … with 105 more rows, and 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\nprint(trains, n = 15, width = 100)## # A tibble: 115 × 14\n##    treatment att_start att_end gender race  liberal party        age income\n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>      <int>  <dbl>\n##  1 Treated          11      11 Female White FALSE   Democrat      31 135000\n##  2 Treated           9      10 Female White FALSE   Republican    34 105000\n##  3 Treated           3       5 Male   White TRUE    Democrat      63 135000\n##  4 Treated          11      11 Male   White FALSE   Democrat      45 300000\n##  5 Control           8       5 Male   White TRUE    Democrat      55 135000\n##  6 Treated          13      13 Female White FALSE   Democrat      37  87500\n##  7 Control          13      13 Female White FALSE   Republican    53  87500\n##  8 Treated          10      11 Male   White FALSE   Democrat      36 135000\n##  9 Control          12      12 Female White FALSE   Democrat      54 105000\n## 10 Treated           9      10 Male   White FALSE   Republican    42 135000\n## 11 Control          10       9 Female White FALSE   Democrat      33 105000\n## 12 Treated          11       9 Male   White FALSE   Democrat      50 250000\n## 13 Treated          13      13 Male   White FALSE   Republican    24 105000\n## 14 Control           6       7 Male   White TRUE    Democrat      40  62500\n## 15 Control           8       8 Male   White TRUE    Democrat      53 300000\n##    line       station      hisp_perc ideology_start ideology_end\n##    <chr>      <chr>            <dbl>          <int>        <int>\n##  1 Framingham Grafton         0.0264              3            3\n##  2 Framingham Southborough    0.0154              4            4\n##  3 Framingham Grafton         0.0191              1            2\n##  4 Framingham Grafton         0.0191              4            4\n##  5 Framingham Grafton         0.0191              2            2\n##  6 Framingham Grafton         0.0231              5            5\n##  7 Framingham Grafton         0.0304              5            5\n##  8 Framingham Grafton         0.0247              4            4\n##  9 Framingham Grafton         0.0247              4            3\n## 10 Framingham Grafton         0.0259              4            4\n## 11 Framingham Grafton         0.0259              3            3\n## 12 Framingham Grafton         0.0259              5            4\n## 13 Framingham Grafton         0.0159              4            4\n## 14 Framingham Grafton         0.0159              1            1\n## 15 Framingham Southborough    0.0392              2            2\n## # … with 100 more rows\nprint(trains, n = Inf, width = Inf)"
  },
  {
    "path": "visualization.html",
    "id": "exploring-tibbles",
    "chapter": "1 Visualization",
    "heading": "1.1.2 Exploring tibbles",
    "text": "many ways get feel data contained tibble.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "view",
    "chapter": "1 Visualization",
    "heading": "1.1.2.1 view()",
    "text": "Run view(trains) Console RStudio. Explore tibble resulting pop viewer.Observe many different types variables. variables quantitative. variables numerical nature. variables , including gender treatment, categorical. Categorical variables take one limited set possible values.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "glimpse",
    "chapter": "1 Visualization",
    "heading": "1.1.2.2 glimpse()",
    "text": "can also explore tibble using glimpse().see first values variable row variable name. addition, data type variable given immediately variable’s name, inside < >.dbl refers “double”, computer terminology quantitative/numerical variables. int “integer.” fct refers “factor,” variable use catagorical nominal data. chr character data.",
    "code": "\nglimpse(trains)## Rows: 115\n## Columns: 14\n## $ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n## $ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n## $ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n## $ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n## $ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n## $ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n## $ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n## $ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n## $ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n## $ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n## $ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n## $ hisp_perc      <dbl> 0.026, 0.015, 0.019, 0.019, 0.019, 0.023, 0.030, 0.025,…\n## $ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n## $ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…"
  },
  {
    "path": "visualization.html",
    "id": "summary-1",
    "chapter": "1 Visualization",
    "heading": "1.1.2.3 summary()",
    "text": "Usesummary() get sense distribution variables tibble.",
    "code": "\nsummary(trains)##    treatment    att_start       att_end        gender         \n##  Treated:51   Min.   : 3.0   Min.   : 3.0   Length:115        \n##  Control:64   1st Qu.: 7.0   1st Qu.: 7.0   Class :character  \n##               Median : 9.0   Median : 9.0   Mode  :character  \n##               Mean   : 9.2   Mean   : 9.1                     \n##               3rd Qu.:11.0   3rd Qu.:11.0                     \n##               Max.   :15.0   Max.   :15.0                     \n##                                                               \n##      race            liberal           party                age    \n##  Length:115         Mode :logical   Length:115         Min.   :20  \n##  Class :character   FALSE:64        Class :character   1st Qu.:33  \n##  Mode  :character   TRUE :51        Mode  :character   Median :43  \n##                                                        Mean   :42  \n##                                                        3rd Qu.:52  \n##                                                        Max.   :68  \n##                                                                    \n##      income           line             station            hisp_perc   \n##  Min.   : 23500   Length:115         Length:115         Min.   :0.01  \n##  1st Qu.: 87500   Class :character   Class :character   1st Qu.:0.02  \n##  Median :135000   Mode  :character   Mode  :character   Median :0.03  \n##  Mean   :141813                                         Mean   :0.04  \n##  3rd Qu.:135000                                         3rd Qu.:0.04  \n##  Max.   :300000                                         Max.   :0.26  \n##                                                         NA's   :1     \n##  ideology_start  ideology_end\n##  Min.   :1.0    Min.   :1.0  \n##  1st Qu.:2.0    1st Qu.:2.0  \n##  Median :3.0    Median :3.0  \n##  Mean   :2.8    Mean   :2.7  \n##  3rd Qu.:4.0    3rd Qu.:3.5  \n##  Max.   :5.0    Max.   :5.0  \n## "
  },
  {
    "path": "visualization.html",
    "id": "operator",
    "chapter": "1 Visualization",
    "heading": "1.1.2.4 $ operator",
    "text": "$ operator allows us extract single variable tibble return vector.",
    "code": "\ntrains$age##   [1] 31 34 63 45 55 37 53 36 54 42 33 50 24 40 53 50 33 33 32 57 41 36 43 25 41\n##  [26] 33 44 46 41 28 36 37 38 48 20 52 38 45 55 38 45 44 36 29 42 43 54 39 31 50\n##  [51] 60 67 54 44 50 20 57 25 60 44 35 54 52 47 60 47 22 56 50 21 29 45 46 42 23\n##  [76] 29 60 41 30 61 21 46 53 45 46 63 21 31 35 22 68 27 22 30 59 56 32 35 23 60\n## [101] 50 31 43 30 54 52 52 50 37 27 55 42 68 52 50"
  },
  {
    "path": "visualization.html",
    "id": "basic-plots",
    "chapter": "1 Visualization",
    "heading": "1.2 Basic Plots",
    "text": "three essential components plot:data: dataset containing variables interest.geom: geometric object display, e.g., scatterplot, line, bar.aes: aesthetic attributes geometric object. important names variables x y axes. Additional attributes include color size. Aesthetic attributes mapped variables dataset.Consider basic scatterplot using data Enos (2014) 115 Boston commuters.Notice data aes specified call ggplot(), followed choice geom.Plots composed layers, combined using + sign. essential layer specifies type geometric object want plot use. graph , geom used geom_point().+ sign comes end code line beginning. adding layers plot, start new line + code layer new line.",
    "code": "\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()"
  },
  {
    "path": "visualization.html",
    "id": "geom_point",
    "chapter": "1 Visualization",
    "heading": "1.2.1 geom_point()",
    "text": "Scatterplots, also called bivariate plots, allow visualize relationship two numerical variables.Recall scatterplot .Let’s break code, piece--piece.data argument set trains via data = trains.data argument set trains via data = trains.aesthetic mapping set via mapping = aes(x = age, y = income). , map age x axis income y axis.aesthetic mapping set via mapping = aes(x = age, y = income). , map age x axis income y axis.geometric object specified using geom_point(), telling R want scatterplot. added layer using + sign.geometric object specified using geom_point(), telling R want scatterplot. added layer using + sign.specify geometric object, blank plot:addition mapping variables x y axes, can also map variables color.use function labs() add plot title, axis labels, subtitles, captions graph. default, R simply uses names variables axes legends. Add better titles labels.Note , geom’s, add layer using + creating labs()plot. general, every plot give title axes labels. also add subtitle, purpose give short “main point” graphic. want viewer notice? also provide source data, usually via caption argument.Let’s now take tour useful geoms.",
    "code": "\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income))\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income,\n                     color = party)) + \n  geom_point()\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point() +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")"
  },
  {
    "path": "visualization.html",
    "id": "color-and-fill",
    "chapter": "1 Visualization",
    "heading": "1.2.2 Color and Fill",
    "text": "take look useful geoms, let’s first talk color fill. two arguments can use change color geoms .Let’s take look plot .Notice plot exact plot previous section, one key change: ’ve added colors separate commuters party. ? ’ll notice mapping = aes() function, ’ve added another argument. argument, color =, can change color dots. case, changed color party.can also change dots different.set color argument call geom_point() directly, points steel blue. (hundreds colors chosen.) Note difference setting color aesthetic (aes()) argument geom_point().another argument can use, called fill. key difference fill color fill defines color geom filled, whereas color sets outline geom.can see color changed outline bars. used fill:",
    "code": "\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income,\n                     color = party)) + \n  geom_point() +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point(color = \"steelblue\") +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\nggplot(data = trains,\n       mapping = aes(x = race)) +\n  geom_bar(color = \"pink\")\nggplot(data = trains,\n       mapping = aes(x = race)) +\n  geom_bar(fill = \"pink\")"
  },
  {
    "path": "visualization.html",
    "id": "geom_jitter",
    "chapter": "1 Visualization",
    "heading": "1.2.3 geom_jitter()",
    "text": "Consider different scatter plot using trains data.problem display “overplotting.” attitudes measured integers, know given point represents just one person dozen. two methods can use address overplotting: transparency jitter.Method 1: Changing transparencyWe can change transparency/opacity points using alpha argument within geom_point(). alpha argument can set value 0 1, 0 sets points 100% transparent 1 sets points 100% opaque. default, alpha set 1.Note aes() surrounding alpha = 0.2. mapping variable aesthetic attribute, changing default setting alpha.Method 2: Jittering pointsWe can also decide jitter points plot. replacing geom_point() geom_jitter(). Keep mind jittering strictly visualization tool; even creating jittered scatterplot, original values saved data frame remain unchanged.order specify much jitter add, use width height arguments geom_jitter(). corresponds hard ’d like shake plot horizontal x-axis units vertical y-axis units, respectively. important add just enough jitter break overlap points, extent alter original pattern.deciding whether jitter scatterplot use alpha argument geom_point(), know single right answer. suggest play around methods see one better emphasizes point trying make.",
    "code": "\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point(alpha = 0.2) +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_jitter() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")"
  },
  {
    "path": "visualization.html",
    "id": "geom_line",
    "chapter": "1 Visualization",
    "heading": "1.2.4 geom_line()",
    "text": "Linegraphs show relationship two numerical variables variable x-axis, also called explanatory, predictive, independent variable, sequential nature. words, inherent ordering variable.common examples linegraphs notion time x-axis: hours, days, weeks, years, etc. Since time sequential, connect consecutive observations variable y-axis line. Linegraphs notion time x-axis also called time series plots.Let’s plot median duration unemployment United States last 50 years.Almost every aspect code used create plot identical scatter plots, except geom used.",
    "code": "\nggplot(data = economics,\n       mapping = aes(x = date, y = uempmed)) +\n  geom_line() +\n  labs(title = \"Unemployment Duration in the United States: 1965 -- 2015\",\n       subtitle = \"Dramatic increase in duration after the Great Recesssion\",\n       x = \"Date\",\n       y = \"Median Duration in Weeks\",\n       caption = \"Source: FRED Economic Data\")"
  },
  {
    "path": "visualization.html",
    "id": "geom_histogram",
    "chapter": "1 Visualization",
    "heading": "1.2.5 geom_histogram()",
    "text": "histogram plot visualizes distribution numerical value.first cut x-axis series bins, bin represents range values.bin, count number observations fall range corresponding bin.draw bar whose height indicates corresponding count.Let’s consider income variable trains tibble. Pay attention changed two arguments ggplot(). removed data = mapping =. code still works R functions allow passing arguments position. first argument ggplot() data. don’t need tell R trains value data. R assumes passed first argument. Similarly, second argument ggplot() mapping, R assumes aes(x = income) value want mapping second item passed .Note message printed :stat_bin() using bins = 30. Pick better value binwidth.get message ran code . Try !message telling us histogram constructed using bins = 30 30 equally spaced bins. default value. Unless override default number bins number specify, R choose 30 default. important aspect making histogram, R insists informing message. make message go away specifying bin number , always .Let’s specify bins also add labels.Unlike scatterplots linegraphs, now one variable mapped aes(). , variable income. y-aesthetic histogram, count observations bin, gets computed automatically. Furthermore, geometric object layer now geom_histogram().can use fill argument change color actual bins. Let’s set fill “steelblue”.can also adjust number bins histogram one two ways:adjusting number bins via bins argument geom_histogram().adjusting number bins via bins argument geom_histogram().adjusting width bins via binwidth argument geom_histogram().adjusting width bins via binwidth argument geom_histogram().data, however, many unique values income, neither approach much effect. Replace income age want experiment options.",
    "code": "\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")"
  },
  {
    "path": "visualization.html",
    "id": "geom_bar",
    "chapter": "1 Visualization",
    "heading": "1.2.6 geom_bar()",
    "text": "geom_bar() visualizes distribution categorical variable. simpler task creating histogram, simply counting different categories within categorical variable, also known levels categorical variable. Often best way visualize different counts, also known frequencies, barplot.",
    "code": "\nggplot(data = trains, \n       mapping = aes(x = race)) +\n  geom_bar()"
  },
  {
    "path": "visualization.html",
    "id": "two-categorical-variables",
    "chapter": "1 Visualization",
    "heading": "1.2.6.1 Two categorical variables",
    "text": "Another use barplots visualize joint distribution two categorical variables. (See Chapter 5 definition joint distribution.) Let’s look race, well treatment, trains data using fill argument inside aes() aesthetic mapping. Recall fill aesthetic corresponds color used fill bars.example stacked barplot. simple make, certain aspects ideal. example, difficult compare heights different colors bars, corresponding comparing number people different races within region.alternative stacked barplots side--side barplots, also known dodged barplots. code create side--side barplot includes position = \"dodge\" argument added inside geom_bar(). words, overriding default barplot type, stacked barplot, specifying side--side barplot instead.Whites -represented Control group even though treatment assigned random.",
    "code": "\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar()\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar(position = \"dodge\")"
  },
  {
    "path": "visualization.html",
    "id": "geom_col",
    "chapter": "1 Visualization",
    "heading": "1.2.7 geom_col()",
    "text": "geom_col() similar geom_bar(), except geom_col() requires calculate number observations category ahead time. geom_bar() calculation . See example .learn filter data later chapter. However, key y-variable supplied geom_col() geom_bar(). geom_col() gives control data presented compared geom_bar(), may come useful circumstances.",
    "code": "\ntrains |> \n  group_by(race,treatment) |>\n  summarize(count = sum(n())) |> \n  ggplot(mapping = aes(x = race,\n                       y = count,\n                       fill = treatment)) +\n   geom_col(position = \"dodge\")## `summarise()` has grouped output by 'race'. You can override using the\n## `.groups` argument."
  },
  {
    "path": "visualization.html",
    "id": "no-pie-charts",
    "chapter": "1 Visualization",
    "heading": "1.2.7.1 No pie charts!",
    "text": "One common plots used visualize distribution categorical data pie chart. may seem harmless enough, pie charts actually present problem humans unable judge angles well. Robbins (2013) argues overestimate angles greater 90 degrees underestimate angles less 90 degrees. words, difficult us determine relative size one piece pie compared another. use pie charts.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "geom_smooth",
    "chapter": "1 Visualization",
    "heading": "1.2.8 geom_smooth()",
    "text": "can add trend lines plots create using geom_smooth() function.Recall following scatterplot previous work.can add trend line graph adding layer geom_smooth(). Including trend lines allow us visualize relationship att_start att_end.Note message. R telling us need specify method formula argument, just way told us provide bins argument used geom_histogram() .Let’s add argument method = \"lm\", “lm” stands linear model. causes fitted line straight rather curved. Let’s also add argument formula = y ~ x. make argument y ~ x since R doesn’t know model estimating, gives us warning y function x. specifying relationship, warning disappears R sure model estimating. , R giving us error . simply telling us options using since specify options .Always include enough detail code make messages disappear.Notice gray section surrounding line plotted. area called confidence interval, set 95% default. learn confidence intervals Chapter 5. can make shaded area disappear adding se = FALSE another argument geom_smooth().",
    "code": "\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth()## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x)"
  },
  {
    "path": "visualization.html",
    "id": "geom_density",
    "chapter": "1 Visualization",
    "heading": "1.2.9 geom_density()",
    "text": "Recall plot geom_histogram() section.Change geom_histogram() geom_density() make density plot, smoothed version histogram.values y-axis scaled total area curve equals one.",
    "code": "\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\nggplot(trains, \n       aes(x = income)) +\n  geom_density() +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = NULL,\n       caption = \"Data source: Enos (2014)\")"
  },
  {
    "path": "visualization.html",
    "id": "conclusion",
    "chapter": "1 Visualization",
    "heading": "1.2.10 Conclusion",
    "text": "geoms talked commonly used ones using R. However, endless list options pick . ’ve listed couple , however, feel free check documentation want learn !geom_boxplot(): creates box whiskers plot, visualizes 5 summary statistics.geom_dotplot(): similar bar graph, except stacked dots top rather bars.geom_map(): geom option turn polygons actual map. However, another method teach Maps also allows create maps.geom_text(): geom allows add text onto plots. use example Visualization Case Studies tutorial!",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "tidyverse",
    "chapter": "1 Visualization",
    "heading": "1.3 Tidyverse",
    "text": "Going forward, ggplot() code omit data = mapping = explicit naming arguments relying default ordering. time, include argument names , rule, . create many plots Primer omissions unlikely cause problems.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "data-wrangling",
    "chapter": "1 Visualization",
    "heading": "1.3.1 Data wrangling",
    "text": "can’t use beautiful plots learned previous chapter “wrangled” data convenient shape. Key wrangling functions include:filter(): pick rows want keep tibble.filter(): pick rows want keep tibble.select(): pick columns want keep tibble.select(): pick columns want keep tibble.arrange(): sort rows tibble, either ascending descending order.arrange(): sort rows tibble, either ascending descending order.mutate(): create new columns.mutate(): create new columns.group_by(): assign row tibble “group.” allows statistics calculated group separately. usually use group_by() summarize().group_by(): assign row tibble “group.” allows statistics calculated group separately. usually use group_by() summarize().summarize(): create new tibble comprised summary statistics one () rows grouped variable, tibble whole ungrouped.summarize(): create new tibble comprised summary statistics one () rows grouped variable, tibble whole ungrouped.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "the-pipe-operator",
    "chapter": "1 Visualization",
    "heading": "1.3.2 The pipe operator: |>",
    "text": "pipe operator (|>) allows us combine multiple operations R single sequential chain actions. Much like + sign come end line constructing plots — building plot layer--layer — pipe operator |> come end line building data wrangling pipeline step--step. include pipe operator, R assumes next line code unrelated layers built get error.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "filter-rows",
    "chapter": "1 Visualization",
    "heading": "1.3.3 filter() rows",
    "text": "\nFIGURE 1.1: filter() reduces rows tibble.\nfilter() function works much like “Filter” option Microsoft Excel. allows specify criteria values variable dataset selects rows match criteria.result using filter() tibble just rows want. alter data, can good idea save result new data frame using <- assignment operator.Let’s break code. assigned new data object named trains_men via trains_men <-. assigned modified data frame trains_men, separate entity initial trains data frame. , however, written code trains <- trains overwritten already-existing tibble.start trains tibble filter() observations gender equals “Male” included. test equality using double equal sign == single equal sign =. words, filter(gender = \"Male\") produce error. convention across many programming languages.can use operators beyond just == operator.> “greater ”< “less ”>= “greater equal ”<= “less equal ”!= “equal .” ! indicates “.”Furthermore, can combine multiple criteria using operators make comparisons:| “”& “”example, let’s filter() trains tibble include women Republicans younger 40.Instead creating single criterion many parts, like &, can just separate parts comma. resulting tibble .",
    "code": "\ntrains |> \n  filter(gender == \"Male\")## # A tibble: 64 × 14\n##    treatment att_start att_end gender race  liberal party       age income line \n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>     <int>  <dbl> <chr>\n##  1 Treated           3       5 Male   White TRUE    Democrat     63 135000 Fram…\n##  2 Treated          11      11 Male   White FALSE   Democrat     45 300000 Fram…\n##  3 Control           8       5 Male   White TRUE    Democrat     55 135000 Fram…\n##  4 Treated          10      11 Male   White FALSE   Democrat     36 135000 Fram…\n##  5 Treated           9      10 Male   White FALSE   Republic…    42 135000 Fram…\n##  6 Treated          11       9 Male   White FALSE   Democrat     50 250000 Fram…\n##  7 Treated          13      13 Male   White FALSE   Republic…    24 105000 Fram…\n##  8 Control           6       7 Male   White TRUE    Democrat     40  62500 Fram…\n##  9 Control           8       8 Male   White TRUE    Democrat     53 300000 Fram…\n## 10 Treated          13      13 Male   Asian FALSE   Republic…    33 250000 Fram…\n## # … with 54 more rows, and 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains_men <- trains |> \n  filter(gender == \"Male\")\ntrains |> \n  filter(gender == \"Female\" & \n           party == \"Republican\" &\n           age < 40)## # A tibble: 3 × 14\n##   treatment att_start att_end gender race  liberal party        age income line \n##   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>      <int>  <dbl> <chr>\n## 1 Treated           9      10 Female White FALSE   Republican    34 105000 Fram…\n## 2 Control          11      10 Female White FALSE   Republican    21 135000 Fran…\n## 3 Control          15      12 Female White FALSE   Republican    21 250000 Fran…\n## # … with 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains |> \n  filter(gender == \"Female\",\n         party == \"Republican\",\n         age < 40)## # A tibble: 3 × 14\n##   treatment att_start att_end gender race  liberal party        age income line \n##   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>      <int>  <dbl> <chr>\n## 1 Treated           9      10 Female White FALSE   Republican    34 105000 Fram…\n## 2 Control          11      10 Female White FALSE   Republican    21 135000 Fran…\n## 3 Control          15      12 Female White FALSE   Republican    21 250000 Fran…\n## # … with 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>"
  },
  {
    "path": "visualization.html",
    "id": "select-variables",
    "chapter": "1 Visualization",
    "heading": "1.3.4 select variables",
    "text": "\nFIGURE 1.2: select() reduces number columns tibble.\nUsing filter() function able pick specific rows (observations) tibble. select() function allows us pick specific columns (variables) instead.Use glimpse() see names variables trains:However, need two variables, say gender treatment. can select() just two:can drop, “de-select,” certain variables using minus (-) sign:can specify range columns using : operator.select() columns two specified variables.\nselect() function can also used rearrange columns used everything() helper function. can put treatment gender variables first :helper functions starts_with(), ends_with(), contains() can used select variables/columns match conditions. Examples:",
    "code": "\nglimpse(trains)## Rows: 115\n## Columns: 14\n## $ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n## $ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n## $ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n## $ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n## $ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n## $ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n## $ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n## $ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n## $ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n## $ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n## $ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n## $ hisp_perc      <dbl> 0.026, 0.015, 0.019, 0.019, 0.019, 0.023, 0.030, 0.025,…\n## $ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n## $ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…\ntrains |> \n  select(gender, treatment)## # A tibble: 115 × 2\n##    gender treatment\n##    <chr>  <fct>    \n##  1 Female Treated  \n##  2 Female Treated  \n##  3 Male   Treated  \n##  4 Male   Treated  \n##  5 Male   Control  \n##  6 Female Treated  \n##  7 Female Control  \n##  8 Male   Treated  \n##  9 Female Control  \n## 10 Male   Treated  \n## # … with 105 more rows\ntrains |> \n  select(-gender, -liberal, -party, -age)## # A tibble: 115 × 10\n##    treatment att_start att_end race  income line       station      hisp_perc\n##    <fct>         <dbl>   <dbl> <chr>  <dbl> <chr>      <chr>            <dbl>\n##  1 Treated          11      11 White 135000 Framingham Grafton         0.0264\n##  2 Treated           9      10 White 105000 Framingham Southborough    0.0154\n##  3 Treated           3       5 White 135000 Framingham Grafton         0.0191\n##  4 Treated          11      11 White 300000 Framingham Grafton         0.0191\n##  5 Control           8       5 White 135000 Framingham Grafton         0.0191\n##  6 Treated          13      13 White  87500 Framingham Grafton         0.0231\n##  7 Control          13      13 White  87500 Framingham Grafton         0.0304\n##  8 Treated          10      11 White 135000 Framingham Grafton         0.0247\n##  9 Control          12      12 White 105000 Framingham Grafton         0.0247\n## 10 Treated           9      10 White 135000 Framingham Grafton         0.0259\n## # … with 105 more rows, and 2 more variables: ideology_start <int>,\n## #   ideology_end <int>\ntrains |> \n  select(gender:age)## # A tibble: 115 × 5\n##    gender race  liberal party        age\n##    <chr>  <chr> <lgl>   <chr>      <int>\n##  1 Female White FALSE   Democrat      31\n##  2 Female White FALSE   Republican    34\n##  3 Male   White TRUE    Democrat      63\n##  4 Male   White FALSE   Democrat      45\n##  5 Male   White TRUE    Democrat      55\n##  6 Female White FALSE   Democrat      37\n##  7 Female White FALSE   Republican    53\n##  8 Male   White FALSE   Democrat      36\n##  9 Female White FALSE   Democrat      54\n## 10 Male   White FALSE   Republican    42\n## # … with 105 more rows\ntrains |> \n  select(treatment, gender, everything())## # A tibble: 115 × 14\n##    treatment gender att_start att_end race  liberal party       age income line \n##    <fct>     <chr>      <dbl>   <dbl> <chr> <lgl>   <chr>     <int>  <dbl> <chr>\n##  1 Treated   Female        11      11 White FALSE   Democrat     31 135000 Fram…\n##  2 Treated   Female         9      10 White FALSE   Republic…    34 105000 Fram…\n##  3 Treated   Male           3       5 White TRUE    Democrat     63 135000 Fram…\n##  4 Treated   Male          11      11 White FALSE   Democrat     45 300000 Fram…\n##  5 Control   Male           8       5 White TRUE    Democrat     55 135000 Fram…\n##  6 Treated   Female        13      13 White FALSE   Democrat     37  87500 Fram…\n##  7 Control   Female        13      13 White FALSE   Republic…    53  87500 Fram…\n##  8 Treated   Male          10      11 White FALSE   Democrat     36 135000 Fram…\n##  9 Control   Female        12      12 White FALSE   Democrat     54 105000 Fram…\n## 10 Treated   Male           9      10 White FALSE   Republic…    42 135000 Fram…\n## # … with 105 more rows, and 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains |> \n  select(starts_with(\"a\"))## # A tibble: 115 × 3\n##    att_start att_end   age\n##        <dbl>   <dbl> <int>\n##  1        11      11    31\n##  2         9      10    34\n##  3         3       5    63\n##  4        11      11    45\n##  5         8       5    55\n##  6        13      13    37\n##  7        13      13    53\n##  8        10      11    36\n##  9        12      12    54\n## 10         9      10    42\n## # … with 105 more rows"
  },
  {
    "path": "visualization.html",
    "id": "slice-and-pull-and",
    "chapter": "1 Visualization",
    "heading": "1.3.5 slice() and pull() and []",
    "text": "slice() pull() additional functions can use pick specific rows columns within data frame.Using slice() gives us specific rows trains tibble:Unlike filter(), slice() relies numeric order data.pull() grabs variable vector, rather leaving within tibble, select() :",
    "code": "\ntrains |> \n  slice(2:5)## # A tibble: 4 × 14\n##   treatment att_start att_end gender race  liberal party        age income line \n##   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>      <int>  <dbl> <chr>\n## 1 Treated           9      10 Female White FALSE   Republican    34 105000 Fram…\n## 2 Treated           3       5 Male   White TRUE    Democrat      63 135000 Fram…\n## 3 Treated          11      11 Male   White FALSE   Democrat      45 300000 Fram…\n## 4 Control           8       5 Male   White TRUE    Democrat      55 135000 Fram…\n## # … with 4 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>\ntrains |> \n  slice(2:5) |> \n  pull(age)## [1] 34 63 45 55"
  },
  {
    "path": "visualization.html",
    "id": "arrange",
    "chapter": "1 Visualization",
    "heading": "1.3.6 arrange()",
    "text": "arrange() allows us sort/reorder tibble’s rows according values specific variable. Unlike filter() select(), arrange() remove rows columns tibble. Example:arrange() always returns rows sorted ascending order default. switch ordering descending order instead, use desc() function:first many “pipes” create Primer. First, trains tibble. Second, pipe select() function. Third, pipe results select() arrange() function. step pipe starts tibble , done, produces tibble. tibbles way !",
    "code": "\ntrains |> \n  select(treatment, gender, age) |> \n  arrange(age)## # A tibble: 115 × 3\n##    treatment gender   age\n##    <fct>     <chr>  <int>\n##  1 Treated   Female    20\n##  2 Control   Male      20\n##  3 Control   Male      21\n##  4 Control   Female    21\n##  5 Control   Female    21\n##  6 Control   Male      22\n##  7 Control   Female    22\n##  8 Treated   Male      22\n##  9 Treated   Male      23\n## 10 Control   Male      23\n## # … with 105 more rows\ntrains |> \n  select(treatment, gender, age) |> \n  arrange(desc(age))## # A tibble: 115 × 3\n##    treatment gender   age\n##    <fct>     <chr>  <int>\n##  1 Control   Female    68\n##  2 Control   Male      68\n##  3 Control   Male      67\n##  4 Treated   Male      63\n##  5 Control   Male      63\n##  6 Control   Male      61\n##  7 Control   Female    60\n##  8 Treated   Male      60\n##  9 Control   Male      60\n## 10 Control   Female    60\n## # … with 105 more rows"
  },
  {
    "path": "visualization.html",
    "id": "mutate",
    "chapter": "1 Visualization",
    "heading": "1.3.7 mutate()",
    "text": "\nFIGURE 1.3: `mutate() adds column tibble.\nmutate() takes existing columns creates new column. Recall income variable trains tibble dollars. Let’s use mutate() create new variable income thousands dollars. (use select() start pipe easier see new old variables time.)Notice newly created column right-hand side tibble named income_in_thousands.creating new variables can also overwrite original tibble:Whenever create new tibble, new variable within tibble, face dilemma: overwrite existing tibble/variable create new one? right answer.example, instead overwriting trains code , created new tibble trains_new. Similarly, instead creating new variable, income_in_thousands, overwritten current value income. Use best judgment careful.",
    "code": "\ntrains |> \n  select(gender, income) |> \n  mutate(income_in_thousands = income / 1000)## # A tibble: 115 × 3\n##    gender income income_in_thousands\n##    <chr>   <dbl>               <dbl>\n##  1 Female 135000               135  \n##  2 Female 105000               105  \n##  3 Male   135000               135  \n##  4 Male   300000               300  \n##  5 Male   135000               135  \n##  6 Female  87500                87.5\n##  7 Female  87500                87.5\n##  8 Male   135000               135  \n##  9 Female 105000               105  \n## 10 Male   135000               135  \n## # … with 105 more rows\ntrains <- trains |> \n  mutate(income_in_thousands = (income) / 1000)"
  },
  {
    "path": "visualization.html",
    "id": "if_else",
    "chapter": "1 Visualization",
    "heading": "1.3.7.1 if_else()",
    "text": "if_else() often used within calls mutate(). three arguments. first argument test logical vector. result contain value second argument, yes, test TRUE, value third argument, , FALSE.Imagine want create new variable old, TRUE age > 50 FALSE otherwise.Another function similar if_else(), dplyr::case_when(). case_when() particularly useful inside mutate want create new variable relies complex combination existing variables. Note different version if_else() base R: ifelse(). works exactly dplyr version somewhat less robust. reason, prefer if_else() version.",
    "code": "\ntrains |> \n  select(age) |> \n  mutate(old = if_else(age > 50, TRUE, FALSE))## # A tibble: 115 × 2\n##      age old  \n##    <int> <lgl>\n##  1    31 FALSE\n##  2    34 FALSE\n##  3    63 TRUE \n##  4    45 FALSE\n##  5    55 TRUE \n##  6    37 FALSE\n##  7    53 TRUE \n##  8    36 FALSE\n##  9    54 TRUE \n## 10    42 FALSE\n## # … with 105 more rows"
  },
  {
    "path": "visualization.html",
    "id": "summarize",
    "chapter": "1 Visualization",
    "heading": "1.3.8 summarize()",
    "text": "often need calculate summary statistics, things like mean (also called average) median (middle value). examples summary statistics include sum, minimum, maximum, standard deviation.function summarize() allows us calculate statistics individual columns tibble. Example:mean() sd() summary functions go inside summarize() function. summarize() function takes tibble returns tibble one row corresponding summary statistics. Remember: Tibbles go tibbles come .mean()mean, average, commonly reported measure center distribution. mean sum data elements divided number elements. \\(N\\) data points, mean given :\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_N}{N}\\]median()median another commonly reported measure center distribution, calculated first sorting vector values smallest largest. middle element sorted list median. middle falls two values, median mean two middle values. median mean two common measures center distribution. median stable, less affected outliers. widely accepted symbol median, although \\(\\tilde{x}\\) uncommon. \\(n\\) data points, \\(n\\) even, median given :\\[m(x) = {\\frac{1}{2}}{(x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1})}\\]\n\\(n\\) odd, median given :sd()standard deviation (sd) distribution measure variation around mean.\\[\\text{sd} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}}\\]mad()scaled median absolute deviation (mad) measure variation around median. popular standard deviation. formula calculating mad bit mysterious.\\[\\text{mad} = 1.4826 \\times \\text{median}(abs(x - \\tilde{x}))\\]basic idea sd mad need measure variation around center distribution variable. sd uses mean, \\(\\bar{x}\\), estimate center mad uses median, \\(\\tilde{x}\\). mad uses absolute difference, opposed squared difference, robust outliers. 1.4826 multiplier causes mad sd identical (important) case standard normal distributions, topic introduce Chapter 2.quantile()quantile distribution value distribution occupies specific percentile location sorted list values..e., 5th percentile distribution point 5% data falls. 95th percentile , similarly, point 95% data falls. 50th percentile, median, splits data two separate, equal, parts. minimum 0th percentile. maximum 100th percentile.Therefore, value 5th percentile quantile 5th percentile. Say dataset consisted numbers 0-100. Therefore, 5th percentile 5, 5 quantile.Let’s take look poverty variable kenya tibble primer.data package. poverty percentage residents community incomes poverty line. Let’s first confirm quantile() works comparing output simpler functions.probs argument allows us specify percentile(s) want. Two important percentiles 2.5th 97.5th define 95% interval, central range includes 95% values.interval two percentiles includes 95% values distribution. Depending context, interval sometimes called “confidence interval” “uncertainty interval” “compatibility interval.” Different percentile ranges create intervals different widths.NA value variable, statistical function like mean() return NA. can fix using na.rm = TRUE within statistical function.",
    "code": "\ntrains |> \n  summarize(mn_age = mean(age), \n            sd_age = sd(age))## # A tibble: 1 × 2\n##   mn_age sd_age\n##    <dbl>  <dbl>\n## 1   42.4   12.2\nc(min(kenya$poverty), median(kenya$poverty), max(kenya$poverty))## [1] 0.18 0.43 0.90\nquantile(kenya$poverty, probs = c(0, 0.5, 1))##   0%  50% 100% \n## 0.18 0.43 0.90\nquantile(kenya$poverty, probs = c(0.025, 0.975))##  2.5% 97.5% \n##  0.22  0.66"
  },
  {
    "path": "visualization.html",
    "id": "group_by",
    "chapter": "1 Visualization",
    "heading": "1.3.9 group_by()",
    "text": "can use mean() summarize() calculate average age people trains, .want mean age gender? Consider:data , note “Groups” message top. R informing tibble grouped operation perform now done gender.Notice message R sends us. warning means tibble issues forth end pipe “ungrouped”. means group attribute applied group_by() removed. behavior (sensible) default.proper way handle situation, everywhere else use group_by() summarize(), specify .groups argument.code thing first version, issue message, since made affirmative decision drop grouping variables.group_by() function doesn’t change data frames . Rather changes meta-data, data data, specifically grouping structure. apply summarize() function tibble changes.tibble grouped, can remove grouping variable using ungroup().R code behaving weird way, especially “losing” rows, problem often solved using ungroup() pipeline.",
    "code": "\ntrains |> \n  summarize(avg = mean(age))## # A tibble: 1 × 1\n##     avg\n##   <dbl>\n## 1  42.4\ntrains |> \n  group_by(gender)## # A tibble: 115 × 15\n## # Groups:   gender [2]\n##    treatment att_start att_end gender race  liberal party       age income line \n##    <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>     <int>  <dbl> <chr>\n##  1 Treated          11      11 Female White FALSE   Democrat     31 135000 Fram…\n##  2 Treated           9      10 Female White FALSE   Republic…    34 105000 Fram…\n##  3 Treated           3       5 Male   White TRUE    Democrat     63 135000 Fram…\n##  4 Treated          11      11 Male   White FALSE   Democrat     45 300000 Fram…\n##  5 Control           8       5 Male   White TRUE    Democrat     55 135000 Fram…\n##  6 Treated          13      13 Female White FALSE   Democrat     37  87500 Fram…\n##  7 Control          13      13 Female White FALSE   Republic…    53  87500 Fram…\n##  8 Treated          10      11 Male   White FALSE   Democrat     36 135000 Fram…\n##  9 Control          12      12 Female White FALSE   Democrat     54 105000 Fram…\n## 10 Treated           9      10 Male   White FALSE   Republic…    42 135000 Fram…\n## # … with 105 more rows, and 5 more variables: station <chr>, hisp_perc <dbl>,\n## #   ideology_start <int>, ideology_end <int>, income_in_thousands <dbl>\ntrains |> \n  group_by(gender) |> \n  summarize(avg = mean(age))## # A tibble: 2 × 2\n##   gender   avg\n##   <chr>  <dbl>\n## 1 Female  41.0\n## 2 Male    43.5\ntrains |> \n  group_by(gender) |> \n  summarize(mean = mean(age),\n            .groups = \"drop\")## # A tibble: 2 × 2\n##   gender  mean\n##   <chr>  <dbl>\n## 1 Female  41.0\n## 2 Male    43.5"
  },
  {
    "path": "visualization.html",
    "id": "advanced-plots",
    "chapter": "1 Visualization",
    "heading": "1.4 Advanced Plots",
    "text": "Good visualizations teach. construct plot, decide message want convey. functions may helpful.",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "plot-objects",
    "chapter": "1 Visualization",
    "heading": "1.4.1 Plot objects",
    "text": "Plots R objects, just like tibbles. can create , print save . now, just “spat” R code chunk. Nothing wrong ! Indeed, common approach plotting R. Sometimes, however, handy work plot object. Consider:code first example geom_point(). train_plot R object. code print anything . order make plot appear, need print explicitly:Recall typing name object thing using print(). Now object, can display whenever want., sometimes, want permanent copy plot, saved computer. purpose ggsave():ggsave() uses suffix provided filename determine type image save. use “enos_trains.jpg”, file saved JPEG format. used “enos_trains.png”, file saved PNG. can display saved file using knitr::include_graphics(). example:code displays image Rmd, assuming file “enos_trains.jpg” located current working directory. common scenario create image store directory named figures/ use figure one Rmd.",
    "code": "\ntrain_plot <- ggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\ntrain_plot\nggsave(filename = \"enos_trains.jpg\", \n       plot = train_plot)\nknitr::include_graphics(\"enos_trains.jpg\")"
  },
  {
    "path": "visualization.html",
    "id": "faceting",
    "chapter": "1 Visualization",
    "heading": "1.4.2 Faceting",
    "text": "Faceting splits visualization parts, one value another variable. create multiple copies type plot matching x y axes, whose contents differ.proceed, let’s create subset tibble gapminder gapminder package. (may need install gapminder package code work. Refer back introduction need refresher .)Let’s plot filtered data using geom_point()difficult compare continents despite colors. much easier “split” scatterplot 4 continents. words, create plots gdpPercap lifeExp continent separately. using function facet_wrap() argument ~ continent. facet_wrap, must always put tilde (~) front variable wish wrap .much better! can specify number rows columns grid using nrow argument inside facet_wrap(). Let’s get continents row setting nrow 1. Let’s also add trend line geom_smooth() faceted plot.expected, can see positive correlation economic development life expectancy continents.",
    "code": "\nlibrary(gapminder)\ngapminder_filt <- gapminder |> \n      filter(year == 2007, continent != \"Oceania\")\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point()\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent)\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x, \n              se = FALSE)"
  },
  {
    "path": "visualization.html",
    "id": "stats",
    "chapter": "1 Visualization",
    "heading": "1.4.3 Stats",
    "text": "Consider following histogram.Recall y-aesthetic histogram — count observations bin — gets computed automatically. can use after_stat() argument within geom_histogram() generate percent values y-aesthetic. after_stat() allows us control values variables calculated specifically specific aesthetic layer.",
    "code": "\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp))+ \n  geom_histogram(bins = 20, \n                 color = \"white\")\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp)) + \n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 20) +\n  labs(y = \"Percentage\")"
  },
  {
    "path": "visualization.html",
    "id": "axis-limits-and-scales",
    "chapter": "1 Visualization",
    "heading": "1.4.4 Axis Limits and Scales",
    "text": "",
    "code": ""
  },
  {
    "path": "visualization.html",
    "id": "coord_cartesian",
    "chapter": "1 Visualization",
    "heading": "1.4.4.1 coord_cartesian()",
    "text": "can also manipulate limits axes using xlim() ylim() within call coord_cartesian(). example, assume interested countries GDP per capita 0 30,000. Recall , data first argument mapping second ggplot(), don’t actually name arguments. can just provide , long correct order.can see GDP per capita x-axis now shown 0 30,000.",
    "code": "\nggplot(gapminder_filt, \n       aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  coord_cartesian(xlim = c(0, 30000))"
  },
  {
    "path": "visualization.html",
    "id": "scale_x-and-scale_y",
    "chapter": "1 Visualization",
    "heading": "1.4.4.2 scale_x and scale_y",
    "text": "can also change scaling axes. example, might useful display axes logarithmic scale using scale_x_log10() scale_y_log10(). Also, note can (lazily!) provide explicit x y argument names aes() long provide values right order: x comes y.Beyond scale_x_log10(), ways change scales. cover scale_y_continuous scale_x_continuous section.two major uses scale_x_continuous, change breaks labels. Take graph .want breaks y-axis every 20 years instead every 10. also want add dollar signs x-axis, breaks every 20,000 dollars. Let’s fix graph!\nLet’s break . used breaks argument create breaks scale. used c() function specify breaks wanted. , used labels modify labels x-axis. :: allows us extract function specific package, function exists multiple packages. , specifically extract dollar_format function scales package change labels x-axis.another function called scale_x_discrete/scale_y_discrete. function similar enough scale_y/x_continuous give section. difference usage discrete vs. continuous function discrete function applied discrete variables. Discrete variables countable (.e. number tables room) nothing , whereas continuous variables infinite possibilities (.e. height infinite number possible values).",
    "code": "\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10()\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) +\n  scale_y_continuous(breaks = c(40, 60, 80)) +\n  scale_x_continuous(labels = scales::dollar_format(),\n                     breaks = c(0, 20000, 40000))"
  },
  {
    "path": "visualization.html",
    "id": "text",
    "chapter": "1 Visualization",
    "heading": "1.4.5 Text",
    "text": "Recall use labs() add labels titles plots. can also change labels inside plots using geom_text().Let’s breakdown code within geom_text(). included new aesthetic called label. defines character variable used basis labels. set label country point corresponds country represents. set text font setting size 2, set text color using color. Finally, included argument check_overlap = TRUE make sure names countries legible.",
    "code": "\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\") +\n  geom_text(aes(label = country), \n            size = 2, \n            color = \"black\", \n            check_overlap = TRUE)"
  },
  {
    "path": "visualization.html",
    "id": "themes",
    "chapter": "1 Visualization",
    "heading": "1.4.6 Themes",
    "text": "Themes can used change overall appearance plot without much effort. add themes layers plots. can find overview different themes ggplot .Consider following faceted scatterplot.Note use breaks argument scale_x_log10(). specifies location labels x-axis. can also use labels argument want change appearence. tricks work entire family scale_* functions.Let’s now add theme faceted scatterplot. use theme theme_economist(), ggthemes package, make plot look like plots Economist.looks pretty good. However, notice legend top graph. crowds graph takes away important part: data. can use theme() customize non-data parts plots background, gridlines, legends. Let’s de-clutter graph removing legend. can using legend.position argument setting “none”.Great. Now graph easier visualize.",
    "code": "\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) \nlibrary(ggthemes)\n\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) +\n  theme_economist()\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000),\n                  labels = scales::dollar_format(accuracy = 1)) + \n    theme_economist() +\n    theme(legend.position = \"none\")"
  },
  {
    "path": "visualization.html",
    "id": "theme",
    "chapter": "1 Visualization",
    "heading": "1.4.6.1 theme()",
    "text": "theme() function also offers wide selection functions manually changing individual elements. cover widely used ones , vast majority listed link.two key elements theme() function.Theme elements: specify non-data elements can control. example, panel.border element controls border grid area.Element function: describes visual properties element. four main element functions, element_blank(), element_rect(), element_line(), element_text().\nelement_blank(): hides element theme.\nelement_line(): modifies elements plot lines, grid lines, axes, etc.\nelement_text(): changes text elements plot, like titles, captions, etc.\nelement_rect(): rectangle elements control background plots, legends, etc.\nelement_blank(): hides element theme.element_line(): modifies elements plot lines, grid lines, axes, etc.element_text(): changes text elements plot, like titles, captions, etc.element_rect(): rectangle elements control background plots, legends, etc.Let’s look example might want modify graph. Take graph .couple problems graph. Firstly, want title bold. want light blue background. also don’t particularly care many grid lines x-axis. Let’s change !! Within theme() function, can change plot title bold, background blue, get rid “minor” grid lines. theme() versatile function. See full list elements .",
    "code": "\nggplot(data = economics,\n       mapping = aes(x = date,\n                     y = unemploy)) +\n  geom_line() +\n  labs(title = \"Unemployed Population in the United States: 1965 - 2015\",\n       subtitle = \"Dramatic spike during the Great Recesssion\",\n       x = \"Year\",\n       y = \"Number of Unemployed (in thousands)\",\n       caption = \"Source: FRED Economic Data\")\nggplot(data = economics,\n       mapping = aes(x = date,\n                     y = unemploy)) +\n  geom_line() +\n  labs(title = \"Unemployed Population in the United States: 1965 - 2015\",\n       subtitle = \"Dramatic spike during the Great Recesssion\",\n       x = \"Year\",\n       y = \"Number of Unemployed (in thousands)\",\n       caption = \"Source: FRED Economic Data\") +\n  theme(plot.title = element_text(face = \"bold\"),\n        panel.background = element_rect(fill = \"lightblue\"),\n        panel.grid.minor.x = element_blank())"
  },
  {
    "path": "visualization.html",
    "id": "going-further",
    "chapter": "1 Visualization",
    "heading": "1.5 Going further",
    "text": "many plots can make R shown yet. example, can create cool animations gganimate package, like plot .can make plots interactive, using plotly package. Remember, need load packages use , load library(plotly) want make interactive plots. Example:can also create maps census data, like plot .",
    "code": "## To enable caching of data, set `options(tigris_use_cache = TRUE)`\n## in your R script or .Rprofile."
  },
  {
    "path": "visualization.html",
    "id": "summary-2",
    "chapter": "1 Visualization",
    "heading": "1.6 Summary",
    "text": "Tibbles rectangular stores data. specific type data frame, use terms interchangeably.need practice every day.use pie charts.Shield eyes ugly messages warning.step pipe starts tibble , done, produces tibble. tibbles way !R code behaving weird way, especially “losing” rows, problem often solved using ungroup() pipeline.two important attributes distribution center variation around center.chapter, first looked basic coding terminology concepts deal programming R. learned three basic components make plot: data, mapping, one geoms. ggplot2 package offers wide range geoms can use create different types plots. Next, examined “super package” tidyverse, includes helpful tools visualization. also offers features importing manipulating data, main topic Chapter @(wrangling). Lastly, explored advanced plotting features axis scaling, faceting, themes.Recall plot began chapter :now know enough make plots like .beautiful plot just collection steps, simple enough . taught () steps. Time start walking .",
    "code": ""
  },
  {
    "path": "wrangling.html",
    "id": "wrangling",
    "chapter": "2 Wrangling",
    "heading": "2 Wrangling",
    "text": "Data science data cleaning.Start loading packages need chapter.tidyverse package used every chapter. Loading makes 8 packages “Tidyverse” available.primer.data data package created specifically primer.lubridate package working dates times.skimr contains functions useful providing summary statistics, especially skim().nycflights13 includes data associated flights New York City’s three major airports.gapminder annual data countries going back 50 years, used Chapter 1.fivethirtyeight cleans data FiveThirtyEight team.",
    "code": "\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(lubridate)\nlibrary(skimr)\nlibrary(nycflights13)\nlibrary(gapminder)\nlibrary(fivethirtyeight)"
  },
  {
    "path": "wrangling.html",
    "id": "tibbles",
    "chapter": "2 Wrangling",
    "heading": "2.1 Tibbles",
    "text": "Tibbles kind data frame, useful storing data number observations variable. can use tibble() function create tibbles. Tibbles composed columns, variable, rows, “unit” “observation.” Furthermore, column (.e., variable) can different type: character, integer, factor, double, date .",
    "code": ""
  },
  {
    "path": "wrangling.html",
    "id": "tibble",
    "chapter": "2 Wrangling",
    "heading": "2.1.1 tibble()",
    "text": "code, specify column names , , b, c. variable, give different value. variable name, data type specified data within column. tibble can consist one atomic vectors, important types double, character, logical, integer. tibble includes variable type. “L” “9L” tells R want d integer rather default, double. print tibble, variable type shown variable name, indicated <dbl>, <chr>, .Variables begin number (like 54abc) include spaces (like var). insist using variable names , must include backticks around name reference .include backticks, R give us error.real world, may come across datasets dirty column names 54abc var.sometimes easier use function tribble() create tibbles.tildes — ~ var1 — specify row column names. formatting makes easier, relative specifying raw vectors, see values observation.",
    "code": "\ntibble(a = 2.1, b = \"Hello\", c = TRUE, d = 9L)## # A tibble: 1 × 4\n##       a b     c         d\n##   <dbl> <chr> <lgl> <int>\n## 1   2.1 Hello TRUE      9\ntibble(`54abc` = 1, `my var` = 2, c = 3)## # A tibble: 1 × 3\n##   `54abc` `my var`     c\n##     <dbl>    <dbl> <dbl>\n## 1       1        2     3tibble(54abc = 1, my var = 2, c = 3)## Error: <text>:1:10: unexpected symbol\n## 1: tibble(54abc\n##              ^\ntribble(\n  ~ var1, ~ `var 2`, ~ myvar,\n  1,           3,      5,\n  4,           6,      8,\n)## # A tibble: 2 × 3\n##    var1 `var 2` myvar\n##   <dbl>   <dbl> <dbl>\n## 1     1       3     5\n## 2     4       6     8"
  },
  {
    "path": "wrangling.html",
    "id": "lists",
    "chapter": "2 Wrangling",
    "heading": "2.2 Lists",
    "text": "\nFIGURE 2.1: Subsetting list, visually.\nEarlier, briefly introduced lists. Lists type vector step complexity atomic vectors, lists can contain lists. makes suitable representing hierarchical tree-like structures. create list function list():useful tool working lists str() focuses displaying structure, contents.Unlike atomic vectors, list() can contain objects several types.Lists can even contain lists!",
    "code": "\nx <- list(1, 2, 3)\nx## [[1]]\n## [1] 1\n## \n## [[2]]\n## [1] 2\n## \n## [[3]]\n## [1] 3\nstr(x)## List of 3\n##  $ : num 1\n##  $ : num 2\n##  $ : num 3\nx_named <- list(a = 1, b = 2, c = 3)\nstr(x_named)## List of 3\n##  $ a: num 1\n##  $ b: num 2\n##  $ c: num 3\ny <- list(\"a\", 1L, 1.5, TRUE)\nstr(y)## List of 4\n##  $ : chr \"a\"\n##  $ : int 1\n##  $ : num 1.5\n##  $ : logi TRUE\nz <- list(list(1, 2), list(3, 4))\nstr(z)## List of 2\n##  $ :List of 2\n##   ..$ : num 1\n##   ..$ : num 2\n##  $ :List of 2\n##   ..$ : num 3\n##   ..$ : num 4"
  },
  {
    "path": "wrangling.html",
    "id": "visualizing-lists",
    "chapter": "2 Wrangling",
    "heading": "2.2.1 Visualizing lists",
    "text": "explain complicated list manipulation functions, ’s helpful visual representation lists. example, take three lists:structured follows:three principles:Lists rounded corners. Atomic vectors square corners.Lists rounded corners. Atomic vectors square corners.Children drawn inside parent, slightly darker background make easier see hierarchy.Children drawn inside parent, slightly darker background make easier see hierarchy.orientation children (.e. rows columns) isn’t important, ’ll pick row column orientation either save space illustrate important property example.orientation children (.e. rows columns) isn’t important, ’ll pick row column orientation either save space illustrate important property example.",
    "code": "\nx1 <- list(c(1, 2), c(3, 4))\nx2 <- list(list(1, 2), list(3, 4))\nx3 <- list(1, list(2, list(3)))"
  },
  {
    "path": "wrangling.html",
    "id": "subsetting",
    "chapter": "2 Wrangling",
    "heading": "2.2.2 Subsetting",
    "text": "three ways subset list, ’ll illustrate list named :[ ] extracts sub-list. result always list.Like vectors, can subset logical, integer, character vector.[[ ]] extracts single component list. removes level hierarchy list.$ shorthand extracting named elements list. works similarly [[ ]] except don’t need use quotes.distinction [ ] [[ ]] really important lists, [[ ]] drills list [ ] returns new, smaller list. Compare code output visual representation.",
    "code": "\na <- list(a = 1:3, b = \"a string\", c = pi, d = list(-1, -5))\nstr(a[1:2])## List of 2\n##  $ a: int [1:3] 1 2 3\n##  $ b: chr \"a string\"\nstr(a[4])## List of 1\n##  $ d:List of 2\n##   ..$ : num -1\n##   ..$ : num -5\nstr(a[[1]])##  int [1:3] 1 2 3\nstr(a[[4]])## List of 2\n##  $ : num -1\n##  $ : num -5\na$a## [1] 1 2 3\na[[\"a\"]]## [1] 1 2 3"
  },
  {
    "path": "wrangling.html",
    "id": "characters",
    "chapter": "2 Wrangling",
    "heading": "2.3 Characters",
    "text": "\nFIGURE 2.2: Real data nasty.\nfar, tibbles clean wholesome, like gapminder trains. Real data nasty. bring data R outside world discover problems. now discuss common remedial tasks cleaning transforming character data, also known strings. string one characters enclosed inside pair matching ‘single’ “double quotes”.use fruit data, vector names different fruits, stringr package, automatically loaded issue library(tidyverse). Although can manipulate character vectors directly, much common, real world situations, work vectors tibble.",
    "code": "\ntbl_fruit <- tibble(fruit = fruit)"
  },
  {
    "path": "wrangling.html",
    "id": "character-vectors",
    "chapter": "2 Wrangling",
    "heading": "2.3.1 Character vectors",
    "text": "Note slice_sample() selects random set rows tibble. argument n, shown , display many rows. Use argument prop return specific percentage rows tibble.str_detect() determines character vector matches pattern. returns logical vector length input. Recall logicals either TRUE FALSE.fruit names actually include letter “c?”str_length() counts characters strings. Note different length() character vector .str_sub() returns portion string. demonstration return first three letters fruit arguments fruit, 1, 3.str_c() combines character vector length single string. similar normal c() function creating vector.str_replace() replaces pattern within string.",
    "code": "\ntbl_fruit |> \n  slice_sample(n = 8)## # A tibble: 8 × 1\n##   fruit       \n##   <chr>       \n## 1 pomelo      \n## 2 watermelon  \n## 3 kumquat     \n## 4 blackcurrant\n## 5 lychee      \n## 6 mango       \n## 7 boysenberry \n## 8 canary melon\ntbl_fruit |> \n  mutate(fruit_in_name = str_detect(fruit, pattern = \"c\")) ## # A tibble: 80 × 2\n##    fruit        fruit_in_name\n##    <chr>        <lgl>        \n##  1 apple        FALSE        \n##  2 apricot      TRUE         \n##  3 avocado      TRUE         \n##  4 banana       FALSE        \n##  5 bell pepper  FALSE        \n##  6 bilberry     FALSE        \n##  7 blackberry   TRUE         \n##  8 blackcurrant TRUE         \n##  9 blood orange FALSE        \n## 10 blueberry    FALSE        \n## # … with 70 more rows\ntbl_fruit |> \n  mutate(name_length = str_length(fruit)) ## # A tibble: 80 × 2\n##    fruit        name_length\n##    <chr>              <int>\n##  1 apple                  5\n##  2 apricot                7\n##  3 avocado                7\n##  4 banana                 6\n##  5 bell pepper           11\n##  6 bilberry               8\n##  7 blackberry            10\n##  8 blackcurrant          12\n##  9 blood orange          12\n## 10 blueberry              9\n## # … with 70 more rows\ntbl_fruit |> \n  mutate(first_three_letters = str_sub(fruit, 1, 3)) ## # A tibble: 80 × 2\n##    fruit        first_three_letters\n##    <chr>        <chr>              \n##  1 apple        app                \n##  2 apricot      apr                \n##  3 avocado      avo                \n##  4 banana       ban                \n##  5 bell pepper  bel                \n##  6 bilberry     bil                \n##  7 blackberry   bla                \n##  8 blackcurrant bla                \n##  9 blood orange blo                \n## 10 blueberry    blu                \n## # … with 70 more rows\ntbl_fruit |> \n  mutate(name_with_s = str_c(fruit, \"s\")) ## # A tibble: 80 × 2\n##    fruit        name_with_s  \n##    <chr>        <chr>        \n##  1 apple        apples       \n##  2 apricot      apricots     \n##  3 avocado      avocados     \n##  4 banana       bananas      \n##  5 bell pepper  bell peppers \n##  6 bilberry     bilberrys    \n##  7 blackberry   blackberrys  \n##  8 blackcurrant blackcurrants\n##  9 blood orange blood oranges\n## 10 blueberry    blueberrys   \n## # … with 70 more rows\ntbl_fruit |> \n  mutate(capital_A = str_replace(fruit, \n                                 pattern = \"a\", \n                                 replacement = \"A\")) ## # A tibble: 80 × 2\n##    fruit        capital_A   \n##    <chr>        <chr>       \n##  1 apple        Apple       \n##  2 apricot      Apricot     \n##  3 avocado      Avocado     \n##  4 banana       bAnana      \n##  5 bell pepper  bell pepper \n##  6 bilberry     bilberry    \n##  7 blackberry   blAckberry  \n##  8 blackcurrant blAckcurrant\n##  9 blood orange blood orAnge\n## 10 blueberry    blueberry   \n## # … with 70 more rows"
  },
  {
    "path": "wrangling.html",
    "id": "regular-expressions-with-stringr",
    "chapter": "2 Wrangling",
    "heading": "2.3.2 Regular expressions with stringr",
    "text": "Sometimes, string tasks expressed terms fixed string, can described terms pattern. Regular expressions, also know “regexes,” standard way specify patterns. regexes, specific characters constructs take special meaning order match multiple strings.explore regular expressions, use str_detect() function, reports TRUE string matches pattern, filter() see matches. example, fruits include “w” name.code , first metacharacter period . , stands single character, except newline (, way, represented \\n). regex b.r match fruits “b”, followed single character, followed “r”. Regexes case sensitive.Anchors can included express expression must occur within string. ^ indicates beginning string $ indicates end.",
    "code": "\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"w\"))## # A tibble: 4 × 1\n##   fruit     \n##   <chr>     \n## 1 honeydew  \n## 2 kiwi fruit\n## 3 strawberry\n## 4 watermelon\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"b.r\"))## # A tibble: 15 × 1\n##    fruit      \n##    <chr>      \n##  1 bilberry   \n##  2 blackberry \n##  3 blueberry  \n##  4 boysenberry\n##  5 cloudberry \n##  6 cranberry  \n##  7 cucumber   \n##  8 elderberry \n##  9 goji berry \n## 10 gooseberry \n## 11 huckleberry\n## 12 mulberry   \n## 13 raspberry  \n## 14 salal berry\n## 15 strawberry\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"^w\"))## # A tibble: 1 × 1\n##   fruit     \n##   <chr>     \n## 1 watermelon\ntbl_fruit |>\n  filter(str_detect(fruit, pattern = \"o$\"))## # A tibble: 5 × 1\n##   fruit    \n##   <chr>    \n## 1 avocado  \n## 2 mango    \n## 3 pamelo   \n## 4 pomelo   \n## 5 tamarillo"
  },
  {
    "path": "wrangling.html",
    "id": "factors",
    "chapter": "2 Wrangling",
    "heading": "2.4 Factors",
    "text": "Factors categorical variables take specified set values. manipulate factors use forcats package, core package Tidyverse.easy make factors either factor(), .factor() parse_factor().three options best depends situation. factor() useful creating factor nothing. .factor() best simple transformations, especially character variables, example. parse_factor() modern powerful three.Let’s use gapminder$continent example. Note str() useful function getting detailed information object.get frequency table tibble, tibble, use count(). get similar result free-range factor, use fct_count().",
    "code": "\ntibble(X = letters[1:3]) |> \n  mutate(fac_1 = factor(X)) |> \n  mutate(fac_2 = as.factor(X)) |> \n  mutate(fac_3 = parse_factor(X))## # A tibble: 3 × 4\n##   X     fac_1 fac_2 fac_3\n##   <chr> <fct> <fct> <fct>\n## 1 a     a     a     a    \n## 2 b     b     b     b    \n## 3 c     c     c     c\nstr(gapminder$continent)##  Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\nlevels(gapminder$continent)## [1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\"\nnlevels(gapminder$continent)## [1] 5\nclass(gapminder$continent)## [1] \"factor\"\ngapminder |> \n  count(continent)## # A tibble: 5 × 2\n##   continent     n\n##   <fct>     <int>\n## 1 Africa      624\n## 2 Americas    300\n## 3 Asia        396\n## 4 Europe      360\n## 5 Oceania      24\nfct_count(gapminder$continent)## # A tibble: 5 × 2\n##   f            n\n##   <fct>    <int>\n## 1 Africa     624\n## 2 Americas   300\n## 3 Asia       396\n## 4 Europe     360\n## 5 Oceania     24"
  },
  {
    "path": "wrangling.html",
    "id": "dropping-unused-levels",
    "chapter": "2 Wrangling",
    "heading": "2.4.1 Dropping unused levels",
    "text": "Removing rows corresponding specific factor level remove level . unused levels can come back haunt later, e.g., figure legends.Watch happens levels country filter gapminder handful countries.Even though h_gap data handful countries, still schlepping around levels original gapminder tibble.can get rid ? base function droplevels() operates factors data frame single factor. function fct_drop() operates single factor variable.",
    "code": "\nnlevels(gapminder$country)## [1] 142\nh_gap <- gapminder |>\n  filter(country %in% c(\"Egypt\", \"Haiti\", \n                        \"Romania\", \"Thailand\", \n                        \"Venezuela\"))\nnlevels(h_gap$country)## [1] 142\nh_gap_dropped <- h_gap |> \n  droplevels()\nnlevels(h_gap_dropped$country)## [1] 5\n# Use fct_drop() on a free-range factor\n\nh_gap$country |>\n  fct_drop() |>\n  levels()## [1] \"Egypt\"     \"Haiti\"     \"Romania\"   \"Thailand\"  \"Venezuela\""
  },
  {
    "path": "wrangling.html",
    "id": "change-the-order-of-the-levels",
    "chapter": "2 Wrangling",
    "heading": "2.4.2 Change the order of the levels",
    "text": "default, factor levels ordered alphabetically.can also order factors :Frequency: Make common level first .Another variable: Order factor levels according summary statistic another variable.Let’s order continent frequency using fct_infreq().can also frequency print backwards using fct_rev().two bar charts frequency continent differ order continents. prefer? show code just second one.Let’s now order country another variable, forwards backwards. variable usually quantitative order factor according grouped summary. factor grouping variable default summarizing function median() can specify something else.reorder factor levels? often makes plots much better! plotting factor numeric variable, generally good idea order factors function numeric variable. Alphabetic ordering rarely best.Compare interpretability two plots life expectancy Aericas 2007. difference order country factor. better?",
    "code": "\ngapminder$continent |>\n  levels()## [1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\"   \"Oceania\"\ngapminder$continent |> \n  fct_infreq() |>\n  levels()## [1] \"Africa\"   \"Asia\"     \"Europe\"   \"Americas\" \"Oceania\"\ngapminder$continent |> \n  fct_infreq() |>\n  fct_rev() |> \n  levels()## [1] \"Oceania\"  \"Americas\" \"Europe\"   \"Asia\"     \"Africa\"\ngapminder |> \n  mutate(continent = fct_infreq(continent)) |> \n  mutate(continent = fct_rev(continent)) |> \n  ggplot(aes(x = continent)) +\n    geom_bar() +\n    coord_flip()\n# Order countries by median life expectancy\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp) |> \n  levels() |> \n  head()## [1] \"Sierra Leone\"  \"Guinea-Bissau\" \"Afghanistan\"   \"Angola\"       \n## [5] \"Somalia\"       \"Guinea\"\n# Order according to minimum life exp instead of median\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp, min) |> \n  levels() |> \n  head()## [1] \"Rwanda\"       \"Afghanistan\"  \"Gambia\"       \"Angola\"       \"Sierra Leone\"\n## [6] \"Cambodia\"\n# Backwards!\n\nfct_reorder(gapminder$country, \n            gapminder$lifeExp, \n            .desc = TRUE) |> \n  levels() |> \n  head()## [1] \"Iceland\"     \"Japan\"       \"Sweden\"      \"Switzerland\" \"Netherlands\"\n## [6] \"Norway\"\ngapminder |> \n  filter(year == 2007, \n         continent == \"Americas\") |> \n  ggplot(aes(x = lifeExp, y = country)) + \n    geom_point()\ngapminder |> \n  filter(year == 2007, \n         continent == \"Americas\") |> \n  ggplot(aes(x = lifeExp, \n             y = fct_reorder(country, lifeExp))) + \n    geom_point()"
  },
  {
    "path": "wrangling.html",
    "id": "recode-the-levels",
    "chapter": "2 Wrangling",
    "heading": "2.4.3 Recode the levels",
    "text": "Use fct_recode() change names levels factor.",
    "code": "\ni_gap <- gapminder |> \n  filter(country %in% c(\"United States\", \"Sweden\", \n                        \"Australia\")) |> \n  droplevels()\n\ni_gap$country |> \n  levels()## [1] \"Australia\"     \"Sweden\"        \"United States\"\ni_gap$country |>\n  fct_recode(\"USA\" = \"United States\", \"Oz\" = \"Australia\") |> \n  levels()## [1] \"Oz\"     \"Sweden\" \"USA\""
  },
  {
    "path": "wrangling.html",
    "id": "date-times",
    "chapter": "2 Wrangling",
    "heading": "2.5 Date-Times",
    "text": "manipulate date-times using lubridate package lubridate makes easier work dates times R. lubridate part core tidyverse need ’re working dates/times.three types date/time data refer instant time:date. Tibbles print <date>.date. Tibbles print <date>.time within day. Tibbles print <time>.time within day. Tibbles print <time>.date-time date plus time. uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>.date-time date plus time. uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>.always use simplest possible data type works needs. means can use date instead date-time, . Date-times substantially complicated need handle time zones, ’ll come back end chapter.get current date date-time can use today() now():Otherwise, three ways ’re likely create date/time:string.individual date-time components.existing date/time object.work follows:",
    "code": "\ntoday()## [1] \"2022-07-24\"\nnow()## [1] \"2022-07-24 21:57:30 EDT\""
  },
  {
    "path": "wrangling.html",
    "id": "from-strings",
    "chapter": "2 Wrangling",
    "heading": "2.5.1 From strings",
    "text": "Date/time data often comes strings. lubridate functions automatically work format specify order component. First, figure want order year, month, day appear dates, arrange “y”, “m”, “d” accordingly. gives name lubridate function parse date. example:functions also take unquoted numbers. concise way create single date/time object, might need filtering date/time data. ymd() short unambiguous:ymd() friends create dates. create date-time, add underscore one “h”, “m”, “s” name parsing function:can also force creation date-time date supplying timezone:",
    "code": "\nymd(\"2017-01-31\")## [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")## [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")## [1] \"2017-01-31\"\nymd(20170131)## [1] \"2017-01-31\"\nymd_hms(\"2017-01-31 20:11:59\")## [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")## [1] \"2017-01-31 08:01:00 UTC\"\nymd(20170131, tz = \"UTC\")## [1] \"2017-01-31 UTC\""
  },
  {
    "path": "wrangling.html",
    "id": "from-individual-components",
    "chapter": "2 Wrangling",
    "heading": "2.5.2 From individual components",
    "text": "Instead single string, sometimes ’ll individual components date-time spread across multiple columns. flights data:create date/time sort input, use make_date() dates, make_datetime() date-times:",
    "code": "\nflights |> \n  select(year, month, day, hour, minute)## # A tibble: 336,776 × 5\n##     year month   day  hour minute\n##    <int> <int> <int> <dbl>  <dbl>\n##  1  2013     1     1     5     15\n##  2  2013     1     1     5     29\n##  3  2013     1     1     5     40\n##  4  2013     1     1     5     45\n##  5  2013     1     1     6      0\n##  6  2013     1     1     5     58\n##  7  2013     1     1     6      0\n##  8  2013     1     1     6      0\n##  9  2013     1     1     6      0\n## 10  2013     1     1     6      0\n## # … with 336,766 more rows\nflights |> \n  select(year, month, day, hour, minute) |> \n  mutate(departure = make_datetime(year, month, day, hour, minute))## # A tibble: 336,776 × 6\n##     year month   day  hour minute departure          \n##    <int> <int> <int> <dbl>  <dbl> <dttm>             \n##  1  2013     1     1     5     15 2013-01-01 05:15:00\n##  2  2013     1     1     5     29 2013-01-01 05:29:00\n##  3  2013     1     1     5     40 2013-01-01 05:40:00\n##  4  2013     1     1     5     45 2013-01-01 05:45:00\n##  5  2013     1     1     6      0 2013-01-01 06:00:00\n##  6  2013     1     1     5     58 2013-01-01 05:58:00\n##  7  2013     1     1     6      0 2013-01-01 06:00:00\n##  8  2013     1     1     6      0 2013-01-01 06:00:00\n##  9  2013     1     1     6      0 2013-01-01 06:00:00\n## 10  2013     1     1     6      0 2013-01-01 06:00:00\n## # … with 336,766 more rows"
  },
  {
    "path": "wrangling.html",
    "id": "from-other-types",
    "chapter": "2 Wrangling",
    "heading": "2.5.3 From other types",
    "text": "may want switch date-time date. ’s job as_datetime() as_date():Sometimes ’ll get date/times numeric offsets “Unix Epoch”, 1970-01-01. offset seconds, use as_datetime(); ’s days, use as_date().",
    "code": "\nas_datetime(today())## [1] \"2022-07-24 UTC\"\nas_date(now())## [1] \"2022-07-24\"\nas_datetime(60 * 60 * 10)## [1] \"1970-01-01 10:00:00 UTC\"\nas_date(365 * 10 + 2)## [1] \"1980-01-01\""
  },
  {
    "path": "wrangling.html",
    "id": "date-time-components",
    "chapter": "2 Wrangling",
    "heading": "2.5.4 Date-time components",
    "text": "Now know get date-time data R’s date-time data structures, let’s explore can . section focus accessor functions let get set individual components. next section look arithmetic works date-times.can pull individual parts date accessor functions year(), month(), mday() (day month), yday() (day year), wday() (day week), hour(), minute(), second().month() wday() can set label = TRUE return abbreviated name month day week. Set label = TRUE abbr = FALSE return full name.",
    "code": "\ndatetime <- ymd_hms(\"2016-07-08 12:34:56\")\nyear(datetime)## [1] 2016\nmonth(datetime)## [1] 7\nmday(datetime)## [1] 8\nyday(datetime)## [1] 190\nwday(datetime)## [1] 6\nmonth(datetime, label = TRUE)## [1] Jul\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nwday(datetime, label = TRUE, abbr = FALSE)## [1] Friday\n## 7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday"
  },
  {
    "path": "wrangling.html",
    "id": "setting-components",
    "chapter": "2 Wrangling",
    "heading": "2.5.5 Setting components",
    "text": "can create new date-time update().values big, roll-:",
    "code": "\nupdate(datetime, year = 2020, month = 2, mday = 2, hour = 2)## [1] \"2020-02-02 02:34:56 UTC\"\nymd(\"2015-02-01\") |> \n  update(mday = 30)## [1] \"2015-03-02\"\nymd(\"2015-02-01\") |> \n  update(hour = 400)## [1] \"2015-02-17 16:00:00 UTC\""
  },
  {
    "path": "wrangling.html",
    "id": "time-zones",
    "chapter": "2 Wrangling",
    "heading": "2.5.6 Time zones",
    "text": "Time zones enormously complicated topic interaction geopolitical entities. Fortunately, don’t need dig details ’re imperative data analysis. can see complete list possible timezones function OlsonNames(). Unless otherwise specified, lubridate always uses UTC (Coordinated Universal Time).R, time zone attribute date-time controls printing. example, three objects represent instant time:",
    "code": "\n(x1 <- ymd_hms(\"2015-06-01 12:00:00\", tz = \"America/New_York\"))## [1] \"2015-06-01 12:00:00 EDT\"\n(x2 <- ymd_hms(\"2015-06-01 18:00:00\", tz = \"Europe/Copenhagen\"))## [1] \"2015-06-01 18:00:00 CEST\"\n(x3 <- ymd_hms(\"2015-06-02 04:00:00\", tz = \"Pacific/Auckland\"))## [1] \"2015-06-02 04:00:00 NZST\""
  },
  {
    "path": "wrangling.html",
    "id": "combining-data",
    "chapter": "2 Wrangling",
    "heading": "2.6 Combining Data",
    "text": "many ways bring data together.\nFIGURE 2.3: Combining data often tricky.\nbind_rows() function used combine rows two tibbles.",
    "code": "\ndata_1 <- tibble(x = 1:2,                \n                 y = c(\"A\", \"B\")) \n\ndata_2 <- tibble(x = 3:4,\n                 y = c(\"C\", \"D\")) \n\n\nbind_rows(data_1, data_2)## # A tibble: 4 × 2\n##       x y    \n##   <int> <chr>\n## 1     1 A    \n## 2     2 B    \n## 3     3 C    \n## 4     4 D"
  },
  {
    "path": "wrangling.html",
    "id": "joins",
    "chapter": "2 Wrangling",
    "heading": "2.6.1 Joins",
    "text": "Consider two tibbles: superheroes publishers.Note easy use tribble() tibble package create tibble fly using text organized easy entry reading. Recall double colon — :: — indicate function comes specific package.",
    "code": "\nsuperheroes <- tibble::tribble(\n       ~name,   ~gender,     ~publisher,\n   \"Magneto\",   \"male\",       \"Marvel\",\n     \"Storm\",   \"female\",     \"Marvel\",\n    \"Batman\",   \"male\",       \"DC\",\n  \"Catwoman\",   \"female\",     \"DC\",\n   \"Hellboy\",   \"male\",       \"Dark Horse Comics\"\n  )\n\npublishers <- tibble::tribble(\n  ~publisher, ~yr_founded,\n        \"DC\",       1934L,\n    \"Marvel\",       1939L,\n     \"Image\",       1992L\n  )"
  },
  {
    "path": "wrangling.html",
    "id": "inner_join",
    "chapter": "2 Wrangling",
    "heading": "2.6.1.1 inner_join()",
    "text": "inner_join(x, y): Returns rows x matching values y, columns x y. multiple matches x y, combination matches returned.\nFIGURE 2.4: Inner join.\nlose Hellboy join , although appears x = superheroes, publisher Dark Horse Comics appear y = publishers. join result variables x = superheroes plus yr_founded, y.Note message ‘Joining, = “publisher”’. Whenever joining, R checks see variables common two tibbles , , uses join. However, concerned may aware , R tells . messages annoying signal made code robust . Fortunately, can specify precisely variables want join . Always .also takes vector key variables want merge multiple variables.Now compare result using inner_join() two datasets opposite positions.way, illustrate multiple matches, think x = publishers direction. Every publisher match y = superheroes appears multiple times result, match. fact, ’re getting result inner_join(superheroes, publishers), variable order (also never rely analysis).",
    "code": "\ninner_join(superheroes, publishers)## Joining, by = \"publisher\"## # A tibble: 4 × 4\n##   name     gender publisher yr_founded\n##   <chr>    <chr>  <chr>          <int>\n## 1 Magneto  male   Marvel          1939\n## 2 Storm    female Marvel          1939\n## 3 Batman   male   DC              1934\n## 4 Catwoman female DC              1934\ninner_join(superheroes, publishers, by = \"publisher\")\ninner_join(publishers, superheroes, by = \"publisher\")## # A tibble: 4 × 4\n##   publisher yr_founded name     gender\n##   <chr>          <int> <chr>    <chr> \n## 1 DC              1934 Batman   male  \n## 2 DC              1934 Catwoman female\n## 3 Marvel          1939 Magneto  male  \n## 4 Marvel          1939 Storm    female"
  },
  {
    "path": "wrangling.html",
    "id": "full_join",
    "chapter": "2 Wrangling",
    "heading": "2.6.1.2 full_join()",
    "text": "full_join(x, y): Returns rows columns x y. matching values, returns NA one missing.get rows x = superheroes plus new row y = publishers, containing publisher Image. get variables x = superheroes variables y = publishers. row derives solely one table carries NAs variables found table.full_join() returns rows columns x y, result full_join(x = superheroes, y = publishers) match full_join(x = publishers, y = superheroes).",
    "code": "\nfull_join(superheroes, publishers, by = \"publisher\")## # A tibble: 6 × 4\n##   name     gender publisher         yr_founded\n##   <chr>    <chr>  <chr>                  <int>\n## 1 Magneto  male   Marvel                  1939\n## 2 Storm    female Marvel                  1939\n## 3 Batman   male   DC                      1934\n## 4 Catwoman female DC                      1934\n## 5 Hellboy  male   Dark Horse Comics         NA\n## 6 <NA>     <NA>   Image                   1992"
  },
  {
    "path": "wrangling.html",
    "id": "left_join",
    "chapter": "2 Wrangling",
    "heading": "2.6.1.3 left_join()",
    "text": "left_join(x, y): Returns rows x, columns x y. multiple matches x y, combination matches returned.basically get x = superheroes back, additional variable yr_founded, unique y = publishers. Hellboy, whose publisher appear y = publishers, NA yr_founded.Now compare result running left_join(x = publishers, y = superheroes). Unlike inner_join() full_join() order arguments significant effect resulting tibble.get similar result inner_join(), publisher Image survives join, even though superheroes Image appear y = superheroes. result, Image NAs name gender.similar function, right_join(x, y) returns rows y, columns x y.",
    "code": "\nleft_join(superheroes, publishers, by = \"publisher\")## # A tibble: 5 × 4\n##   name     gender publisher         yr_founded\n##   <chr>    <chr>  <chr>                  <int>\n## 1 Magneto  male   Marvel                  1939\n## 2 Storm    female Marvel                  1939\n## 3 Batman   male   DC                      1934\n## 4 Catwoman female DC                      1934\n## 5 Hellboy  male   Dark Horse Comics         NA\nleft_join(publishers, superheroes, by = \"publisher\")## # A tibble: 5 × 4\n##   publisher yr_founded name     gender\n##   <chr>          <int> <chr>    <chr> \n## 1 DC              1934 Batman   male  \n## 2 DC              1934 Catwoman female\n## 3 Marvel          1939 Magneto  male  \n## 4 Marvel          1939 Storm    female\n## 5 Image           1992 <NA>     <NA>"
  },
  {
    "path": "wrangling.html",
    "id": "semi_join",
    "chapter": "2 Wrangling",
    "heading": "2.6.1.4 semi_join()",
    "text": "semi_join(x, y): Returns rows x matching values y, keeping just columns x. semi join differs inner join inner join return one row x matching row y, whereas semi join never duplicate rows x. filtering join.Compare result switching values arguments.Now effects switching x y roles clear. result resembles x = publishers, publisher Image lost, since observations publisher == \"Image\" y = superheroes.",
    "code": "\nsemi_join(superheroes, publishers, by = \"publisher\")## # A tibble: 4 × 3\n##   name     gender publisher\n##   <chr>    <chr>  <chr>    \n## 1 Magneto  male   Marvel   \n## 2 Storm    female Marvel   \n## 3 Batman   male   DC       \n## 4 Catwoman female DC\nsemi_join(x = publishers, y = superheroes, by = \"publisher\")## # A tibble: 2 × 2\n##   publisher yr_founded\n##   <chr>          <int>\n## 1 DC              1934\n## 2 Marvel          1939"
  },
  {
    "path": "wrangling.html",
    "id": "anti_join",
    "chapter": "2 Wrangling",
    "heading": "2.6.1.5 anti_join()",
    "text": "anti_join(x, y): Return rows x matching values y, keeping just columns x.keep Hellboy now.Now switch arguments compare result.keep publisher Image now (variables found x = publishers).",
    "code": "\nanti_join(superheroes, publishers, by = \"publisher\")## # A tibble: 1 × 3\n##   name    gender publisher        \n##   <chr>   <chr>  <chr>            \n## 1 Hellboy male   Dark Horse Comics\nanti_join(publishers, superheroes, by = \"publisher\")## # A tibble: 1 × 2\n##   publisher yr_founded\n##   <chr>          <int>\n## 1 Image           1992"
  },
  {
    "path": "wrangling.html",
    "id": "example",
    "chapter": "2 Wrangling",
    "heading": "2.6.2 Example",
    "text": "Consider relationships among tibbles nycflights13 package:\nFIGURE 2.5: Relationships among nycflights tables\nflights airlines data frames, key variable want join/merge/match rows name: carrier. Let’s use inner_join() join two data frames, rows matched variable carrierThis first example using join function “pipe.” flights fed first argument inner_join(). pipe . symbol created combining vertical line | greater-symbol ‘>’. Cmd-Shift-M shortcut key inserting pipe.code equivalent :airports data frame contains airport codes airport:However, look airports flights data frames, ’ll find airport codes variables different names. airports airport code faa, whereas flights airport codes origin dest.order join two data frames airport code, inner_join() operation use = c(\"dest\" = \"faa\") thereby allowing us join two data frames key variable different name.Let’s construct chain pipe operators |> computes number flights NYC destination, also includes information destination airport:\"ORD\" airport code Chicago O’Hare airport \"FLL\" code main airport Fort Lauderdale, Florida, can seen airport_name variable.",
    "code": "\nflights |> \n  inner_join(airlines, by = \"carrier\")## # A tibble: 336,776 × 20\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      544            545        -1     1004           1022\n##  5  2013     1     1      554            600        -6      812            837\n##  6  2013     1     1      554            558        -4      740            728\n##  7  2013     1     1      555            600        -5      913            854\n##  8  2013     1     1      557            600        -3      709            723\n##  9  2013     1     1      557            600        -3      838            846\n## 10  2013     1     1      558            600        -2      753            745\n## # … with 336,766 more rows, and 12 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   name <chr>\ninner_join(flights, airlines, by = \"carrier\")\nairports## # A tibble: 1,458 × 8\n##    faa   name                             lat    lon   alt    tz dst   tzone    \n##    <chr> <chr>                          <dbl>  <dbl> <dbl> <dbl> <chr> <chr>    \n##  1 04G   Lansdowne Airport               41.1  -80.6  1044    -5 A     America/…\n##  2 06A   Moton Field Municipal Airport   32.5  -85.7   264    -6 A     America/…\n##  3 06C   Schaumburg Regional             42.0  -88.1   801    -6 A     America/…\n##  4 06N   Randall Airport                 41.4  -74.4   523    -5 A     America/…\n##  5 09J   Jekyll Island Airport           31.1  -81.4    11    -5 A     America/…\n##  6 0A9   Elizabethton Municipal Airport  36.4  -82.2  1593    -5 A     America/…\n##  7 0G6   Williams County Airport         41.5  -84.5   730    -5 A     America/…\n##  8 0G7   Finger Lakes Regional Airport   42.9  -76.8   492    -5 A     America/…\n##  9 0P2   Shoestring Aviation Airfield    39.8  -76.6  1000    -5 U     America/…\n## 10 0S9   Jefferson County Intl           48.1 -123.    108    -8 A     America/…\n## # … with 1,448 more rows\nflights |> \n  inner_join(airports, by = c(\"dest\" = \"faa\"))## # A tibble: 329,174 × 26\n##     year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n##    <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n##  1  2013     1     1      517            515         2      830            819\n##  2  2013     1     1      533            529         4      850            830\n##  3  2013     1     1      542            540         2      923            850\n##  4  2013     1     1      554            600        -6      812            837\n##  5  2013     1     1      554            558        -4      740            728\n##  6  2013     1     1      555            600        -5      913            854\n##  7  2013     1     1      557            600        -3      709            723\n##  8  2013     1     1      557            600        -3      838            846\n##  9  2013     1     1      558            600        -2      753            745\n## 10  2013     1     1      558            600        -2      849            851\n## # … with 329,164 more rows, and 18 more variables: arr_delay <dbl>,\n## #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n## #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>,\n## #   name <chr>, lat <dbl>, lon <dbl>, alt <dbl>, tz <dbl>, dst <chr>,\n## #   tzone <chr>\nflights |>\n  group_by(dest) |>\n  summarize(num_flights = n(),\n            .groups = \"drop\") |>\n  arrange(desc(num_flights)) |>\n  inner_join(airports, by = c(\"dest\" = \"faa\")) |>\n  rename(airport_name = name)## # A tibble: 101 × 9\n##    dest  num_flights airport_name             lat    lon   alt    tz dst   tzone\n##    <chr>       <int> <chr>                  <dbl>  <dbl> <dbl> <dbl> <chr> <chr>\n##  1 ORD         17283 Chicago Ohare Intl      42.0  -87.9   668    -6 A     Amer…\n##  2 ATL         17215 Hartsfield Jackson At…  33.6  -84.4  1026    -5 A     Amer…\n##  3 LAX         16174 Los Angeles Intl        33.9 -118.    126    -8 A     Amer…\n##  4 BOS         15508 General Edward Lawren…  42.4  -71.0    19    -5 A     Amer…\n##  5 MCO         14082 Orlando Intl            28.4  -81.3    96    -5 A     Amer…\n##  6 CLT         14064 Charlotte Douglas Intl  35.2  -80.9   748    -5 A     Amer…\n##  7 SFO         13331 San Francisco Intl      37.6 -122.     13    -8 A     Amer…\n##  8 FLL         12055 Fort Lauderdale Holly…  26.1  -80.2     9    -5 A     Amer…\n##  9 MIA         11728 Miami Intl              25.8  -80.3     8    -5 A     Amer…\n## 10 DCA          9705 Ronald Reagan Washing…  38.9  -77.0    15    -5 A     Amer…\n## # … with 91 more rows"
  },
  {
    "path": "wrangling.html",
    "id": "tidy-data",
    "chapter": "2 Wrangling",
    "heading": "2.7 Tidy data",
    "text": "Consider first five rows drinks data frame fivethirtyeight package:reading help file running ?drinks, ’ll see drinks data frame contains results survey average number servings beer, spirits, wine consumed 193 countries. data originally reported FiveThirtyEight.com Mona Chalabi’s article: “Dear Mona Followup: People Drink Beer, Wine Spirits?”.Let’s apply data wrangling verbs drinks data frame:filter() drinks data frame consider 4 countries: United States, China, Italy, Saudi Arabia, thenselect() columns except total_litres_of_pure_alcohol using - sign, thenrename() variables beer_servings, spirit_servings, wine_servings beer, spirit, wine, respectively.save resulting data frame drinks_smaller.",
    "code": "## # A tibble: 5 × 5\n##   country     beer_servings spirit_servings wine_servings total_litres_of_pure_…\n##   <chr>               <int>           <int>         <int>                  <dbl>\n## 1 Afghanistan             0               0             0                    0  \n## 2 Albania                89             132            54                    4.9\n## 3 Algeria                25               0            14                    0.7\n## 4 Andorra               245             138           312                   12.4\n## 5 Angola                217              57            45                    5.9\ndrinks_smaller <- drinks |>\n  filter(country %in% c(\"USA\", \"China\", \"Italy\", \"Saudi Arabia\")) |>\n  select(-total_litres_of_pure_alcohol) |>\n  rename(beer = beer_servings, spirit = spirit_servings, wine = wine_servings)\ndrinks_smaller## # A tibble: 4 × 4\n##   country       beer spirit  wine\n##   <chr>        <int>  <int> <int>\n## 1 China           79    192     8\n## 2 Italy           85     42   237\n## 3 Saudi Arabia     0      5     0\n## 4 USA            249    158    84"
  },
  {
    "path": "wrangling.html",
    "id": "definition-of-tidy-data",
    "chapter": "2 Wrangling",
    "heading": "2.7.1 Definition of “tidy” data",
    "text": "mean data “tidy”? “tidy” clear English meaning “organized,” word “tidy” data science using R means data follows standardized format.“Tidy” data standard way mapping meaning dataset structure. dataset messy tidy depending rows, columns tables matched observations, variables types. tidy data:variable forms column.observation forms row.type observational unit forms table.",
    "code": ""
  },
  {
    "path": "wrangling.html",
    "id": "converting-to-tidy-data",
    "chapter": "2 Wrangling",
    "heading": "2.7.2 Converting to “tidy” data",
    "text": "book far, ’ve seen data frames already “tidy” format. Furthermore, rest book, ’ll mostly see data frames already tidy well. always case, however, datasets world. original data frame wide (non-“tidy”) format like use ggplot2 dplyr packages, first convert “tidy” format. , recommend using pivot_longer() function tidyr package (Wickham Girlich 2022).Going back drinks_smaller data frame earlier:tidy using pivot_longer() function tidyr package follows:Let’s dissect arguments pivot_longer().first argumentnames_to corresponds name variable new “tidy”/long data frame contain column names original data. Observe set names_to = \"type\". resulting drinks_smaller_tidy, column type contains three types alcohol beer, spirit, wine. Since type variable name doesn’t appear drinks_smaller, use quotation marks around . ’ll receive error just use names_to = type .first argumentnames_to corresponds name variable new “tidy”/long data frame contain column names original data. Observe set names_to = \"type\". resulting drinks_smaller_tidy, column type contains three types alcohol beer, spirit, wine. Since type variable name doesn’t appear drinks_smaller, use quotation marks around . ’ll receive error just use names_to = type .second argument values_to corresponds name variable new “tidy” data frame contain values original data. Observe set values_to = \"servings\" since numeric value beer, wine, spirit columns drinks_smaller corresponds value servings. resulting drinks_smaller_tidy, column servings contains 4 \\(\\times\\) 3 = 12 numerical values. Note servings doesn’t appear variable drinks_smaller needs quotation marks around values_to argument.second argument values_to corresponds name variable new “tidy” data frame contain values original data. Observe set values_to = \"servings\" since numeric value beer, wine, spirit columns drinks_smaller corresponds value servings. resulting drinks_smaller_tidy, column servings contains 4 \\(\\times\\) 3 = 12 numerical values. Note servings doesn’t appear variable drinks_smaller needs quotation marks around values_to argument.third argument cols columns drinks_smaller data frame either want don’t want “tidy.” Observe set -country indicating don’t want “tidy” country variable drinks_smaller rather beer, spirit, wine. Since country column appears drinks_smaller don’t put quotation marks around .third argument cols columns drinks_smaller data frame either want don’t want “tidy.” Observe set -country indicating don’t want “tidy” country variable drinks_smaller rather beer, spirit, wine. Since country column appears drinks_smaller don’t put quotation marks around .third argument cols little nuanced, let’s consider code written slightly differently produces output:Note third argument now specifies columns want “tidy” c(beer, spirit, wine), instead columns don’t want “tidy” using -country. use c() function create vector columns drinks_smaller ’d like “tidy.” Note since three columns appear one another drinks_smaller data frame, can also following cols argument:Converting “wide” format data “tidy” format often confuses new R users. way get comfortable pivot_longer() function practice, practice, practice using different datasets. example, run ?pivot_longer look examples bottom help file., however, want convert “tidy” data frame “wide” format, need use pivot_wider() function instead. Run ?pivot_wider look examples bottom help file examples.can also view examples pivot_longer() pivot_wider() tidyverse.org webpage. ’s nice example check different functions available data tidying case study using data World Health Organization webpage. Furthermore, week R4DS Online Learning Community posts dataset weekly #TidyTuesday event might serve nice place find data explore transform.",
    "code": "\ndrinks_smaller## # A tibble: 4 × 4\n##   country       beer spirit  wine\n##   <chr>        <int>  <int> <int>\n## 1 China           79    192     8\n## 2 Italy           85     42   237\n## 3 Saudi Arabia     0      5     0\n## 4 USA            249    158    84\ndrinks_smaller_tidy <- drinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = -country)\ndrinks_smaller_tidy## # A tibble: 12 × 3\n##    country      type   servings\n##    <chr>        <chr>     <int>\n##  1 China        beer         79\n##  2 China        spirit      192\n##  3 China        wine          8\n##  4 Italy        beer         85\n##  5 Italy        spirit       42\n##  6 Italy        wine        237\n##  7 Saudi Arabia beer          0\n##  8 Saudi Arabia spirit        5\n##  9 Saudi Arabia wine          0\n## 10 USA          beer        249\n## 11 USA          spirit      158\n## 12 USA          wine         84\ndrinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = c(beer, spirit, wine))\ndrinks_smaller |> \n  pivot_longer(names_to = \"type\", \n               values_to = \"servings\", \n               cols = beer:wine)"
  },
  {
    "path": "wrangling.html",
    "id": "other-commands",
    "chapter": "2 Wrangling",
    "heading": "2.8 Other commands",
    "text": "topics prove important later Primer.",
    "code": ""
  },
  {
    "path": "wrangling.html",
    "id": "random-variables",
    "chapter": "2 Wrangling",
    "heading": "2.8.1 Random variables",
    "text": "",
    "code": ""
  },
  {
    "path": "wrangling.html",
    "id": "sample",
    "chapter": "2 Wrangling",
    "heading": "2.8.1.1 sample()",
    "text": "common distributions work empirical frequency distributions, values age trains tibble, values poverty kenya tibble, . can also create data making “draws” distribution concocted.Consider distribution possible values rolling fair die. can use sample() function create draws distribution, meaning change (sometimes stay ) every subsequent draw.produces one “draw” distribution possible values one roll fair six-sided die.Now, suppose wanted roll die 10 times. One arguments sample() function replace. must specify TRUE values can appear . Since, rolling die 10 times, expect value like 3 can appear , need set replace = TRUE.words, rolling 1 first roll preclude rolling 1 later roll.die “fair,” meaning sides likely appear others? final argument sample() function prob argument. takes vector (length initial vector x) contains probabilities selecting one elements x. Suppose probability rolling 1 0.5, probability rolling value 0.1. (probabilities sum 1. don’t sample() automatically re-scale .)Remember: real data . actually rolled die. just made assumptions happen roll die. assumptions built urn — data generating mechanism — can draw many values like. Let’s roll unfair die 10,000 times.sample() just one many functions creating draws — , colloquially, “drawing” — distribution. Three important functions : runif(), rbinom(), rnorm().",
    "code": "\ndie <- c(1, 2, 3, 4, 5, 6)\n\nsample(x = die, size = 1)## [1] 3\nsample(x = die, size = 10, replace = TRUE)##  [1] 5 6 3 3 3 4 3 6 4 5\nsample(x = die, \n       size = 10, \n       replace = TRUE, \n       prob = c(0.5, 0.1, 0.1, 0.1, 0.1, 0.1))##  [1] 1 1 3 1 2 1 1 6 1 1\ntibble(result = sample(x = die, \n                       size = 10000, \n                       replace = TRUE, \n                       prob = c(0.5, rep(0.1, 5)))) |> \n  ggplot(aes(x = result)) +\n    geom_bar() +\n    labs(title = \"Distribution of Results of an Unfair Die\",\n         x = \"Result of One Roll\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 1:6,\n                       labels = as.character(1:6)) +\n    scale_y_continuous(labels = scales::comma_format())"
  },
  {
    "path": "wrangling.html",
    "id": "runif",
    "chapter": "2 Wrangling",
    "heading": "2.8.1.2 runif()",
    "text": "Consider “uniform” distribution. case every outcome range possible outcomes chance occurring. function runif() (spoken “r-unif”) enables us draw uniform contribution. runif() three arguments: n, min, max. runif() produce n draws min max, value equal chance occurring.Mathematically, notate:\\[y_i \\sim U(4, 6)\\],means value \\(y\\) drawn uniform distribution four six.",
    "code": "\nrunif(n = 10, min = 4, max = 6)##  [1] 5.6 5.0 4.6 5.7 4.9 5.2 4.3 5.6 4.8 5.6"
  },
  {
    "path": "wrangling.html",
    "id": "rbinom",
    "chapter": "2 Wrangling",
    "heading": "2.8.1.3 rbinom()",
    "text": "Consider binomial distribution, case probability Boolean variable (instance success failure) calculated repeated, independent trials. One common example probability flipping coin landing heads. function rbinom() allows us draw binomial distribution. function takes three arguments, n, size, prob.n number values seek draw.\nsize number trials n.\n*prob probability success trial.Suppose wanted flip fair coin one time, let landing heads represent success.thing 100 times:graph , use function scale_x_continuous() x-axis variable continuous, meaning can take real values. breaks argument scale_x_continuous() converts x-axis two different “tick marks”. fairly even distribution Tails Heads. draws typically result even equal split.Randomness creates (inevitable) tension distribution “thing” distribution vector draws thing. case, vector draws balanced Tails Heads. Yet, “know” since coin , definition, fair. sense, mathematics require even split. Yet, randomness means vector draws rarely match mathematically “true” result. OK! First, randomness intrinsic property real world. Second, can make effect randomness small want increasing number draws.Suppose instead wanted simulate unfair coin, probability landing Heads 0.75 instead 0.25.distribution — imaginary urn — draw results coin flip fair coin different distribution — different imaginary urn — distribution biased coin. fact, infinite number distributions. Yet long can draw values distribution, can work . Mathematics:\\[y_i \\sim B(n, p)\\].value \\(y\\) drawn binomial distribution parameters \\(n\\) number trials \\(p\\) probability success.Instead n consisting single trial, situation , 10,000 times, flipping coin 10 times summing , experiment, number heads. case:",
    "code": "\nrbinom(n = 1 , size = 1, prob = 0.5)## [1] 0\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.5)) |> \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\ntibble(heads = rbinom(n = 100, size = 1, prob = 0.75)) |> \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 100 Times\",\n         x = \"Result\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = c(0, 1),\n                       labels = c(\"Tails\", \"Heads\"))\nset.seed(9)\ntibble(heads = rbinom(n = 10000, size = 10, prob = 0.5)) |> \n  ggplot(aes(x = heads)) +\n    geom_bar() +\n    labs(title = \"Flipping a Fair Coin 10 Times\",\n         subtitle = \"Extreme results are possible with enough experiments\",\n         x = \"Total Number of Heads in Ten Flips\",\n         y = \"Count\") +\n    scale_x_continuous(breaks = 0:10)"
  },
  {
    "path": "wrangling.html",
    "id": "normal",
    "chapter": "2 Wrangling",
    "heading": "2.8.1.4 rnorm()",
    "text": "important distribution normal distribution. Mathematics:\\[y_i \\sim N(\\mu, \\sigma^2)\\].value \\(y_i\\) drawn normal distribution parameters \\(\\mu\\) mean \\(\\sigma\\) standard deviation.bell-shaped distribution defined two parameters: (1) mean \\(\\mu\\) (spoken “mu”) locates center distribution (2) standard deviation \\(\\sigma\\) (spoken “sigma”) determines variation values around center. figure , plot three normal distributions :solid normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 2\\).dotted normal curve mean \\(\\mu = 5\\) & standard deviation \\(\\sigma = 5\\).dashed normal curve mean \\(\\mu = 15\\) & standard deviation \\(\\sigma = 2\\).\nFIGURE 2.6: Three normal distributions.\nNotice solid dotted line normal curves center due common mean \\(\\mu\\) = 5. However, dotted line normal curve wider due larger standard deviation \\(\\sigma = 5\\). hand, solid dashed line normal curves variation due common standard deviation \\(\\sigma = 2\\). However, centered different locations.mean \\(\\mu = 0\\) standard deviation \\(\\sigma = 1\\), normal distribution special name. ’s called standard normal distribution \\(z\\)-curve.Furthermore, variable follows normal curve, three rules thumb can use:68% values lie within \\(\\pm\\) 1 standard deviation mean.95% values lie within \\(\\pm\\) 1.96 \\(\\approx\\) 2 standard deviations mean.99.7% values lie within \\(\\pm\\) 3 standard deviations mean.Let’s illustrate standard normal curve. dashed lines -3, -1.96, -1, 0, 1, 1.96, 3. 7 lines cut x-axis 8 segments. areas normal curve 8 segments marked add 100%. example:middle two segments represent interval -1 1. shaded area interval represents 34% + 34% = 68% area curve. words, 68% values.middle four segments represent interval -1.96 1.96. shaded area interval represents 13.5% + 34% + 34% + 13.5% = 95% area curve. words, 95% values.middle six segments represent interval -3 3. shaded area interval represents 2.35% + 13.5% + 34% + 34% + 13.5% + 2.35% = 99.7% area curve. words, 99.7% values.\nFIGURE 2.7: Rules thumb areas normal curves.\n",
    "code": ""
  },
  {
    "path": "wrangling.html",
    "id": "combinations",
    "chapter": "2 Wrangling",
    "heading": "2.8.2 Combinations",
    "text": "often need create tibble possible combinations two variables. expand_grid() easiest approach.crossing() something similar input variables already tibble.",
    "code": "\nexpand_grid(x = c(\"A\", \"B\"), y = c(1, 2, 3, 4))## # A tibble: 8 × 2\n##   x         y\n##   <chr> <dbl>\n## 1 A         1\n## 2 A         2\n## 3 A         3\n## 4 A         4\n## 5 B         1\n## 6 B         2\n## 7 B         3\n## 8 B         4"
  },
  {
    "path": "wrangling.html",
    "id": "matrices",
    "chapter": "2 Wrangling",
    "heading": "2.8.3 Matrices",
    "text": "Recall “matrix” R rectangular array data, shaped like data frame tibble, containing one type data, e.g., numeric. Large matrices also print ugly. (differences, none care .) Example:easiest way pull information matrix use [ ], subset operator. grab second third columns m:Note, however, matrices just one dimension “collapse” single vectors:Tibbles, hand, always maintain rectangular shapes, even one column row.can turn matrices tibbles as_tibble().m column names, as_tibble() creates won variables names, using “V” variable.",
    "code": "\nm <- matrix(c(3, 4, 8, 9, 12, 13, 0, 15, -1), ncol = 3)\nm##      [,1] [,2] [,3]\n## [1,]    3    9    0\n## [2,]    4   12   15\n## [3,]    8   13   -1\nm[, 2:3]##      [,1] [,2]\n## [1,]    9    0\n## [2,]   12   15\n## [3,]   13   -1\nm[, 2]## [1]  9 12 13\nm |> \n  as_tibble()## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\n## Using compatibility `.name_repair`.\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.## # A tibble: 3 × 3\n##      V1    V2    V3\n##   <dbl> <dbl> <dbl>\n## 1     3     9     0\n## 2     4    12    15\n## 3     8    13    -1"
  },
  {
    "path": "wrangling.html",
    "id": "missing-values",
    "chapter": "2 Wrangling",
    "heading": "2.8.4 Missing Values",
    "text": "observations tibble blank. called missing values, often marked NA. can create tibble follows:presence NA values can problematic.Fortunately, R functions take argument, na.rm, , set TRUE, removes NA values calculations.Another approach use drop_na().However, careful! use drop_na() without specific variable provided, remove rows missing value variable tibble.final approach use .na() explicitly determine value missing.",
    "code": "\ntbl <- tribble(\n  ~ a, ~ b, ~ c,\n    2,   3,   5,\n    4,  NA,   8,\n   NA,   7,   9,\n)\n\ntbl## # A tibble: 3 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\n## 2     4    NA     8\n## 3    NA     7     9\ntbl |> \n  summarize(avg_a = mean(a))## # A tibble: 1 × 1\n##   avg_a\n##   <dbl>\n## 1    NA\ntbl |> \n  summarize(avg_a = mean(a, na.rm = TRUE))## # A tibble: 1 × 1\n##   avg_a\n##   <dbl>\n## 1     3\ntbl |> \n  drop_na(a)## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\n## 2     4    NA     8\ntbl |> \n  drop_na()## # A tibble: 1 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\ntbl |> \n  mutate(a_missing = is.na(a))## # A tibble: 3 × 4\n##       a     b     c a_missing\n##   <dbl> <dbl> <dbl> <lgl>    \n## 1     2     3     5 FALSE    \n## 2     4    NA     8 FALSE    \n## 3    NA     7     9 TRUE"
  },
  {
    "path": "wrangling.html",
    "id": "working-by-rows",
    "chapter": "2 Wrangling",
    "heading": "2.8.5 Working by rows",
    "text": "Tibbles main Tidyverse functions designed work columns. something values variable . , sometimes, want work across tibble, comparing value first row value b first row, . , need two tricks. First, use rowwise() inform R next set commands executed across rows.Note “# Rowwise:” printed . set pipe work across rows, need pass c_across() whichever function using, generally specifying variables want use. don’t provide arguments c_across(), use columns tibble.",
    "code": "\ntbl |> \n  rowwise()## # A tibble: 3 × 3\n## # Rowwise: \n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     2     3     5\n## 2     4    NA     8\n## 3    NA     7     9\ntbl |> \n  rowwise() |> \n  mutate(sum_a_c = sum(c_across(c(a, c)))) |> \n  mutate(largest = max(c_across())) |> \n  mutate(largest_na = max(c_across(), na.rm = TRUE))## # A tibble: 3 × 6\n## # Rowwise: \n##       a     b     c sum_a_c largest largest_na\n##   <dbl> <dbl> <dbl>   <dbl>   <dbl>      <dbl>\n## 1     2     3     5       7       7          7\n## 2     4    NA     8      12      NA         12\n## 3    NA     7     9      NA      NA          9"
  },
  {
    "path": "wrangling.html",
    "id": "using-skim",
    "chapter": "2 Wrangling",
    "heading": "2.8.6 Using skim()",
    "text": "skimr package offers useful function known skim() function, allowing get valuable information data set one glance. similar glimpse() function, ’s little detailed offers preliminary analysis topic.Let’s try skimming nhanes dataset.TABLE 2.1: Data summaryVariable type: characterVariable type: factorVariable type: numericThe skim() function provides information mean, number unique counts, even bar graphs distribution data. information extremely useful allows us easily find things data give us starting point. example, 7000 missing values pregnancies variable. means ’re going run drop_na() can ignore 7000 missing values.running skim() can create starting point, analysis creation graph.",
    "code": "\nskim(nhanes)"
  },
  {
    "path": "wrangling.html",
    "id": "summary-3",
    "chapter": "2 Wrangling",
    "heading": "2.9 Summary",
    "text": "Data science data cleaning.Real data nasty.chapter covered many, many commands. memorized now.! ridiculous. don’t memorized. ? point chapter give tour can R . information, base try solve problems encounter future.key data science concept chapter , , idea “distribution.” word distribution used two different ways. First, distribution invisible object can never see touch. imaginary urn can take draws. special cases ever able “know” distribution , mainly cases physical process, like roulette wheel, can inspect case assumed mathematical formula. almost real world data science problems, “distribution” mental creation whose reality can never confirm.second way word distribution used refer vector values, variable R tibble. 115 ages trains distribution 1,000 draws rnorm().Whether “distribution” means imaginary object vector numbers drawn imaginary object depends context.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "data",
    "chapter": "3 Data",
    "heading": "3 Data",
    "text": "can never look data much. – Mark Engerman",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "introduction",
    "chapter": "3 Data",
    "heading": "3.1 Introduction",
    "text": "Getting data R major part real world data science project. multiple data formats use transport data, positives negatives.Start loading packages need chapter.can find files use chapter . can also access files saving URL R using can download access files located internet.",
    "code": "\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(dbplyr)\nlibrary(janitor)\ngithub_url <- \"https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/tutorials/031-data-files/data/\""
  },
  {
    "path": "data.html",
    "id": "reading-and-writing-files",
    "chapter": "3 Data",
    "heading": "3.2 Reading and writing files",
    "text": "\nFIGURE 3.1: Choose file formats wisely.\nfirst method can use import data using file. ’ve likely downloaded files , whether ’s game’s EXE file, image’s JPG file, essay’s PDF file. core, files just data. JPG file bunch data colors image PDF file bunch data text. can use files store data experiments surveys can analyze data later share data people.section, ’ll going common file formats can pull data files R create new files can share people.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "text-files",
    "chapter": "3 Data",
    "heading": "3.2.1 Text files",
    "text": "common type data text file “CSV,” stands comma separated value. words, CSV files files whose values separated commas. comma csv file corresponds column, column names , default, taken first line file.CSV files (counterparts) easily transferable computers programming languages extremely simple ’re effectively just text files special format. Additionally, ’re easy parse small amounts data ’re easily readable humans computers.\nHowever, due simple nature, CSV files able move basic data text values. also poor support special characters like commas, can make dataset harder organize understand adding new column specific entries. make CSV files good sharing data computers languages, efficient transporting saving large amounts data.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "reading-and-writing-from-a-csv-file",
    "chapter": "3 Data",
    "heading": "3.2.1.1 Reading and writing from a CSV file",
    "text": "’s example CSV file looks like. Use read_csv() readr package — one main packages within tidyverse collection packages — load data R. file argument file path CSV file.Use write_csv() save tibble csv file. write_csv() two main arguments: x file. x argument data set want save. file argument file path want save file. end file argument name want use file.read csv file , data shows . useful saving information share projects.want remove file system, use file.remove().Sometimes, CSV files want. Maybe wrong column names, information top file, comments interspersed within file.",
    "code": "\n# We can access files either by using their URL or their file path. In this\n# case, we're using the GitHub URL to access the database. This is done by\n# pasting the file name into our URL using the paste0() command.\n\nfile_1 <- paste0(github_url, \"test_1.csv\")\n\nread_csv(file = file_1)## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): a, b, c\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\ncsv_data <- tribble(\n  ~ `a`, ~ `b`, ~ `c`,\n      1,     2,     3,\n      4,     5,     6)\n\nwrite_csv(x = csv_data, file = \"my_csv.csv\")\nread_csv(\"my_csv.csv\")## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): a, b, c\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6\nfile.remove(\"my_csv.csv\")## [1] TRUE"
  },
  {
    "path": "data.html",
    "id": "skip",
    "chapter": "3 Data",
    "heading": "3.2.1.2 skip",
    "text": "Consider following csv file: test_2.csv. ’s looks like text file:can see, text top file. Often times information data collected, relevant information, included top data file. However, read_csv() can’t differentiate text data want read, causing fail output gibberish.can use skip argument skip first 2 text lines allow read_csv() work.Now ’ve gotten rid warnings, let’s look message R sends: column specification message.",
    "code": "## [1] \"Top two rows consist of junk which\"        \n## [2] \"we don't care about. Data starts on row 3.\"\n## [3] \"a,b,c\"                                     \n## [4] \"9,8,7\"                                     \n## [5] \"4,5,6\"\n# You can also get a csv file by using the URL of the file. This won't work for\n# all file types though.\n\nfile_2 <- paste0(github_url, \"test_2.csv\")\n\nread_csv(file_2)## Warning: One or more parsing issues, see `problems()` for details## Rows: 4 Columns: 1\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): Top two rows consist of junk which\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 4 × 1\n##   `Top two rows consist of junk which`      \n##   <chr>                                     \n## 1 we don't care about. Data starts on row 3.\n## 2 a,b,c                                     \n## 3 9,8,7                                     \n## 4 4,5,6\nread_csv(file = file_2,\n         skip = 2)## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): a, b, c\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     9     8     7\n## 2     4     5     6"
  },
  {
    "path": "data.html",
    "id": "col_types",
    "chapter": "3 Data",
    "heading": "3.2.1.3 col_types",
    "text": "column specification message message R sends tell data types using column.Data types types discussed Chapter 2 Wrangling, characters, factors, integers, dates.\nuse tibble, column specific type data. example, columns numbers . characters column, columns going character data type.get rid column specification message, use col_types() argument specify data types. can just copying column specification message putting col_types() argument.can also change column arguments get data type want.\nTake test_7.csv.Let’s try parsing student column factor grade column integer.clever columns, can make lives easier line graph data.can also manipulate arguments CSV files.",
    "code": "\nread_csv(file = file_2,\n         skip = 2,\n         col_types = cols(a = col_double(),\n                          b = col_double(),\n                          c = col_double()))## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     9     8     7\n## 2     4     5     6\ntest_7 <- paste0(github_url, \"test_7.csv\")\n\nread_csv(test_7)## Rows: 2 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): student\n## dbl (1): grade\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 2\n##   grade student\n##   <dbl> <chr>  \n## 1     1 Sam    \n## 2     5 Becca\nread_csv(test_7,\n         col_types = cols(grade = col_integer(),\n                          student = col_factor()))## # A tibble: 2 × 2\n##   grade student\n##   <int> <fct>  \n## 1     1 Sam    \n## 2     5 Becca"
  },
  {
    "path": "data.html",
    "id": "col_names-and-clean_names",
    "chapter": "3 Data",
    "heading": "3.2.1.4 col_names and clean_names()",
    "text": "Let’s try changing column names test_3.csv file.can see , file doesn’t column names, resulting first row considered names rest file.can fix changing col_names argument.can also create names automatically setting col_names FALSEChanging names columns allows call columns later data actually tibble. Setting column names something can understand makes much easier understand code later .good column names, just aren’t formatted correctly? Let’s look test_4.csv example.can see, function compile names aren’t easy access. ’s possible access column using ` tickmark like :can still cause problems line ’s just annoying use backticks every time want column name. can use clean_names() function janitor package. essentially formats column names follow underscore separated naming convention unique.cleans column names, saving time file large amount columns need type lot column names. issues common ’re pulling data internet ’s normally dirty data can lot columns formatted weird ways.",
    "code": "\nfile_3 <- paste0(github_url, \"test_3.csv\")\n\nread_csv(file_3)## Rows: 1 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): 11, 21, 33\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 1 × 3\n##    `11`  `21`  `33`\n##   <dbl> <dbl> <dbl>\n## 1     4     5     6\nread_csv(file_3, col_names = c(\"a\", \"b\", \"c\"))## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): a, b, c\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    11    21    33\n## 2     4     5     6\nread_csv(file_3, col_names = FALSE)## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): X1, X2, X3\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##      X1    X2    X3\n##   <dbl> <dbl> <dbl>\n## 1    11    21    33\n## 2     4     5     6\nfile_4 <- paste0(github_url, \"test_4.csv\")\n\nfile_4_tibble <- read_csv(file_4)## New names:\n## Rows: 3 Columns: 4\n## ── Column specification\n## ──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n## (4): one powers, Two_Powers...2, 3_Powers, Two_Powers...4\n## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ\n## Specify the column types or set `show_col_types = FALSE` to quiet this message.\n## • `Two_Powers` -> `Two_Powers...2`\n## • `Two_Powers` -> `Two_Powers...4`\nfile_4_tibble## # A tibble: 3 × 4\n##   `one powers` Two_Powers...2 `3_Powers` Two_Powers...4\n##          <dbl>          <dbl>      <dbl>          <dbl>\n## 1            1              2          3              2\n## 2            1              4         81              5\n## 3            1              8         27              8\nfile_4_tibble$`one powers`## [1] 1 1 1\nfile_4_tibble |> \n  clean_names()## # A tibble: 3 × 4\n##   one_powers two_powers_2 x3_powers two_powers_4\n##        <dbl>        <dbl>     <dbl>        <dbl>\n## 1          1            2         3            2\n## 2          1            4        81            5\n## 3          1            8        27            8"
  },
  {
    "path": "data.html",
    "id": "na",
    "chapter": "3 Data",
    "heading": "3.2.1.5 na",
    "text": "Another feature read_csv() na argument.test_5.csv missing value, uses . substitute. makes computer think period actual value data point, obviously true (want numbers instead). default, read_csv() treats white space like spaces tabs missing value, can set argument directly well.",
    "code": "\nfile_5 <- paste0(github_url, \"test_5.csv\")\n\nread_csv(file_5)## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (1): b\n## dbl (2): a, c\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##       a b         c\n##   <dbl> <chr> <dbl>\n## 1     1 .         3\n## 2     4 5         6\nread_csv(file_5,\n         na = \".\")## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): a, b, c\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1    NA     3\n## 2     4     5     6"
  },
  {
    "path": "data.html",
    "id": "comment",
    "chapter": "3 Data",
    "heading": "3.2.1.6 comment",
    "text": "can also tell code ignore comment lines, may common csv file written human comments . test_6.csv perfect example . ’s looks like:setting comment argument, ’re able skip lines certain starting point.\ncase, comment # sign, just need include .ever want skip certain lines, just use comment argument order designate something skipped. best used cases read_csv() command compile without .",
    "code": "## [1] \"a,b,c\"                          \"# This is a comment line\"      \n## [3] \"98,99,100\"                      \"# Here is another comment line\"\n## [5] \"4,5,6\"\nread_csv(file_6, comment = \"#\")## Rows: 2 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## dbl (3): a, b, c\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1    98    99   100\n## 2     4     5     6"
  },
  {
    "path": "data.html",
    "id": "read_delim",
    "chapter": "3 Data",
    "heading": "3.2.1.7 read_delim()",
    "text": "far, ’ve covered can organize CSV data, ’s core ’re working comma separated values. happens want read data something doesn’t use commas separate values?tabular data comes different format, can use read_delim() function instead. example, different version test_6.csv exist column names uses pipes (|) delimiter instead commas.’s another file named delim_1, uses | separate lines instead comma like normal CSV file.read_delim(), specify first argument path file, done read_csv(). provide values delim argument code use | separator instead comma.can often find CSV files websites like kaggle well exporting Excel spreadsheet. Keep mind data imported internet often messy, try use functions listed clean . full list arguments -depth documentation read_csv() function, please visit website.",
    "code": "## [1] \"population|town\"   \"150|Cambridge, MA\" \"92|Newton, MA\"\n# Because delim_1 uses pipes to separate values, we can just use that as our\n# delim value. However, for more complex symbols like tab, we use something\n# different like \"\\\\t\". This varies for every symbol, but you can find most\n# delim values on the internet.\n\ndelim_1 <- paste0(github_url, \"delim_1.txt\")\n\nread_delim(delim_1, delim = \"|\")## Rows: 2 Columns: 2\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \"|\"\n## chr (1): town\n## dbl (1): population\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.## # A tibble: 2 × 2\n##   population town         \n##        <dbl> <chr>        \n## 1        150 Cambridge, MA\n## 2         92 Newton, MA"
  },
  {
    "path": "data.html",
    "id": "excel-files",
    "chapter": "3 Data",
    "heading": "3.2.2 Excel files",
    "text": "Excel spreadsheet program use tables analyze, store, manipulate data. tables composed cells include text, numbers, formulas. Excel files filename extensions .xls .xlsx, ’re capable storing additional things store .csv file fonts, text formatting, graphics, etc.order write excel files install complex packages, hard create. Writing excel files beyond scope Primer.makes Excel files valuable ’re commonly accepted usable (Microsoft Excel common program), ’re also hard use can’t write new data . , Excel files common data originally Excel, like accounting data spreadsheet applications.Reading Excel files easy. , use read_excel() function readxl package..xlsx file multiple sheets, use sheet argument specify sheet number name.\nread_excel() function also arguments similar read_csv() function col_names, col_types, na.",
    "code": "\nlibrary(readxl)\n\n# Unfortunately, it is not possible to read Excel files directly from the web.\n# So we download the file by hand and then read it in from the current working\n# directory. Note that the \"proper\" way of handling this would be to create a\n# temp directory with tempdir(), download the file into that directory, read it,\n# and then delete the temp directory. That way, you would not have random\n# downloaded files hanging around.\n\n# The mode = \"wb\" is a necessary addition for Windows users because Windows is\n# weird. It's not necessary on MacOS and may cause an error as well.\n\ndownload.file(url = paste0(github_url, \"excel_1.xlsx\"), \n              destfile = \"example_excel.xlsx\", mode = \"wb\")\n\n\nread_excel(path = \"example_excel.xlsx\")## # A tibble: 2 × 3\n##       a     b     c\n##   <dbl> <dbl> <dbl>\n## 1     1     2     3\n## 2     4     5     6"
  },
  {
    "path": "data.html",
    "id": "rds-files",
    "chapter": "3 Data",
    "heading": "3.2.3 RDS files",
    "text": "One important aspect R saving objects RDS files, store single R object file.\nfiles allow us save R objects plots tibbles R reload object contains later without re-running code made .\nespecially useful ’re dealing bulk data want save plot comes later don’t data wrangling plotting .\nRDS file, save entire object, allowing things later without go code.\nHowever, RDS files limited R projects , ’re incomprehensible human eyes programming languages. makes RDS files ideal saving objects temporarily project sharing objects R users.Take following R object, graph iris data set.save RDS file, use function write_rds(). Just like write_csv(), function two main arguments: x file. x argument object want save. file argument file path want save file. determines name use file.read_rds() reads file back R. Just like read_csv() read_rds() one main argument, path file wanting read R.can use R object operations, adding trend line.saving iris_p plot RDS file, eliminate time needed calculate generate plot can use saved information. can use object reading file back r using like normal plot, adding new layers new operations.also .Rdata files can store multiple objects, RDS files can accomplish similar task. makes much easier use RDS files anything want keep R.",
    "code": "\niris_p <- iris |> \n  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_jitter() +\n  labs(title = \"Sepal Dimensions of Various Species of Iris\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\")\niris_p\nwrite_rds(x = iris_p, file = \"iris_p.rds\")\nread_rds(file = \"iris_p.rds\")\nrds_p <- read_rds(file = \"iris_p.rds\")\nrds_p + \n  geom_smooth(method = \"loess\",\n              formula = y ~ x,\n              se = FALSE)"
  },
  {
    "path": "data.html",
    "id": "json",
    "chapter": "3 Data",
    "heading": "3.2.4 JSON",
    "text": "increasingly common format sharing data JavaScript Object Notation JSON. format general, nothing like spreadsheet. Note JSON files often made available via internet. Several organizations provide JSON API web service can connect directly can obtain data.JSON files minimal readable format structures data, ’re commonly used transmit data server web application like website. people familiar Javascript coding language, ’ll likely see similarities JSON file format Javascript syntax. makes JSON files ideal internet transport, don’t see much use within project like RDS files .functions fromJSON() toJSON() allow convert R objects JSON. functions come jsonlite package.function toJSON() converts tibble JSON format. Consider example_1 tibble:function fromJSON() converts JSON format tibble.Make sure follow JSON format exactly ’re writing JSON files, format makes special allows work.",
    "code": "\nlibrary(jsonlite)\nexample_1 <- tibble(name= c(\"Miguel\", \"Sofia\", \"Aya\", \"Cheng\"), \n                    student_id = 1:4, exam_1 = c(85, 94, 87, 90), \n                    exam_2 = c(86, 93, 88, 91))\n\nexample_1## # A tibble: 4 × 4\n##   name   student_id exam_1 exam_2\n##   <chr>       <int>  <dbl>  <dbl>\n## 1 Miguel          1     85     86\n## 2 Sofia           2     94     93\n## 3 Aya             3     87     88\n## 4 Cheng           4     90     91\n# The pretty argument adds indentation and whitespace when TRUE.\n\ntoJSON(example_1, pretty = TRUE) ## [\n##   {\n##     \"name\": \"Miguel\",\n##     \"student_id\": 1,\n##     \"exam_1\": 85,\n##     \"exam_2\": 86\n##   },\n##   {\n##     \"name\": \"Sofia\",\n##     \"student_id\": 2,\n##     \"exam_1\": 94,\n##     \"exam_2\": 93\n##   },\n##   {\n##     \"name\": \"Aya\",\n##     \"student_id\": 3,\n##     \"exam_1\": 87,\n##     \"exam_2\": 88\n##   },\n##   {\n##     \"name\": \"Cheng\",\n##     \"student_id\": 4,\n##     \"exam_1\": 90,\n##     \"exam_2\": 91\n##   }\n## ]\njson_format_ex <-\n'[\n  {\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"}, \n  {\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n  {},\n  {\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}\n]'\n\nfromJSON(json_format_ex) ##     Name Age Occupation\n## 1  Mario  32    Plumber\n## 2  Peach  21   Princess\n## 3   <NA>  NA       <NA>\n## 4 Bowser  NA      Koopa"
  },
  {
    "path": "data.html",
    "id": "databases",
    "chapter": "3 Data",
    "heading": "3.3 Databases",
    "text": "\nFIGURE 3.2: DROP TABLE particularly infamous SQL command\nDatabases one common methods storing data, capable storing large amounts data also allowing multiple people access change time. Think like giant book bunch tables hold data. useful , advent computers now able use relational databases.relational database database tables interact one another based common data, allowing create custom tables existing set records. example, relational database may hold multiple tables use ID keep track different information, like one table ID movie name another ID rating. can combine two code, creating table ID, name, rating. allows person made database easily put new data well allow pull data time without loading entire database.’s common use interact databases real world due businesses using relational databases keep track data update leisure. ’s uncommon databases hold thousands even millions rows due need keep track data.section, ’ll going pull data databases interact data without using entire database.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "reading-data-from-a-sqlite-database",
    "chapter": "3 Data",
    "heading": "3.3.1 Reading data from a SQLite database",
    "text": "SQLite probably simplest relational database one can use combination R. SQLite databases self-contained usually stored accessed locally one computer, instead internet cloud. Data usually stored file .db extension. Similar Excel files, plain text files read plain text editor.first thing need read data R database connect database. using dbConnect() function DBI (database interface) package. read data, simply tells R database opens communication channel.Often relational databases many tables, power comes useful ways can joined. Thus anytime want access data relational database, need know table names. can get names tables database using dbListTables().get one table name returned, tells us one table database. reference table database things like select columns filter rows, use tbl() function dbplyr package. package dbplyr allows us work data stored databases local data frames, useful can lot big datasets without actually bring vast amounts data computer!Although looks like just got data frame database, didn’t! ’s reference, showing us data still SQLite database (note first two lines output). databases often efficient selecting, filtering joining large data sets R. typically, database even stored computer, rather powerful machine somewhere web. R lazy waits bring data memory explicitly tell . , use collect() function.filter rows Aboriginal languages category according 2016 Canada Census, use collect() finally bring data R data frame.bother use collect() function? data looks pretty similar outputs shown . dbplyr provides lots functions similar filter() can use directly feed database reference (.e. tbl() gives ) downstream analysis functions (e.g., ggplot2 data visualization lm linear regression modeling). However, work every case; look happens try use nrow count rows data frame:tail preview last 6 rows data frame:functions work use version used collect() :order delete stop using SQLite server, need first disconnect file connection using dbDisconnect() passing connection object argument. can safely delete database file computer using file.remove().Additionally, operations work extract columns single values reference given tbl function. Thus, finished data wrangling tbl() database reference object, advisable bring local machine’s memory using collect() data frame.Warning: Usually, databases big! Reading object local machine may give error take lot time run careful plan !",
    "code": "\nlibrary(DBI)\nlibrary(RSQLite)\n\n# This example uses a different github URL, so you can't use the paste0() trick\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/can_lang.db?raw=true\",\n              destfile = \"example_db.db\", mode = \"wb\")\n\ncon_lang_data <- dbConnect(RSQLite::SQLite(), \"example_db.db\")\ntables <- dbListTables(con_lang_data)\ntables## [1] \"lang\"\nlang_db <- tbl(con_lang_data, \"lang\")\nlang_db## # Source:   table<lang> [?? x 6]\n## # Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/example_db.db]\n##    category          language mother_tongue most_at_home most_at_work lang_known\n##    <chr>             <chr>            <dbl>        <dbl>        <dbl>      <dbl>\n##  1 Aboriginal langu… Aborigi…           590          235           30        665\n##  2 Non-Official & N… Afrikaa…         10260         4785           85      23415\n##  3 Non-Official & N… Afro-As…          1150          445           10       2775\n##  4 Non-Official & N… Akan (T…         13460         5985           25      22150\n##  5 Non-Official & N… Albanian         26895        13135          345      31930\n##  6 Aboriginal langu… Algonqu…            45           10            0        120\n##  7 Aboriginal langu… Algonqu…          1260          370           40       2480\n##  8 Non-Official & N… America…          2685         3020         1145      21930\n##  9 Non-Official & N… Amharic          22465        12785          200      33670\n## 10 Non-Official & N… Arabic          419890       223535         5585     629055\n## # … with more rows\naboriginal_lang_db <- filter(lang_db, category == \"Aboriginal languages\")\naboriginal_lang_db## # Source:   SQL [?? x 6]\n## # Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/example_db.db]\n##    category          language mother_tongue most_at_home most_at_work lang_known\n##    <chr>             <chr>            <dbl>        <dbl>        <dbl>      <dbl>\n##  1 Aboriginal langu… Aborigi…           590          235           30        665\n##  2 Aboriginal langu… Algonqu…            45           10            0        120\n##  3 Aboriginal langu… Algonqu…          1260          370           40       2480\n##  4 Aboriginal langu… Athabas…            50           10            0         85\n##  5 Aboriginal langu… Atikame…          6150         5465         1100       6645\n##  6 Aboriginal langu… Babine …           110           20           10        210\n##  7 Aboriginal langu… Beaver             190           50            0        340\n##  8 Aboriginal langu… Blackfo…          2815         1110           85       5645\n##  9 Aboriginal langu… Carrier           1025          250           15       2100\n## 10 Aboriginal langu… Cayuga              45           10           10        125\n## # … with more rows\naboriginal_lang_data <- collect(aboriginal_lang_db)\naboriginal_lang_data## # A tibble: 67 × 6\n##    category          language mother_tongue most_at_home most_at_work lang_known\n##    <chr>             <chr>            <dbl>        <dbl>        <dbl>      <dbl>\n##  1 Aboriginal langu… Aborigi…           590          235           30        665\n##  2 Aboriginal langu… Algonqu…            45           10            0        120\n##  3 Aboriginal langu… Algonqu…          1260          370           40       2480\n##  4 Aboriginal langu… Athabas…            50           10            0         85\n##  5 Aboriginal langu… Atikame…          6150         5465         1100       6645\n##  6 Aboriginal langu… Babine …           110           20           10        210\n##  7 Aboriginal langu… Beaver             190           50            0        340\n##  8 Aboriginal langu… Blackfo…          2815         1110           85       5645\n##  9 Aboriginal langu… Carrier           1025          250           15       2100\n## 10 Aboriginal langu… Cayuga              45           10           10        125\n## # … with 57 more rows\nnrow(aboriginal_lang_db)## [1] NAtail(aboriginal_lang_db)## Error: tail() is not supported by sql sources\nnrow(aboriginal_lang_data)## [1] 67\ntail(aboriginal_lang_data)## # A tibble: 6 × 6\n##   category           language mother_tongue most_at_home most_at_work lang_known\n##   <chr>              <chr>            <dbl>        <dbl>        <dbl>      <dbl>\n## 1 Aboriginal langua… Tahltan             95            5            0        265\n## 2 Aboriginal langua… Thompso…           335           20            0        450\n## 3 Aboriginal langua… Tlingit             95            0           10        260\n## 4 Aboriginal langua… Tsimshi…           200           30           10        410\n## 5 Aboriginal langua… Wakasha…            10            0            0         25\n## 6 Aboriginal langua… Woods C…          1840          800           75       2665\ndbDisconnect(con_lang_data)\nfile.remove(\"example_db.db\")"
  },
  {
    "path": "data.html",
    "id": "interacting-with-sqlite-databases",
    "chapter": "3 Data",
    "heading": "3.3.2 Interacting with SQLite databases",
    "text": "Now ’ve figured get data database, let’s look wrangle data within database.databases normally contain large amounts data, ’s advisable wrangling use collect() transform database table tibble. stops pulling large amounts data onto computer just ignoring .let’s try pulling database see can manipulate .First , look data database holds.13 tables database. Let’s access first couple tables, just can get good look see ’re structured.can see, tables common column: ArtistId column. However, different information linked ID, “albums” albums artist produced “artists” name artist.full relationship diagram database ’re using.Let’s go operations can SQLite database.",
    "code": "\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Again, use mode = \"wb\" if you're using a Windows operating system.\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/chinook.db?raw=true\",\n             dest = \"chinook.db\", mode = \"wb\")\n\ncon <- dbConnect(RSQLite::SQLite(), \"chinook.db\")\ndbListTables(con)##  [1] \"albums\"          \"artists\"         \"customers\"       \"employees\"      \n##  [5] \"genres\"          \"invoice_items\"   \"invoices\"        \"media_types\"    \n##  [9] \"playlist_track\"  \"playlists\"       \"sqlite_sequence\" \"sqlite_stat1\"   \n## [13] \"tracks\"\nalbums <- tbl(con, \"albums\")\nalbums## # Source:   table<albums> [?? x 3]\n## # Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/chinook.db]\n##    AlbumId Title                                 ArtistId\n##      <int> <chr>                                    <int>\n##  1       1 For Those About To Rock We Salute You        1\n##  2       2 Balls to the Wall                            2\n##  3       3 Restless and Wild                            2\n##  4       4 Let There Be Rock                            1\n##  5       5 Big Ones                                     3\n##  6       6 Jagged Little Pill                           4\n##  7       7 Facelift                                     5\n##  8       8 Warner 25 Anos                               6\n##  9       9 Plays Metallica By Four Cellos               7\n## 10      10 Audioslave                                   8\n## # … with more rows\nartists <- tbl(con, \"artists\")\nartists## # Source:   table<artists> [?? x 2]\n## # Database: sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/chinook.db]\n##    ArtistId Name                \n##       <int> <chr>               \n##  1        1 AC/DC               \n##  2        2 Accept              \n##  3        3 Aerosmith           \n##  4        4 Alanis Morissette   \n##  5        5 Alice In Chains     \n##  6        6 Antônio Carlos Jobim\n##  7        7 Apocalyptica        \n##  8        8 Audioslave          \n##  9        9 BackBeat            \n## 10       10 Billy Cobham        \n## # … with more rows"
  },
  {
    "path": "data.html",
    "id": "using-dbplyr",
    "chapter": "3 Data",
    "heading": "3.3.2.1 Using dbplyr",
    "text": "’ve already seen can use dbpylr package get table database, let’s look can also use operations databases well.dbpylr package allows use many functions tidyverse like select(), filter(), mutate() without issues, making easiest operations .’s example using dbpylr package get number albums artist created.functions ’re already familiar using, work just well database tables well.However, cases use SQL code accomplish certain tasks.",
    "code": "\n# You can keep a column during the summarize() if you just put the name equal to\n# itself. This code is deliberately long in order to show the common functions\n# that we use.\n\nband_albums <- tbl(con, \"albums\") |>\n                 inner_join(tbl(con, \"artists\"), by = \"ArtistId\") |>\n                 select(\"AlbumId\", \"ArtistId\", \"Title\", \"Name\") |>\n                 group_by(ArtistId) |>\n                 summarize(Name = Name, num_albums = n()) |>\n                 mutate(artist_name = Name) |>\n                 mutate(artist_id = ArtistId) |>\n                 filter(num_albums > 3) |>\n                 arrange(desc(num_albums)) |>\n                 select(artist_id, artist_name, num_albums)\nband_albums## # Source:     SQL [?? x 3]\n## # Database:   sqlite 3.38.5 [/Users/dkane/Desktop/projects/primer/chinook.db]\n## # Ordered by: desc(num_albums)\n##    artist_id artist_name     num_albums\n##        <int> <chr>                <int>\n##  1        90 Iron Maiden             21\n##  2        22 Led Zeppelin            14\n##  3        58 Deep Purple             11\n##  4        50 Metallica               10\n##  5       150 U2                      10\n##  6       114 Ozzy Osbourne            6\n##  7       118 Pearl Jam                5\n##  8        21 Various Artists          4\n##  9        82 Faith No More            4\n## 10        84 Foo Fighters             4\n## # … with more rows"
  },
  {
    "path": "data.html",
    "id": "using-sql-code",
    "chapter": "3 Data",
    "heading": "3.3.2.2 Using SQL code",
    "text": "First , let’s try understand SQLite SQL really .SQL, Structured Query Language, coding language works build relational databases ’ve using far. SQLite special type database can download onto computer uses SQL basic structure basic language. Essentially, SQLite uses SQL code things .shouldn’t need write SQL code primer, ’s common read SQL database order access data can graph .SQL runs something known query, line code pulls information database. effectively asking database specific set data. ’s known SELECT command, “selects” data database brings computer use.TABLE 3.1: Displaying records 1 - 10You can modify queries operations like joins.TABLE 3.2: Displaying records 1 - 10If ’re interested SQL statements mean write SQL code, visit tutorial. ’re interested create \ndatabase SQL, visit .fact, dbplyr commands actually use SQL code create effect.\ncan run show_query() see SQL code, query dbplyr produces.However, can’t everything SQL dbplyr package. example, can’t use slice() command.possible SQL database. couple ways run SQL code R, easiest way using dbGetQuery() function DBI library.method allows use R code within project, making easier.can also use SQL code directly project changing header Rmarkdown file. code Rmarkdown file, create chunks look like :can code SQL changing chunk look like :allows write SQL code genuine SQL syntax save output variable.can use variable R code.Coding SQL allows things dbplyr package doesn’t, using certain rows. add LIMIT 10; end query (first SELECT statement), can limit query first 10 rows.SQL versatile dbplyr, ’s also lot confusing ’s straightforward. creates one golden rule:Use dbplyr functions data analysis SQL code accessing dataThis SQL code weird hard understand, allows easily pull data. SQL coding language designed query ask databases data. Meanwhile, dbplyr meant analyzing data modifying .Now, worked SQLite database within tutorial due simplicity. However, databases can accessed different ways. example, PostgresSQL server located internet allows pull data internet instead downloading computer. can learn types SQL databases differences .bother databases ?Opening database stored .db file involved lot effort just opening .csv, .tsv, plain text Excel formats. bit pain use database setting since use dbplyr translate tidyverse-like commands (filter, select, head, etc.) SQL commands database understands. tidyverse commands can currently translated SQLite databases, like showed slice(). might wondering, use databases ?Databases beneficial large-scale setting:enable storing large data sets across multiple computers automatic redundancy backupsthey allow multiple users access simultaneously remotely without conflicts errorsthey provide mechanisms ensuring data integrity validating inputthey provide security keep data safe\nexample, billions Google searches conducted daily.makes databases extremely useful business ’ll employees thousands millions operations daily. makes knowing SQL databases extremely vital data science general usage perspective.",
    "code": "-- This code is written in SQL syntax, so it won't work if you just put it into R. \n-- We'll be going over how to write SQL in R later in this section.\n\nSELECT AlbumId,\n       Title,\n       ArtistId\nFROM albums;-- This does the same thing as the inner join from Chapter 2 Wrangling\n-- Here we're joining the albums and the artists table by the ArtistId column\n\nSELECT artistid,\n       NAME,\n       title\nFROM\n  (SELECT title,\n          albums.artistid AS \"ArtistId\",\n          \"name\"\n   FROM albums\n   INNER JOIN artists ON albums.artistid = artists.artistid);\nshow_query(band_albums)## <SQL>\n## SELECT `artist_id`, `artist_name`, `num_albums`\n## FROM (\n##   SELECT *, `Name` AS `artist_name`, `ArtistId` AS `artist_id`\n##   FROM (\n##     SELECT `ArtistId`, `Name`, COUNT(*) AS `num_albums`\n##     FROM (\n##       SELECT `AlbumId`, `ArtistId`, `Title`, `Name`\n##       FROM (\n##         SELECT `AlbumId`, `Title`, `LHS`.`ArtistId` AS `ArtistId`, `Name`\n##         FROM `albums` AS `LHS`\n##         INNER JOIN `artists` AS `RHS`\n##           ON (`LHS`.`ArtistId` = `RHS`.`ArtistId`)\n##       )\n##     )\n##     GROUP BY `ArtistId`\n##   )\n## )\n## WHERE (`num_albums` > 3.0)\n## ORDER BY `num_albums` DESCslice(band_albums, 1:10)## Error: slice() is not supported on database backends\n# This code is just copy-pasted and reformatted from the earlier dbplyr code so\n# you can best understand it.\n\ndbGetQuery(con, \"\nSELECT `artist_id`,\n       `artist_name`,\n       `num_albums`\nFROM\n  (SELECT `artistid`,\n          `name`,\n          `num_albums`,\n          `artist_name`,\n          `artistid` AS `artist_id`\n   FROM\n     (SELECT `artistid`,\n             `name`,\n             `num_albums`,\n             `name` AS `artist_name`\n      FROM\n        (SELECT `artistid`,\n                `name`,\n                Count(*) AS `num_albums`\n         FROM\n           (SELECT `albumid`,\n                   `artistid`,\n                   `title`,\n                   `name`\n            FROM\n              (SELECT `albumid`,\n                      `title`,\n                      `LHS`.`artistid` AS `ArtistId`,\n                      `name`\n               FROM `albums` AS `LHS`\n               INNER JOIN `artists` AS `RHS` ON (`LHS`.`artistid` = `RHS`.`artistid`))) GROUP  BY `artistid`)))\nWHERE (`num_albums` > 3.0)\n  ORDER  BY `num_albums` DESC\n\")##    artist_id     artist_name num_albums\n## 1         90     Iron Maiden         21\n## 2         22    Led Zeppelin         14\n## 3         58     Deep Purple         11\n## 4         50       Metallica         10\n## 5        150              U2         10\n## 6        114   Ozzy Osbourne          6\n## 7        118       Pearl Jam          5\n## 8         21 Various Artists          4\n## 9         82   Faith No More          4\n## 10        84    Foo Fighters          4\n## 11       149            Lost          4\n## 12       152       Van Halen          4## ```{r}\n##\n## ```## ```{sql, connection = con, output.var = \"your_variable_name\"}\n## \n## ```## ```{sql, connection = con, output.var = \"your_variable_name\"}\n## SELECT * FROM albums\n## ```\nhead(your_variable_name)##   AlbumId                                 Title ArtistId\n## 1       1 For Those About To Rock We Salute You        1\n## 2       2                     Balls to the Wall        2\n## 3       3                     Restless and Wild        2\n## 4       4                     Let There Be Rock        1\n## 5       5                              Big Ones        3\n## 6       6                    Jagged Little Pill        4-- We're writing this code using the sql code chunk method. \n-- This is because it allows you to see what's highlighted and what's not.\n-- SQL code is also inconvenient because you can't just add a new line,\n-- Instead, you need to treat all of these lines of code as a single statement\n-- Then add in the command where you think that it would fit.\nSELECT `artist_id`,\n       `artist_name`,\n       `num_albums`\nFROM\n  (SELECT `artistid`,\n          `name`,\n          `num_albums`,\n          `artist_name`,\n          `artistid` AS `artist_id`\n   FROM\n     (SELECT `artistid`,\n             `name`,\n             `num_albums`,\n             `name` AS `artist_name`\n      FROM\n        (SELECT `artistid`,\n                `name`,\n                Count(*) AS `num_albums`\n         FROM\n           (SELECT `albumid`,\n                   `artistid`,\n                   `title`,\n                   `name`\n            FROM\n              (SELECT `albumid`,\n                      `title`,\n                      `LHS`.`artistid` AS `ArtistId`,\n                      `name`\n               FROM `albums` AS `LHS`\n               INNER JOIN `artists` AS `RHS` ON (`LHS`.`artistid` = `RHS`.`artistid`))) GROUP  BY `artistid`)))\nWHERE (`num_albums` > 3.0)\n  ORDER  BY `num_albums` DESC\nLIMIT 10;\n# You can't see this, but output.var = \"band_albums_sql\" in the chunk above\n# this. So we're outputting our data into band_albums_sql\nband_albums_sql##    artist_id     artist_name num_albums\n## 1         90     Iron Maiden         21\n## 2         22    Led Zeppelin         14\n## 3         58     Deep Purple         11\n## 4         50       Metallica         10\n## 5        150              U2         10\n## 6        114   Ozzy Osbourne          6\n## 7        118       Pearl Jam          5\n## 8         21 Various Artists          4\n## 9         82   Faith No More          4\n## 10        84    Foo Fighters          4"
  },
  {
    "path": "data.html",
    "id": "webscraping",
    "chapter": "3 Data",
    "heading": "3.4 Webscraping",
    "text": "first part chapter, learned read data plain text files usually “rectangular” shape using tidyverse read_* functions. Sadly, data comes simple format, can happily use many tools read messy/wild data formats. formal name gathering non-rectangular data web transforming useful format data analysis web scraping.can web scraping r using rvest library, library allows us look HTML CSS selectors, pick apart get data want. Let’s look means.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "html-and-css-selectors",
    "chapter": "3 Data",
    "heading": "3.4.1 HTML and CSS selectors",
    "text": "jump scraping, let’s learn little bit “source code” website looks like.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "website-structure",
    "chapter": "3 Data",
    "heading": "3.4.1.1 Website structure",
    "text": "\nFIGURE 3.3: little bit HTML editing can bring long way.\nWebsites coded language called HTML, HyperText Markup Language. code puts information website, numbers text. files created “knit” .Rmd files ’s makes put information screen. ’re webscraping, can look information website looking HTML code. can try ! Just go website right-click, press “Inspect”. ’ll able see HTML code behind website even edit liking!\nFIGURE 3.4: can see HTML code website even edit temporarily inspecting HTML code \nHTML uses many nested “tags” organize structure. example, HTML file like :looks like result:\nHello World\ntag defines element, part website. example, used <p> tag define paragraph element HTML. close paragraph using </p> tag, ending paragraph. can find full list HTML tags elements .Now, ’re data science. let’s say find really cool website internet lot really useful data, ’s download file (’s legal scrape website). means pull information HTML code.can using rvest package like talked earlier. essentially package allows scrape information HTML code, allowing scrape information complete website.Let’s try scraping Hello World earlier HTML code.First , need use minimal_html() function can get R object can work . temporary use ’re trying Think like changing information HTML code something R can understand.need filter elements HTML file contains. case, ’re looking paragraph tag, <p>. means can just use html_element() get first paragraph element.returns HTML node, specific element chose. paragraphs HTML code, return HTML nodes. Now, interesting thing nodes contain information can access using html_text2(). lets us parse code without problems.websites, especially ones useful data, don’t just exist state “Hello World”. ’re much complex . layering different elements, can create website contains lot information.example, HTML:Creates table paragraph looks like :\nuseless, shouldn’t even reading .\nNow, let’s say want get information table don’t want get useless description end.can looking table (<table> tag) getting cells (<td> tag). ’ll also save variable later.Notice actually outputs list elements text. means can use output like list order get information want. Just use [[]] syntax.method ’d use just wanted get information inside specific table without filtering data . pulling directly HTML makes lot sense scenario fiddling around junk. Just pull data go, fancy things.However, want important information table? ’s CSS selectors .",
    "code": "\nknitr::include_graphics(\"03-data/images/html_inspect.gif\")<!-- This is using the HTML language. You don't actually put this in a chunk if \nyou want to put this in an Rmarkdown file, you can just type it straight in.-->\n<html>\n  <p>\n    Hello World\n  </p>\n</html>\nlibrary(rvest)\nraw_html_1 <- \"<html>\n                 <p>\n                   Hello World\n                 </p>\n               </html>\"\nraw_html_1 |>\n  minimal_html()## {html_document}\n## <html>\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body>\\n<p>\\n                   Hello World\\n                 </p>\\n      ...\n# Only use the letter part of the tag, not the <, >, or /\n\nraw_html_1 |>\n  minimal_html() |>\n  html_element(\"p\")## {html_node}\n## <p>\n# html_text2() is different from the normal html_text() because it returns an\n# actual string. This is normally what we want.\n\nraw_html_1 |>\n  minimal_html() |>\n  html_element(\"p\") |>\n  html_text2()## [1] \"Hello World\"<html>\n  <table>\n    <tr>\n      <td>\n        This is some important info.\n      </td>\n      <td>\n        This is some unimportant info.\n      </td>\n    </tr>\n    <tr>\n      <td>\n        This is really important info.\n      </td>\n      <td>\n        This is some other unimportant information.\n      </td>\n    </tr>\n  </table>\n  <p>\n    This is so useless, you shouldn't even be reading it.\n  </p>\n</html>\nraw_html_2 <- \"<html>\n                 <table>\n                   <tr>\n                     <td>\n                       This is some important info.\n                     </td>\n                     <td>\n                       This is some unimportant info.\n                     </td>\n                   </tr>\n                   <tr>\n                     <td>\n                       This is really important info.\n                     </td>\n                     <td>\n                       This is some other unimportant information.\n                     </td>\n                   </tr>\n                 </table>\n                 <p>\n                   This is so useless, you shouldn't even be reading it.\n                 </p>\n               </html>\"\nraw_html_2## [1] \"<html>\\n                 <table>\\n                   <tr>\\n                     <td>\\n                       This is some important info.\\n                     </td>\\n                     <td>\\n                       This is some unimportant info.\\n                     </td>\\n                   </tr>\\n                   <tr>\\n                     <td>\\n                       This is really important info.\\n                     </td>\\n                     <td>\\n                       This is some other unimportant information.\\n                     </td>\\n                   </tr>\\n                 </table>\\n                 <p>\\n                   This is so useless, you shouldn't even be reading it.\\n                 </p>\\n               </html>\"\n# We use html_elements() to get all of the elements in the HTML file.\n\ntd_tags <- raw_html_2 |>\n             minimal_html() |>\n             html_element(\"table\") |>\n             html_elements(\"td\")\ntd_tags## {xml_nodeset (4)}\n## [1] <td>\\n                       This is some important info.\\n               ...\n## [2] <td>\\n                       This is some unimportant info.\\n             ...\n## [3] <td>\\n                       This is really important info.\\n             ...\n## [4] <td>\\n                       This is some other unimportant information.\\ ...\ntd_tags[[2]] |>\n  html_text2()## [1] \"This is some unimportant info.\""
  },
  {
    "path": "data.html",
    "id": "css-selectors",
    "chapter": "3 Data",
    "heading": "3.4.1.2 CSS selectors",
    "text": "CSS, Cascading Style Sheets, coding language defines style webpages. may noticed earlier HTML code output earlier just black white. websites aren’t black white, colors cool things. using CSS add special rules specific elements.Now, number ways . first one using class tell webpage want elements style. CSS classes way multiple elements style, rather limited unique ID. can see example , set elements class good-info green class amazing-info pale red background.can use class HTML code.\nuseless, shouldn’t even reading .\nNow, ’s really clever trick can use . Let’s say want table cells important, don’t know exactly (know class). Well cases, data want different color, like see data . Think like making important information italic font make stand . CSS makes information italic assigning specific class, can just look everything class pull get important information. can using html_elements(). Just plug class element want . front tell R ’s class.can sort multiple classes just chaining together.Webpages also use something called ID change color specific element. thing time “#” beginning signify ’s symbol.\nred!\n\nred.\nfar, ’ve covered 3 ways find parts website ’s HTML code: element (<p>), ID (#red-id), class (.red-class). However, can also mix--match tags. Take following HTML element:can try accessing normal methods, like :can mix match make even narrow search chaining together.can also get link node refers rather text. case, link stored href attribute, use html_attr() access .accesses href attribute element without accessing link, allowing find links outside websites.chains known CSS Selectors ’re important part data science allow us find data information website without download every file copying everything . Instead, can just use website’s inbuilt code get information.",
    "code": "/*This uses the `css` language instead of the R language. You can use this\n  code in an Rmarkdown file by substituting the \"r\" for \"css\" in the top \n  brackets*/\n  \n.good-info {\n  color: green\n}\n  \n.amazing-info {\n  background-color: lightpink\n}<html>\n  <table>\n    <tr>\n      <td class = 'good-info'>\n        This is some important info.\n      </td>\n      <td>\n        This is some unimportant info.\n      </td>\n    </tr>\n    <tr>\n      <td class = 'good-info amazing-info'>\n        This is really important info.\n      </td>\n      <td>\n        This is some other unimportant information.\n      </td>\n    </tr>\n  </table>\n  <p>\n    This is so useless, you shouldn't even be reading it.\n  </p>\n</html>\nraw_html_3 <- \"<html>\n                 <table>\n                   <tr>\n                     <td class = 'good-info'>\n                       This is some important info.\n                     </td>\n                     <td>\n                       This is some unimportant info.\n                     </td>\n                   </tr>\n                   <tr>\n                     <td class = 'good-info amazing-info'>\n                       This is really important info.\n                     </td>\n                     <td>\n                       This is some other unimportant information.\n                     </td>\n                   </tr>\n                 </table>\n                 <p>\n                   This is so useless, you shouldn't even be reading it.\n                 </p>\n               </html>\"\n\nraw_html_3 |>\n  minimal_html() |>\n  html_elements(\".good-info\") |>\n  html_text2()## [1] \"This is some important info.\"   \"This is really important info.\"\nraw_html_3 |>\n  minimal_html() |>\n  html_elements(\".good-info.amazing-info\") |>\n  html_text2()## [1] \"This is really important info.\".red-id {\n  color: red\n}<html>\n<p id = \"red-id\"> This is red! </p>\n<p> This is not red. </p>\n</html><html>\n<a href=\"https://ppbds.github.io/primer/index.html\" id=\"abcd1234\" class=\"hello hi\">\nlink to primer\n</a>\n</html>\nraw_html_5 <- '<a href = \"https://ppbds.github.io/primer/index.htm\" id=\"abcd1234\" class=\"hello hi\">link to primer</a>'\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a\") |>\n  html_text2()## [1] \"link to primer\"\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"#abcd1234\") |>\n  html_text2()## [1] \"link to primer\"\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\".hello\") |>\n  html_text2()## [1] \"link to primer\"\n# Keep in mind that IDs (the # part) are usually unique to that element only, so\n# there's not really a point in filtering it even more when it's already unique.\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a#abcd1234\") |>\n  html_text2()## [1] \"link to primer\"\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello\") |>\n  html_text2()## [1] \"link to primer\"\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello#abcd1234\") |>\n  html_text2()## [1] \"link to primer\"\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\".hello#abcd1234\") |>\n  html_text2()## [1] \"link to primer\"\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello.hi#abcd1234\") |>\n  html_text2()## [1] \"link to primer\"\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello.hi#abcd1234\") |>\n  html_attr(\"href\")## [1] \"https://ppbds.github.io/primer/index.htm\""
  },
  {
    "path": "data.html",
    "id": "application",
    "chapter": "3 Data",
    "heading": "3.4.2 Application",
    "text": "Now let’s look real-world context. Say interested knowing average rental price (per square footage) recently available one-bedroom apartments Vancouver according . visit Vancouver Craigslist website search one-bedroom apartments, shown:page, ’s pretty easy us find apartment price square footage (Craigslist utterly incomprehensible otherwise). computer can’t deal human eyes computer eyes different. can’t understand anything screen. instead looking screen, can just parse HTML find correct information. ’s part HTML code Vancouver’s Craigslist:genuinely sucks read. ’s links, random numbers, elements 4 classes time, stupidly long point eyes fall . can pick CSS selectors need, (“result-price” “housing”), hassle pain ’s better solution.using SelectorGadget tool order find CSS selectors. ’s open source tool simplifies generating finding CSS selectors. recommend use Chrome web browser use tool, install selector gadget tool Chrome Web Store. short video install use SelectorGadget tool get CSS selector use web scraping:installing using selectorgadget shown video , get two CSS selectors .housing .result-price can use scrape information square footage rental price, respectively. selector gadget returns us comma separated list (.housing , .result-price), exactly format need provide R.However, ’re scraping actual websites need take just size code consideration. also need take legal aspects account,",
    "code": "        <span class=\"result-meta\">\n                <span class=\"result-price\">$800</span>\n\n                <span class=\"housing\">\n                    1br -\n                </span>\n\n                <span class=\"result-hood\"> (13768 108th Avenue)</span>\n\n                <span class=\"result-tags\">\n                    <span class=\"maptag\" data-pid=\"6786042973\">map</span>\n                </span>\n\n                <span class=\"banish icon icon-trash\" role=\"button\">\n                    <span class=\"screen-reader-text\">hide this posting</span>\n                </span>\n\n            <span class=\"unbanish icon icon-trash red\" role=\"button\" aria-hidden=\"true\"></span>\n            <a href=\"#\" class=\"restore-link\">\n                <span class=\"restore-narrow-text\">restore</span>\n                <span class=\"restore-wide-text\">restore this posting</span>\n            </a>\n\n        </span>\n    </p>\n</li>\n         <li class=\"result-row\" data-pid=\"6788463837\">\n\n        <a href=\"https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html\" class=\"result-image gallery\" data-ids=\"1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV\">\n                <span class=\"result-price\">$2285</span>\n        </a>\n\n    <p class=\"result-info\">\n        <span class=\"icon icon-star\" role=\"button\">\n            <span class=\"screen-reader-text\">favorite this post</span>\n        </span>\n\n            <time class=\"result-date\" datetime=\"2019-01-06 12:06\" title=\"Sun 06 Jan 12:06:01 PM\">Jan  6</time>\n\n\n        <a href=\"https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html\" data-id=\"6788463837\" class=\"result-title hdrlnk\">Luxury 1 Bedroom CentreView with View - Lonsdale</a>\n\n"
  },
  {
    "path": "data.html",
    "id": "legal-to-scrape",
    "chapter": "3 Data",
    "heading": "3.4.2.1 Are you allowed to scrape that website?",
    "text": "scraping data web, always check whether ALLOWED scrape ! two documents important : robots.txt file (found adding /robots.txt end URL like ) reading website’s Terms Service document. website’s Terms Service document far away important two ’s actually legally binding, look first. happens look Craigslist’s Terms Service document? Well read :“agree copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, automated manual equivalent (e.g., hand).”source: https://www.craigslist.org//terms..useWant learn legalities web scraping crawling? Read interesting blog post titled “Web Scraping Crawling Perfectly Legal, Right?” Benoit Bernard (optional, required reading).now? Well, can’t scrape Craigslist, find something else allows scrape. Let’s use database Open Secrets foreign-connected PACs within country.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "scraping-from-actual-websites",
    "chapter": "3 Data",
    "heading": "3.4.2.2 Scraping from actual websites",
    "text": "Now, earlier scraped pre-written HTML code, can websites well. can just plugging URL website using read_html() instead minimal_html().case, can use Selector Gadget find correct CSS selector “table.DataTable-Partial”. just proceed normal, need use html_table() ’re trying bring table use tibble.allows us scrape tibbles websites. can use tibbles create new graphs.Let’s try using Wikipedia page gun violence United States.First, need save URL read HTML code R.can use SelectorGadget find correct CSS selector. can’t easily click table, can also find selector reading HTML code . can highlighting table, right clicking, pressing “Inspect”. ’ll dig HTML order find correct selector, perfectly viable way find selectors.can save table provided variable known raw_data_wiki.Past point, can work data just like normal data. ’s example graph states denser population experience consistent gun deaths states less dense population.Webscraping powerful tool gain data directly internet, make sure ’re following proper protocols don’t break law actually get information need.",
    "code": "\nweb_url <- \"https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020\"\n\nweb_url |>\n  read_html()## {html_document}\n## <html class=\"no-js\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body>\\n\\n    <!-- Google Adsense Script -->\\n<!--    <script async src=\" ...\nweb_url |>\n  read_html() |>\n  html_element(\"table.DataTable-Partial\") |>\n  html_table()## # A tibble: 225 × 5\n##    `PAC Name (Affiliate)`                `Country of Origin…` Total Dems  Repubs\n##    <chr>                                 <chr>                <chr> <chr> <chr> \n##  1 7-Eleven                              Japan/Seven & I Hol… $20,… $1,0… $19,0…\n##  2 ABB Group (ABB Group)                 Switzerland/Asea Br… $16,… $6,8… $10,1…\n##  3 Accenture (Accenture)                 Ireland/Accenture p… $83,… $50,… $33,0…\n##  4 Air Liquide America                   France/L'Air Liquid… $37,… $15,… $22,0…\n##  5 Airbus Group                          Netherlands/Airbus … $182… $79,… $103,…\n##  6 Alkermes Inc                          Ireland/Alkermes Plc $94,… $30,… $64,0…\n##  7 Allianz of America (Allianz)          Germany/Allianz AG … $71,… $36,… $35,1…\n##  8 AMG Vanadium                          Netherlands/AMG Adv… $2,0… $0    $2,000\n##  9 Anheuser-Busch (Anheuser-Busch InBev) Belgium/Anheuser-Bu… $336… $174… $162,…\n## 10 AON Corp (AON plc)                    UK/AON PLC           $80,… $44,… $36,5…\n## # … with 215 more rows\nwiki_url <- \"https://en.wikipedia.org/w/index.php?title=%22,%22Gun_violence_in_the_United_States_by_state%22,%22&direction=prev&oldid=810166167\"\n\nwiki_url |>\n  read_html()## {html_document}\n## <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n## [1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n## [2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\nraw_data_wiki <- wiki_url |>\n                   read_html() |>\n                   html_element(\"table.wikitable.sortable\") |>\n                   html_table()\nraw_data_wiki## # A tibble: 51 × 4\n##    State                `Population (total i…` `Murders and N…` `Murder and No…`\n##    <chr>                <chr>                  <chr>                       <dbl>\n##  1 Alabama              4,853,875              348                           7.2\n##  2 Alaska               737,709                59                            8  \n##  3 Arizona              6,817,565              309                           4.5\n##  4 Arkansas             2,977,853              181                           6.1\n##  5 California           38,993,940             1,861                         4.8\n##  6 Colorado             5,448,819              176                           3.2\n##  7 Connecticut          3,584,730              117                           3.3\n##  8 Delaware             944,076                63                            6.7\n##  9 District of Columbia 670,377                162                          24.2\n## 10 Florida              20,244,914             1,041                         5.1\n## # … with 41 more rows\nclean_data <- raw_data_wiki |>\n                rename(\"population\" = \"Population (total inhabitants) (2015) [1]\",\n                       \"death_rate\" = \"Murder and Nonnegligent\\nManslaughter Rate(per 100,000 inhabitants) (2015)\",\n                       \"total_deaths\" = \"Murders and Nonnegligent\\nManslaughter(total deaths) (2015) [2]\") |>\n                        select(population, death_rate) |>\n                        mutate(population = parse_number(population)) |>\n                        mutate(pop_tile = ntile(population, 20)) |>\n                        group_by(pop_tile) |>\n                        summarize(num_states = n(), sum_rate = sum(death_rate)) |>\n                        mutate(avg_rate = sum_rate/num_states) |>\n                        mutate(percent_rate = avg_rate / sum(avg_rate))\n\nggplot(clean_data, aes(x = pop_tile, y = percent_rate)) + \n  geom_col() +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", se = FALSE) +\n  scale_x_continuous(breaks = 1:20) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  theme_classic() +\n  labs(title = \"Distribution of US Death Rate by Guns in Quantile of Population in 2015\",\n       subtitle = \"Death rate of less populated states fluctuate more than states with a denser population\",\n       x = \"Quantile by Population (least to most)\",\n       y = \"Percent of Average Death Rate\",\n       caption = \"Wikipedia: Gun Violence in the United States (2017)\")"
  },
  {
    "path": "data.html",
    "id": "working-with-apis",
    "chapter": "3 Data",
    "heading": "3.5 Working with APIs",
    "text": "“API” stands Application Program Interface. allow us access open data government agencies, companies, organizations. API provides rules software applications interact one another. Open data APIs provide rules need know write R code request pull data organization’s web server R. Usually, computational burden querying subsetting data taken source’s server, create subset requested data pass computer. practice, means can often pull subset data want large available dataset without download full dataset load locally R session.overview, basic steps accessing using data web API working R :Figure API rules HTTP requestsWrite R code create request proper formatSend request using GET POST HTTP methodsOnce get back data request, parse easier--use format necessaryTo get data API, first read organization’s API documentation. organization post details data available API(s), well set HTTP requests get data. request data API, typically need send organization’s web server HTTP request using GET POST method. API documentation details typically show example GET POST request API, including base URL use possible query parameters can used customize dataset request.example:National Aeronautics Space Administration (NASA) API pulling Astronomy Picture Day. API documentation, specify base URL API request https://api.nasa.gov/planetary/apod can include parameters specify date daily picture want, whether pull high-resolution version picture, NOAA API key requested NOAA.Many organizations require get API key use key API requests. key allows organization control API access, including enforcing rate limits per user. API rate limits restrict often can request data (hourly limit 1,000 requests per user NASA APIs).API keys kept private, writing code includes API key, careful include actual key code public (even code public GitHub repositories). ensure privacy, save value key file named .Renviron home directory. file plain text file must end blank line. ’ve saved API key global variable file (e.g., line added .Renviron file like NOAA_API_KEY = “abdafjsiopnab038”), can assign key value R object R session using Sys.getenv function (e.g., noaa_api_key <- Sys.getenv(“NOAA_API_KEY”)), use object noaa_api_key anywhere otherwise used character string API key.find R packages accessing exploring open data, check Open Data CRAN task view. can also browse ROpenSci packages, GitHub repositories can explore package works! ROpenSci organization mission create open software tools science. create package access data relevant scientific research API, consider submitting peer-review ROpenSci.riem package, developed Maelle Salmon ROpenSci package, excellent straightforward example can use R pull open data web API. package allows pull weather data airports around world directly Iowa Environmental Mesonet. show pull data R API, section walk code riem package code based closely code package.get certain set weather data Iowa Environmental Mesonet, can send HTTP request specifying base URL, https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/, well parameters describing subset dataset want (e.g., date ranges, weather variables, output format). know rules names possible values parameters (), can submit HTTP GET request using functionGET() httr package.making HTTP request using GET() POST() functions httr package, can include key-value pairs query parameters list object query argument function. example, suppose want get wind speed miles per hour (data = “sped”) Denver, CO, (station = “DEN”) month June 2016 (year1 = “2016”, month1 = “6”, etc.) Denver’s local time zone (tz = “America/Denver”) comma-separated file (format = “comma”). get weather dataset, can run:content() call extracts content response HTTP request sent GET() function. Iowa Environmental Mesonet API offers option return requested data comma-separated file (format = “comma” GET request), content read_csv() used extract read csv file. Usually, data returned JSON format instead.tricky part process figuring available parameter names (e.g., station) possible values (e.g., “DEN” Denver). Currently, details can send HTTP request Iowa Environmental Mesonet’s API include:four-character weather station identifier (station)weather variables (e.g., temperature, wind speed) include (data)Starting ending dates describing range ’d like pull data (year1, month1, day1, year2, month2, day2)time zone use date-times weather observations (tz)Different formatting options (e.g., delimiter use resulting data file [format], whether include longitude latitude)Typically, parameter names possible values explained API documentation. cases, however, documentation limited. case, may able figure possible values, especially API specifies GET rather POST method, playing around website’s point--click interface looking url resulting data pages. example, look Iowa Environmental Mesonet’s page accessing data, ’ll notice point--click web interface allows options list , click access dataset using interface, web address data page includes parameter names values.riem package implements ideas three clean straightforward functions. can explore code behind package see ideas can incorporated small R package, /R directory package’s GitHub page.R packages already exist many open data APIs. R package already exists API, can use functions package directly, rather writing code using API protocols httr functions. examples existing R packages interact open data APIs include:twitteR: Twitterrnoaa: National Oceanic Atmospheric AdministrationQuandl: Quandl (financial data)RGoogleAnalytics: Google Analyticscensusr, acs: United States CensusWDI, wbstats: World BankGuardianR, rdian: Guardian Media GroupblsAPI: Bureau Labor Statisticsrtimes: New York TimesdataRetrieval, waterData: United States Geological Survey\nR package doesn’t exist open API ’d like write package, find writing API packages vignette httr package. document includes advice error handling within R code accesses data open API.Information section API’s taken Mastering Software Development R textbook, authored Roger D. Peng, Sean Kross, Brooke Anderson.",
    "code": "\nlibrary(httr)\nlibrary(httr)\nmeso_url <- \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/\"\ndenver <- GET(url = meso_url,\n                    query = list(station = \"DEN\",\n                                 data = \"sped\",\n                                 year1 = \"2016\",\n                                 month1 = \"6\",\n                                 day1 = \"1\",\n                                 year2 = \"2016\",\n                                 month2 = \"6\",\n                                 day2 = \"30\",\n                                 tz = \"America/Denver\",\n                                 format = \"comma\")) |>\n  content() |> \n  read_csv(skip = 5, na = \"M\")## Rows: 9108 Columns: 3\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr  (1): station\n## dbl  (1): sped\n## dttm (1): valid\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n# There are 9,106 rows of data to look at! Let's just look at subset for our\n# purposes.\n\ndenver |> \n  slice(1:3)## # A tibble: 3 × 3\n##   station valid                sped\n##   <chr>   <dttm>              <dbl>\n## 1 DEN     2016-06-01 00:00:00   9.2\n## 2 DEN     2016-06-01 00:05:00   9.2\n## 3 DEN     2016-06-01 00:10:00   6.9"
  },
  {
    "path": "data.html",
    "id": "distill",
    "chapter": "3 Data",
    "heading": "3.6 Distill",
    "text": "Now, may noticed whenever knit .Rmd files, produce HTML file. just little bit earlier webscraping section, talked websites using HTML make basic structure. logic, can’t make websites using HTML files created knit .Rmd?’s exactly distill package . Essentially, distill allows make websites using HTML files output knit . also organizes pages nicely make website navigable look professional.’s example distill website. can see, clear, easy website pages easily distinguishable information.cover use distill package associated primer tutorials, ’ll walk creating distill page developing citations.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "common-errors-and-functions",
    "chapter": "3 Data",
    "heading": "3.6.1 Common Errors and Functions",
    "text": "forge onwards world distill, likely encounter variety errors ’ll make want throw computer away. , ’ll go common functions errors encountered making distill page.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "create_website",
    "chapter": "3 Data",
    "heading": "3.6.1.1 create_website()",
    "text": "first step creating website using create_website() function. creates entire distill website first function run whenever want make one. Just use following syntax:essentially creates distill website current R project, titles “title---website” formats can publish Github Pages (Github’s website publishing service). run , see index.Rmd .Rmd file project, well miscellaneous files. “Home” “” pages can use website.",
    "code": "\nlibrary(distill)\ncreate_website(dir = \".\", title = \"title-of-your-website\", gh_pages = TRUE)"
  },
  {
    "path": "data.html",
    "id": "create_article",
    "chapter": "3 Data",
    "heading": "3.6.1.2 create_article()",
    "text": "want pages ? restricted Home page pretty bad, especially want bibliography. use create_article() create new page within website. Just run function title file ’ll automatically create .Rmd file code ., need put website. Just go _site.yml file add page using text: \"Sources\" \"href: sources.html\". _site.yml file look like add page._site.yml file keeps everything line organizes distill website. However, ’s specific syntax, take care add specific things. Additionally, keep mind always use create_article() function whenever need create new page .Rmd file. add .Rmd file manually. lead errors can break project.",
    "code": "\ncreate_article(\"sources.Rmd\")name: \".\"\ntitle: \"title-of-your-website\"\ndescription: |\n  Welcome to the website. I hope you enjoy it!\noutput_dir: \"docs\"\nnavbar:\n  right:\n    # These two pages were automatically put in.\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    # This page was added by us.\n    - text: \"Sources\"\n      href: sources.html\noutput: distill::distill_article"
  },
  {
    "path": "data.html",
    "id": "formatting-the-article",
    "chapter": "3 Data",
    "heading": "3.6.1.3 Formatting the article",
    "text": "can format article just like ’s still RMarkdown file. However, can also put advanced features like block quotes . Just make sure edit headers top file without knowing exactly ’re order prevent errors.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "error-operator-is-invalid-for-atomic-vectors",
    "chapter": "3 Data",
    "heading": "3.6.1.4 Error: $ operator is invalid for atomic vectors",
    "text": "error caused creating .Rmd file without using _site.yml using create_article() make . Essentially, distill doesn’t know deal extra .Rmd file crashes . can normally solved just deleting extra .Rmd, may necessary just nuke project orbit restart., make sure never create extra .Rmd, ’s just going work end start encountering error. want write R code graph can put website, just use normal R script file going File -> New File -> R Script RStudio.",
    "code": ""
  },
  {
    "path": "data.html",
    "id": "yaml-errors",
    "chapter": "3 Data",
    "heading": "3.6.1.5 YAML Errors",
    "text": "errors created problems _site.yml file headers (— part) top file. parts essentially tell distill title description page , well information. However, missing parts can cause pretty significant problems distill just doesn’t information needs. example, deleting site: part header allow website work including description: | stop description showing .’s YAML header top .Rmd file look like. can make edits add information, make sure keep basic structure order prevent errors.Keep mind can modify need make new page. However, whenever error beginning Error yaml::yaml.load(..., eval.expr = TRUE) :, normally means YAML error need fix part code.",
    "code": "---\ntitle: \"Home\"\ndescription: |\n  Description of the page\nsite: distill::distill_website\n---"
  },
  {
    "path": "data.html",
    "id": "summary-4",
    "chapter": "3 Data",
    "heading": "3.7 Summary",
    "text": "Pulling data R key factor data science process.Use files like CSV, Excel, RDS, JSON, SQL files/databases organize share data.Use SelectorGadget pull data websites, make sure ’s legal .Use API get data government agencies companies.Publish results website using Distill.chapter pulling data R. ’s graphing data, cleaning , wrangling , anything like , ’s just pulling data without losing mind.chapter, looked common file formats used readr package read pull data can use data plots. allowed us download data websites like kaggle.com use R session.also looked databases can pull select pieces data aren’t overloading computers thousands millions data rows. went can write SQL queries can acheive special effects without causing errors detonating computers.finally, looked can pull data websites webscraping APIs, letting us pull data internet quickly easily. lets us find data projects load R session without create download file internet.end, chapter getting data people using inside projects.\nFIGURE 3.5: Make sure use date data.\n",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "rubin-causal-model",
    "chapter": "4 Rubin Causal Model",
    "heading": "4 Rubin Causal Model",
    "text": "ever wondered world like without ?George Bailey, character movie “’s Wonderful Life,” believes life served purpose. movie follows George explores world never born. clear profound impact lives many people community. actions mattered, ever realized.showing world like without George, get idea causal effect life town people live . chapter explains causation using framework potential outcomes Rubin Causal Model (RCM).start, mentioned several concepts, Preceptor Table, never see textbooks – ’s exclusive concept used Primer. However, whole chapter ultimately seeks help visualize data allow figure whether question truly worth pursuing .",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "preceptor-table",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.1 Preceptor Table",
    "text": "Preceptor Table table rows columns , none data missing, thing want know trivial calculate. Preceptor Tables vary number rows columns. use question marks indicate missing data Preceptor Table. rows Preceptor Table units — people, galaxies, oak trees — subjects interest. Even simplest Preceptor Table two columns. first ID column serves label unit. second column outcome interest, variable trying predict/understand/change.Assume five adult brothers given four heights. average height five brothers? Consider Preceptor Table problem: case, row brother column height. individual unit brother. outcome brother’s height. always ID column Preceptor Tables can identify different units. always furthest left. addition ID column, call column brothers’ heights outcome column.calculate average height, need know Andy’s height – missing data. Keep mind truth , state world independent knowledge . Andy specific height. complete Preceptor Table, missing values, calculate average height brothers exactly. fancy statistics needed, just arithmetic.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "harvard-height",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.1.1 Harvard Height",
    "text": "Consider complex problem. heights 100 Harvard students, want know average height students school. , 100 students randomly sampled? estimate 90th percentile height student population? questions complicated, might less confident best guess. Note, also, sometimes know exactly many rows Precptor Table. case, use “Student N” last ID, N total number students Harvard.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "population-table",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.1.2 Population Table",
    "text": "Population Table distinct Preceptor Table. aim Population Table illustrate broader population interested , including data Preceptor Table dataset. table three sources data: data units want (Preceptor Table), data units actually (actual data), data units care (rest population, included data Preceptor Table). represents pieces data actually .Preceptor Table rows contain information want know order answer questions. rows contain entries covariates (sex year) contain outcome results (height). trying answer questions height Harvard students 2021, age column read somewhere 15 27 year entries rows read “2021”.actual data rows contain information know. rows contain entries covariates outcomes. case, actual data comes data Harvard students 2015, age column read matching age year entries rows either read “2015”.population rows contain data. subjects fall desired population, data. , rows missing.Population2010??............Data201518180Data201523163............Population2018??............Preceptor Table202119?............Population2025??Implicit Preceptor Table notion time. Now can see actual data compared greater population desired data, must expand observations. say , given data sourced 2015 desired data 2021, must include greater time span predictions., see rows larger population may include anywhere 2010 2025. ballpark range. Height relatively stable, simpler expand timeframe. greater sense time future problems depend individual circumstance.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "causal-effect",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.2 Causal effect",
    "text": "\nFIGURE 4.1: study, shows impact spanish-speaking attitudes towards immigration, conducted Ryan Enos.\nRubin Causal Model (RCM) based idea potential outcomes. example, Enos (2014) measured attitudes toward immigration among Boston commuters. Individuals exposed one two possible conditions, attitudes towards immigrants recorded. One condition waiting train platform near individuals speaking Spanish. train platform without Spanish-speakers. calculate causal effect Spanish-speakers nearby, need compare outcome individual one possible state world (Spanish-speakers) outcome individual another state world (without Spanish-speakers). However, impossible observe potential outcomes . One potential outcomes always missing, since unit travel back time, experience treatments. dilemma Fundamental Problem Causal Inference.circumstances, interested comparing two experimental manipulations, one generally termed “treatment” “control.” difference potential outcome treatment potential outcome control “causal effect” “treatment effect.” According RCM, causal effect platform Spanish-speakers difference attitude “treatment” (Spanish-speakers) “control” (Spanish-speakers).commuter survey consisted three questions, measuring agreement 1 5 integer scale, 1 liberal 5 conservative. person, three answers summed, generating overall measure attitude toward immigration ranged 3 (liberal) 15 (conservative). attitude towards immigrants 13 Spanish-speakers 9 without Spanish-speakers, causal effect platform Spanish-speakers 4-point increase score.use symbol \\(Y\\) represent potential outcomes, variable interested understanding modeling. \\(Y\\) called response outcome variable. variable want “explain.” case attitude score. trying understand causal effect, need two symbols control treated values can represented separately: \\(Y_t\\) \\(Y_c\\).",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "potential-outcomes",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.2.1 Potential outcomes",
    "text": "Suppose Yao one commuters surveyed experiment. omniscient, know outcomes Yao treatment (Spanish-speakers) control (Spanish-speakers), ’d able ignore Fundamental Problem Causal Inference. can show using Preceptor Table. Calculating number interested trivial none data missing. table know causal effect Yao. Everyone else study might lower (liberal) higher (conservative) attitude scores treated. Regardless causal effect subjects, causal effect Yao train platform Spanish-speakers shift towards conservative attitude.Using response variable — actual symbol rather written description — makes concise Preceptor Table.Yao139 Recall “causal effect” difference Yao’s potential outcome treatment potential outcome control.Yao139+4 Remember , real world, bunch missing data! can use simple arithmetic calculate causal effect Yao’s attitude toward immigration. Instead, required estimate . estimand unknown variable real world trying measure. case, \\(Y_{t}-Y_{c}\\), \\(+4\\). estimand value calculated, rather unknown variable want estimate.\nFIGURE 4.2: Don Rubin professor Statistics Harvard.\n",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "causal-and-predictive-models",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.2.2 Causal and predictive models",
    "text": "Causal inference often compared prediction. prediction, want know outcome, \\(Y(u)\\). causal inference, want know function potential outcomes, treatment effect: \\(Y_t(u) - Y_c(u)\\).missing data problems. Prediction involves estimating outcome variable don’t , thus missing, whether future data unable collect. Thus, prediction term using statistical inference fill missing data individual outcomes. Causal inference, however, term filling missing data numerous potential outcomes. unlike prediction, one potential outcome can ever observed, even principle.Key point: predictive model, one \\(Y(u)\\) value unit. different RCM (least) two potential outcomes (treatment control). one outcome column predictive model, whereas two causal model.predictive model, infer happen outcome \\(Y(u)\\) changed \\(X\\) given unit. can compare two units, one one value \\(X\\) another different value \\(X\\).sense, models predictive. However, subset models causal, meaning , given individual, can change value \\(X\\) observe change outcome, \\(Y(u)\\), calculate causal effect.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "no-causation-without-manipulation",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.2.3 No causation without manipulation",
    "text": "order potential outcome make sense, must possible, least priori. example, way Yao, circumstance, ever train study, \\(Y_{t}(u)\\) impossible . can never happen. \\(Y_{t}(u)\\) can never observed, even theory, causal effect treatment Yao’s attitude undefined.causal effect train study well defined simple difference two potential outcomes, might happen. case, (something else) can manipulate world, least conceptually, possible one thing different thing might happen.definition causal effects becomes much problematic way one potential outcomes happen, ever. example, causal effect Yao’s height weight? might seem just need compare two potential outcomes: Yao’s weight treatment (treatment defined 3 inches taller) Yao’s weight control (control defined current height).moment’s reflection highlights problem: can’t increase Yao’s height. way observe, even conceptually, Yao’s weight taller way make taller. can’t manipulate Yao’s height, makes sense investigate causal effect height weight. Hence slogan: causation without manipulation.raises question can manipulated. something manipulated, consider causal. can race ever considered causal? sex? genetic condition like color-blindness? Can manipulate characteristics? modern world questions simple.Take color-blindness example. Say interested color-blindness impacts ability complete jig-saw puzzle. color-blindness genetic might argue manipulated. advances technology like gene-therapy might allow us actually change someone’s genes. claim ability manipulate color-blindness? yes, measure causal effect color-blindness ability complete jig-saw puzzles.slogan “causation without manipulation” may first seem straight forward, clearly simple. Questions race, sex, gender genetics complex considered care.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "multiple-units",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.2.4 Multiple units",
    "text": "Generally, study many individuals (, broadly, “units”) potential outcomes. notation needed allow us differentiate different units.words, needs distinction \\(Y_t\\) Yao, \\(Y_t\\) Emma. use variable \\(u\\) (\\(u\\) “unit”) indicate outcome control outcome treatment can differ individual unit (person).Instead \\(Y_t\\), use \\(Y_t(u)\\) represent “Attitude Treated.” want talk Emma, say “Emma’s Attitude Treated” “\\(Y_t(u = Emma)\\)” “\\(Y_t(u)\\) Emma”, just \\(Y_t\\). notation ambiguous one subject.Let’s look Preceptor Table subjects using new notation:Yao139+4Emma1411+3Cassidy116+5Tahmid912-3Diego34-1 Preceptor Table, many possible estimands might interested . Consider examples, along true values:potential outcome one person, e.g., Yao’s potential outcome treatment: \\(13\\).causal effect one person, Emma. difference potential outcomes: \\(14 - 11 = +3\\).positive causal effect: \\(+5\\), Cassidy.negative causal effect: \\(-3\\), Tahmid.median causal effect: \\(+3\\).median percentage change: \\(+27.2\\%\\). see , calculate percentage change person. ’ll get 5 percentages: \\(+44.4\\%\\), \\(+27.2\\%\\), \\(+83.3\\%\\), \\(-25.0\\%\\), \\(-25.0\\%\\).Similar concepts can also applied Population Table, similar Preceptor Tables, can also answer several questions one may interested knowing.Population2010????..................Data2015Tao53+2Data2015Cassidy-12-3..................Population2018????..................Preceptor Table2021Tao139+4..................Population2025????example, get much better picture data, combines one nice looking Population Table. can take look past data Yao, Cassidy, previous outcomes causal effects. can also see rest units fall desired population, don’t data , hence question makes.Consider examples:Difference potential outcome one person, eg., difference Yao’s \\(Y_t(u)\\) values: \\(-8\\)Difference causal effect person, Yao \\(-2\\): \\(+2- +4\\)variables calculated Preceptor Population Tables examples estimands might interested . One estimand important enough name: average treatment effect, often abbreviated ATE. average treatment effect mean individual causal effects. , mean \\(+1.6\\).real-world Preceptor Table look like?Yao13??Emma14??Cassidy?6?Tahmid?12?Diego3??Yao13Emma14Cassidy6Tahmid12Diego3 Calculating values table longer simple math problem. See discussion Harvard Professor Matt Blackwell:",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "simple-models",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.3 Simple models",
    "text": "can fill question marks? Fundamental Problem Causal Inference, can never know missing values. can never know missing values, must make assumptions. “Assumption” just means need “model,” models parameters.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "a-single-value-for-tau",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.3.1 A single value for tau",
    "text": "One model might causal effect everyone. single parameter, \\(\\tau\\), estimate. (\\(\\tau\\) Greek letter, written “tau” rhyming “cow.”) estimate, can fill Preceptor Table , knowing , can estimate unobserved potential outcome person. use assumption \\(\\tau\\) estimate counterfactual outcome unit.Remember Preceptor Table looks like missing data:Yao13??Emma14??Cassidy?6?Tahmid?12?Diego3?? assume \\(\\tau\\) treatment effect everyone, fill table? using \\(\\tau\\) estimate causal effect. definition: \\(Y_t(u) - Y_c(u) = \\tau\\). Using simple algebra, clear \\(Y_t(u) = Y_c(u) + \\tau\\) \\(Y_c(u) = Y_t(u) - \\tau\\). words, add observed value every observation control group (subtract observed value every observation treatment group), thus fill missing values.Assuming constant treatment effect, \\(\\tau\\), everyone, filling missing values look like :Yao13$$13 - \\tau$$$$\\tau$$Emma14$$14 - \\tau$$$$\\tau$$Cassidy$$6 + \\tau$$6$$\\tau$$Tahmid$$12 + \\tau$$12$$\\tau$$Diego3$$3 - \\tau$$$$\\tau$$ Now need find estimate \\(\\tau\\) order fill missing values. One approach subtract average observed control values average observed treated values. \\[((13 + 14 + 3) / 3) - ((6 + 12) /  2)\\] \\[10 - 9 = +1\\], words, use formula:\\[$\\frac{\\Sigma Y_t(u)}{n_t} + \\frac{\\Sigma Y_c(u)}{n_c} = \\widehat{ATE}\\]\\(\\Sigma\\) represents sum treated/control values, n represents amount values within control treated group.formula something called \\(\\widehat{ATE}\\), discuss depth later section.Continuing example, calculating ATE causal effect, gives us estimate \\(+1\\) \\(\\tau\\). Let’s fill missing values adding \\(\\tau\\) observed values control subtracting \\(\\tau\\) observed value treatment like :Yao13$$13 - (+1)$$+1Emma14$$14 - (+1)$$+1Cassidy$$6 + (+1)$$6+1Tahmid$$12 + (+1)$$12+1Diego3$$3 - (+1)$$+1 gives us:Yao1312+1Emma1413+1Cassidy76+1Tahmid1312+1Diego32+1 make assumption single value \\(\\tau\\) \\(1\\) good estimate value, can determine missing potential outcomes. Preceptor Table longer missing values, can use easily answer (almost) conceivable question. ",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "two-values-for-tau",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.3.2 Two values for tau",
    "text": "second model might assume causal effect different levels category within levels. example, perhaps \\(\\tau_F\\) females \\(\\tau_M\\) males \\(\\tau_F != \\tau_M\\). making assumption give us different model can fill missing values Preceptor Table. can’t make progress unless make assumptions. inescapable result Fundamental Problem Causal Inference.Consider model causal effects differ based sex. looking “category” units — instance, gender — call covariate. covariate independent variable can influence outcome given statistical trial, direct interest. Possible covariates include, limited , sex, age, political party.Yao13$$13 - \\tau_M$$Male$$\\tau_M$$Emma14$$14 - \\tau_F$$Female$$\\tau_F$$Cassidy$$6 + \\tau_F$$6Female$$\\tau_F$$Tahmid$$12 + \\tau_M$$12Male$$\\tau_M$$Diego3$$3 - \\tau_M$$Male$$\\tau_M$$ two different estimates \\(\\tau\\).\\(\\tau_M\\) \\[(13+3)/2 - 12 = -4\\]\n\\(\\tau_F\\) \\[(14-6 = +8)\\]Using values, fill new table like :Yao13$$13 - (-4)$$-4Emma14$$14 - (+8)$$+8Cassidy$$6 + (+8)$$6+8Tahmid$$12 + (-4)$$12-4Diego3$$3 - (-4)$$-4 gives us:Yao1317-4Emma146+8Cassidy146+8Tahmid812-4Diego37-4 now two different estimates Emma (everyone else table). estimate \\(Y_c(Emma)\\) using assumption constant treatment effect (single value \\(\\tau\\)), get \\(Y_c(Emma) = 13\\). estimate assuming treatment effect constant sex, calculate \\(Y_c(Emma) = 8\\). difference estimates Emma highlights difficulties inference. Models drive inference. Different models produce different inferences.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "heterogenous-treatment-effects",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.3.3 Heterogenous treatment effects",
    "text": "assumption constant treatment effect, \\(\\tau\\), usually true? ! never true. People vary. effect pill always different effect pill friend, least measure outcomes accurately enough. Treatment effects always heterogeneous, meaning vary across individuals.Reality looks like :Yao13$$13 - \\tau_{yao}$$$$\\tau_{yao}$$Emma14$$14 - \\tau_{emma}$$$$\\tau_{emma}$$Cassidy$$6 + \\tau_{cassidy}$$6$$\\tau_{cassidy}$$Tahmid$$12 + \\tau_{tahmid}$$12$$\\tau_{tahmid}$$Diego3$$3 - \\tau_{diego}$$$$\\tau_{diego}$$ Can solve \\(\\tau_{yao}\\)? ! Fundamental Problem Causal Inference. can make progress unwilling assume least structure causal effect across different individuals? Instead worrying causal effect specific individuals, , instead, focus causal effect entire population.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "average-treatment-effect",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.3.4 Average treatment effect",
    "text": "average treatment effect (ATE) average difference potential outcomes treated group control groups. averaging linear operator, average difference difference averages. distinction estimand estimands like \\(\\tau\\), \\(\\tau_M\\) \\(\\tau_F\\), , case, care using average treatment effect fill missing values row. average treatment effect useful don’t assume anything individuals’ \\(\\tau\\), like \\(\\tau_{yao}\\), can still understand something average causal effect across whole population., simplest way estimate ATE take mean treated group (\\(10\\)) mean control group (\\(9\\)) take difference means (\\(1\\)). use method estimate ATE, ’ll call \\(\\widehat{ATE}\\), pronounced “ATE-hat.”already exact calculation , talking ? Remember unwilling assume treatment effect constant study population, solve \\(\\tau\\) \\(\\tau\\) different different individuals. \\(\\widehat{ATE}\\) helpful.estimands may require filling question marks Preceptor Table. can get good estimate average treatment effect without filling every question mark — average treatment effect just single number. Rarely study care happens individuals. case, don’t care specifically happen Cassidy’s attitude treated. Instead, care generally experiment impacts people’s attitudes towards immigrants. average estimate, like \\(\\widehat{ATE}\\) can helpful.noted , popular estimand. ?’s obvious estimator estimand: mean difference observed outcomes treated group control group: \\(Y_t(u) - Y_c(u)\\).’s obvious estimator estimand: mean difference observed outcomes treated group control group: \\(Y_t(u) - Y_c(u)\\).treatment randomly assigned, estimator unbiased: can fairly confident estimate large enough treatment control group.treatment randomly assigned, estimator unbiased: can fairly confident estimate large enough treatment control group.earlier, willing assume causal effect everyone (big assumption!), can use estimate ATE, \\(\\widehat{ATE}\\), fill missing individual values Preceptor Table.earlier, willing assume causal effect everyone (big assumption!), can use estimate ATE, \\(\\widehat{ATE}\\), fill missing individual values Preceptor Table.Just ATE often useful estimand doesn’t mean always .Consider point #3. example, let’s say treatment effect vary dependent sex. males small negative effect (-4), females larger positive effect (+8). However, average treatment effect whole sample, even estimate correctly, single negative number (+1) – since positive effect females larger negative effect males.Estimating average treatment effect, calculating \\(\\widehat{ATE}\\), easy. \\(\\widehat{ATE}\\) good estimate actual ATE? , knew missing values Preceptor Table, calculate ATE perfectly. missing values may wildly different observed values. Consider Preceptor Table:Yao1310+3Emma1411+3Cassidy96+3Tahmid1512+3Diego30+3 example, indeed constant treatment effect everyone: \\(+3\\). Note observed values , unobserved values estimated ATE, \\(+1\\), pretty far actual ATE, \\(+3\\). think reasonable estimate ATE, using value constant \\(\\tau\\) might best guess. discussion, see Matt Blackwell:",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "assumptions",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.4 Assumptions",
    "text": "section, explore three topics: validity, stability, representativeness. start, look new Population Table.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "population-table-1",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.4.1 Population Table",
    "text": "earlier Population Table familiarized us three sources data making inferences: data, greater population, Preceptor Table.rows data information desire. rows Preceptor Table include covariates, outcomes. rows greater population include data, know nothing units. ",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "validity",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.4.2 Validity",
    "text": "understand validity regards Population Table, must first recognize inherent flaw experiment design: two units receive exactly treatment.doesn’t ring true, consider Spanish speaking train experiment. units Spanish-speaking platform received treatment, right? , actually!Consider different volume levels, measured decibels (dB), Spanish spoken.Yao13????Emma11????Cassidy????10Tahmid????12Diego6???? can see, certain units heard Spanish-speakers higher volumes units. entirely possible volume speaking affects outcome. also issue time spent platform. Maybe Yao tends run late, hears Spanish-speakers thirty seconds. Emma, hand, plans ahead. hears Spanish-speakers fifteen minutes train arrives! Thus, despite fact Emma Yao treatment — , hearing Spanish platform — different versions treatment.Indeed, infinite number possible treatments. crucial define one’s estimand precisely: interested difference potential outcomes Spanish spoken 10 minutes 60 dB versus control, can ignore possible columns Population Table.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "stability",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.4.3 Stability",
    "text": "Stability means relationship columns three categories rows: data, Preceptor Table, larger population drawn.height example, much easier assume stability greater period time. Changes global height occur extremely slowly, height stable across span 20 years reasonable assume. Can say example, looking attitudes immigration?something like political ideology, much harder make assertion data collected 2010 stable data collected 2025. data, instance, collected 2014. want make predictions 2021. , frankly, may difficult argue results stable re-conducted experiment.confronted uncertainty, can consider making timeframe smaller. However, still need assume stability 2014 (time data collection) today. Stability allows us ignore issue time.Alternatively, believe unlikely columns stable, two choices. First, abandon experiment. believe data useless, experiment. Second, can choose provide sort warning message conclusions: based data ten years ago, recent data available us.forming future models, ask :units X years ago likely similar outcomes future year Y?world changed drastically year Z, data collected?decide Yao today Yao week future stable, can collapse two units one unit. assumption make forming Population Table.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "representativeness",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.4.4 Representativeness",
    "text": "external validity study often directly related representativeness sample. Representativeness well sample represents larger population interested generalizing .train experiment allow us calculate causal effect people commute cars? Can calculate causal effect people New York City? generalize broader populations consider experimental estimates applicable beyond experiment. Maybe think commuters Boston New York similar enough generalize findings. also conclude people commute car fundamentally different people commute train. true, say estimate true commuters sample accurately represent broader group want generalize .Generally: chance certain type person experiment, make assumption person.Additionally, representativeness can also consider whether sample biased . Let’s look lenses missing data mechanisms. Preceptor Table missing values, easy. just calculate answer. Sadly, Preceptor Tables (almost) always missing values, resulting “missing data.” process data missing known “missing data mechanism.” two main missing data mechanisms interest: assignment mechanism sampling mechanism. assignment mechanism process units receive (“assigned”) treatment units receive control. sampling mechanism process units appear data . sampling mechanism talked section, whereas assignment mechanism discussed next section.can just select healthy people drug trial, might lead incorrect conclusion whether drug valuable. example, study might conclude got healthier drug valuable. reality, maybe got better already healthy begin . , perhaps drug actually valuable, study might considered null since used healthy people. suppose can conclude isn’t right wrong representativeness, just need discuss consider sides conducting experiment.way improve representativeness, least regarding biased sample, use sampling, good sampling mechanism. sampling mechanism process units appear data . trying estimate average attitude towards immigrants US, usually taking sample. process people enter sample called sampling mechanism. process people enter sample related attitude, even indirectly, estimates sample good estimates population.",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "unconfoundedness",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.4.5 Unconfoundedness",
    "text": "fourth assumption use working causal models — predictive models — “unconfoundedness.” whether unit received treatment control random, write treatment assignment “confounded.” , however, treatment assignment depends value potential outcome, treatment assignment confounded. lives easiest can (reasonably!) assume unconfoundedness. case, can estimate average treatment effect subtracting average outcome control unit average outcome treated units, .Consider “Perfect Doctor” example problems caused confounded treatment assignments. Imagine omniscient doctor knows patient respond certain drug. perfect knowledge entire Preceptor Table. Using information, always assign patient treatment best outcome, whether treatment control. Consider:Yao130105+25Emma120140-20Cassidy100170-70Tahmid115125-10Diego13510035MEAN120128-8 Perfect Doctor assign treatment Emma, Cassidy Tahmid. assign control Yao Diego. good! doctor . best treatment assignment patients. good assignment mechanism estimating average causal effect treatment assignment confounded values potential outcomes., non-Perfect Doctors, access entire Precetor Table. can see : Yao?105?Emma120??Cassidy100??Tahmid115??Diego?100?MEAN111.66102.59.16 true causal effect treatment, can see first table, -8. words, treatment lowers blood pressure average. , using just data access Perfect Doctor performs treatment assignment, — mistakenly assume random assignment — causal effect positive, treatment increase blood pressure.Unconfoundedness can also seen :’s pandemic give healthy people drugs, see significant improvement health\nmay drug, people already healthy .\nmay drug, people already healthy .’re handing free breadsticks Harvard students got 97/100 test, see cry \n’re sobbing hate breadsticks, ’re sobbing think ’ve brought dishonor family\n’re sobbing hate breadsticks, ’re sobbing think ’ve brought dishonor familyNow ’ve established unconfoundedness sucks, let’s discuss can prevent . ensure covariants potential outcomes don’t affect assignment mechanism. Sounds easy!way actually care randomization, randomly assign control treatment groups. Flip coin, draw sticks, whatever. long use randomization assignment mechanism, ’re good. possibility can’t use pure randomization due ethical practical reasons, forced use non-random assignment mechanisms. Many statistical methods developed causal inference non-random assignment mechanism. methods, however, beyond scope book. , can pretend don’t exist, just say “try best randomize everything!”",
    "code": ""
  },
  {
    "path": "rubin-causal-model.html",
    "id": "summary-5",
    "chapter": "4 Rubin Causal Model",
    "heading": "4.5 Summary",
    "text": "fundamental components every problem causal inference units, treatments outcomes. units rows tibble. treatments columns. outcomes values. Whenever confront problem causal inference, start identifying units, treatments outcomes.causal effect difference one potential outcome another. different life missed train?Preceptor Table includes data like solve problem. Preceptor Table involves missing data. know outcome unit \\(\\) treatment control. ideal Preceptor Table easy calculate, using algebra, quantity interest.Population Table three sources data: Preceptor Table, dataset, greater population. rows Preceptor Table, know covariates (outcomes). Rows dataset include covariates outcomes. Rows greater population include data (since observed units).causal effect treatment single unit point time difference value outcome variable treatment without treatment. call “potential outcomes” , , can observe one . Fundamental Problem Causal Inference impossible observe causal effect single unit. must make assumptions — .e, must make models — order estimate causal effects.Random assignment treatments units best way estimate causal effects. assignment mechanisms subject confounding. treatment assigned correlated potential outcomes, hard estimate true treatment effect. (always, use terms “causal effects” “treatment effects” interchangeably. random assignment, can, mostly safely, estimate average treatment effect (ATE) looking difference average outcomes treated control units.wary claims made situations without random assignment: dragons!",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "probability",
    "chapter": "5 Probability",
    "heading": "5 Probability",
    "text": "chapter draft. errors.usual touchstone whether someone asserts mere persuasion least subjective conviction, .e., firm belief, betting. Often someone pronounces propositions confident inflexible defiance seems entirely laid aside concern error. bet disconcerts . Sometimes reveals persuaded enough one ducat ten. happily bet one, ten suddenly becomes aware previously noticed, namely quite possible erred. -— Immanuel Kant, Critique Pure ReasonThe central tension, opportunity, data science interplay data science, empirical observations models use understand . Probability language use explore interplay; connects models data, data models.mean Trump 30% chance winning election fall 2016? 90% probability rain today? dice casino fair?Probability quantifies uncertainty. Think probability proportion. probability event occurring number 0 1, 0 means event impossible 1 means event 100% certain.Begin simplest events: coin flips dice rolls. set outcomes sample space. fair coins dice, know :probability rolling 1 2 2/6, 1/3.probability rolling 1, 2, 3, 4, 5, 6 1.probability flipping coin getting tails 1/2.probability outcome unknown, often refer unknown parameter, something might use data estimate. usually use Greek letters refer parameters. Whenever talking specific probability (represented single value), use \\(\\rho\\) (Greek letter “rho” spoken aloud “p” us) subscript specifies exact outcome probability. instance, \\(\\rho_h = 0.5\\) denotes probability getting heads coin toss coin fair. \\(\\rho_t\\) — spoken “PT” “P sub T” “P tails” — denotes probability getting tails coin toss. notation can become annoying outcome whose probability seek less concise. example, might write probability rolling 1, 2 3 using fair die :\\[\n\\rho_{die\\ roll\\ \\ 1,\\ 2\\ \\ 3} = 0.5\n\\]rarely write full definition event \\(\\rho\\) symbol. Instead, define event “” rolled die equals 1, 2 3 , , write\\[\\rho_a = 0.5\\]random variable function produces value sample set. random variable can either discrete — sample set limited number members, like H T result coin flip, 2, 3, …, 12 sum two die — continuous (value within range). Probability claim value random variable, .e., 50% probability getting 1, 2 3 roll fair die.usually use capital letters random variables. , \\(C\\) might symbol random variable coin toss \\(D\\) might symbol random variable sum two dice. discussing random variables general, grow tired coming new symbols, use \\(Y\\).Small letters refer single outcome result random variable. \\(c\\) outcome one coin toss. \\(d\\) result one throw dice. value outcome must come sample space. , \\(c\\) can take two possible values: heads tails. discussing random variables general, use \\(y\\) refer one outcome random variable \\(Y\\). multiple outcomes — , example, flipped coin multiple times — use subscripts indicate separate outcomes: \\(y_1\\), \\(y_2\\), . symbol arbitrary outcome \\(y_i\\), \\(\\) ranges 1 \\(N\\), total number events experiments outcome \\(y\\) produced.package need chapter tidyverse.understand probability fully, first need understand distributions.",
    "code": "\nlibrary(tidyverse)"
  },
  {
    "path": "probability.html",
    "id": "distributions",
    "chapter": "5 Probability",
    "heading": "5.1 Distributions",
    "text": "variable tibble column, vector values. sometimes refer vector “distribution.” somewhat sloppy distribution can many things, commonly mathematical formula. , strictly speaking, “frequency distribution” “empirical distribution” list values, usage unreasonable.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "scaling-distributions",
    "chapter": "5 Probability",
    "heading": "5.1.1 Scaling distributions",
    "text": "Consider vector result rolling one die 10 times.ways storing data vector. Instead reporting every observation, record number times value appears percentage total number accounts .case, 10 values, actually less efficient store data like . happens 1,000 rolls?Instead keeping around vector length 1,000, can just keep 12 values — 6 possible outcomes frequency — without losing information.Two distributions can identical even different lengths. Let’s compare original distribution 10 rolls die another distribution just features 100 copies 10 rolls.two graphs exact shape , even though vectors different lengths, relative proportions outcomes identical. sense, vectors distribution. Relative proportions, total counts, matter.",
    "code": "\nten_rolls <- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)\nmore_rolls <- rep(ten_rolls, 100)"
  },
  {
    "path": "probability.html",
    "id": "normalizing-distributions",
    "chapter": "5 Probability",
    "heading": "5.1.2 Normalizing distributions",
    "text": "two distributions shape, differ labels y-axis. various ways “normalizing” distributions make scale. common scale one area distribution adds 1, e.g., 100%. example, can transform plots look like:sometimes refer distribution “unnormalized” area curve add 1.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "simulating-distributions",
    "chapter": "5 Probability",
    "heading": "5.1.3 Simulating distributions",
    "text": "two distinct concepts: distribution set values drawn distribution. , everyday use, use “distribution” . given distribution (meaning vector numbers), often use geom_histogram() geom_density() graph . , sometimes, don’t want look whole thing. just want summary measures report key aspects distribution. two important attributes distribution center variation around center.use summarize() calculate statistics variable, column, vector values distribution. Note language sloppiness. purposes book, “variable,” “column,” “vector,” “distribution” mean thing. Popular statistical functions include: mean(), median(), min(), max(), n() sum(). Functions may new include three measures “spread” distribution: sd() (standard deviation), mad() (scaled median absolute deviation) quantile(), used calculate interval includes specified proportion values.Think distribution variable urn can pull , random, values variable. Drawing thousand values urn, looking histogram, can show values centered vary. people sloppy, use word distribution refer least three related entities:(imaginary!) urn drawing values.values urnall values drawn urn, whether 10 1,000Sloppiness usage word distribution universal. However, keep three distinct ideas separate:unknown true distribution , reality, generates data see. Outside stylized examples assume distribution follows simple mathematical formula, never access unknown true distribution. can estimate . unknown true distribution often referred data generating mechanism, DGM. function black box urn produces data. can see data. can’t see urn.unknown true distribution , reality, generates data see. Outside stylized examples assume distribution follows simple mathematical formula, never access unknown true distribution. can estimate . unknown true distribution often referred data generating mechanism, DGM. function black box urn produces data. can see data. can’t see urn.estimated distribution , think, generates data see. , can never know unknown true distribution. , making assumptions using data , can estimate distribution. estimate may close true distribution. may far away. main task data science create use estimated distributions. Almost always, distributions instantiated computer code. Just true data generating mechanism associated (unknown) true distribution, estimated data generating mechanism associated estimated ditribution.estimated distribution , think, generates data see. , can never know unknown true distribution. , making assumptions using data , can estimate distribution. estimate may close true distribution. may far away. main task data science create use estimated distributions. Almost always, distributions instantiated computer code. Just true data generating mechanism associated (unknown) true distribution, estimated data generating mechanism associated estimated ditribution.vector numbers drawn estimated distribution. true estimated distributions can complex animals, difficult describe accurately detail. vector numbers drawn distribution easy understand use. , general, work vectors numbers. someone — either colleague piece R code — creates distribution want use answer question, don’t really want distribution . Rather, want vectors “draws” distribution. Vectors easy work ! Complex computer code .vector numbers drawn estimated distribution. true estimated distributions can complex animals, difficult describe accurately detail. vector numbers drawn distribution easy understand use. , general, work vectors numbers. someone — either colleague piece R code — creates distribution want use answer question, don’t really want distribution . Rather, want vectors “draws” distribution. Vectors easy work ! Complex computer code ., people (including us!) often sloppy use word, “distribution,” without making clear whether talking true distribution, estimated distribution, vector draws estimated distribution. sloppiness applies use term data generating mechanism. Try sloppy.Much rest Primer involves learning work distributions, generally means working draws distributions. Fortunately, usual rules arithmetic apply. can add/subtract/multiply/divide distributions working draws distributions, just can add/subtract/multiply/divide regular numbers.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "probability-distributions",
    "chapter": "5 Probability",
    "heading": "5.2 Probability distributions",
    "text": "\nFIGURE 5.1: Bruno de Finetti, Italian statistician wrote famous treatise theory probability began statement “PROBABILITY EXIST.”\npurposes Primer, probability distribution mathematical object maps set outcomes probabilities, distinct outcome chance occurring 0 1 inclusive. probabilities must sum 1. set possible outcomes, .e., sample space — heads tails coin, 1 6 single die, 2 12 sum pair dice — can either discrete continuous. Remember, discrete data can take certain values. Continuous data, like height weight, can take value within range. set outcomes domain probability distribution. range associated probabilities.Assume probability distribution created probability function, set function maps outcomes probabilities. concept “probability function” often split two categories: probability mass functions (discrete random variables) probability density functions (continuous random variables). usual, bit sloppy, using term probability distribution mapping function creates mapping.discuss three types probability distributions: empirical, mathematical, posterior.key difference distribution, explored Section 5.1, probability distribution requirement sum probabilities individual outcomes must exactly 1. requirement distribution general. distribution can turned probability distribution “normalizing” . context, often refer distribution (yet) probability distribution “unnormalized” distribution.Pay attention notation. Recall talking specific probability (represented single value), use \\(\\rho\\) (Greek letter “rho”) subscript specifies exact outcome probability. instance, \\(\\rho_h = 0.5\\) denotes probability getting heads coin toss coin fair. \\(\\rho_t\\) — spoken “PT” “P sub T” “P tails” — denotes probability getting tails coin toss. However, referring entire probability distribution set outcomes, use \\(P()\\). example, probability distribution coin toss \\(P(\\text{coin})\\). , \\(P(\\text{coin})\\) composed two specific probabilities (50% 50%) mapped two values domain (Heads Tails). Similarly, \\(P(\\text{sum two dice})\\) probability distribution set 11 outcomes (2 12) possible take sum two dice. \\(P(\\text{sum two dice})\\) made 11 numbers — \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_{12}\\) — representing unknown probability sum equal value. , \\(\\rho_2\\) probability rolling 2.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "flipping-a-coin",
    "chapter": "5 Probability",
    "heading": "5.2.1 Flipping a coin",
    "text": "data science problems start question. Example: chances getting three heads row flipping fair coin? questions answered help probability distributions.empirical distribution based data. can think probability distribution created running simulation. theory, increase number coins flip simulation, empirical distribution look similar mathematical distribution. mathematical distribution Platonic form. empirical distribution often look like mathematical probability distribution, rarely exactly .simulation, 56 heads 44 tails. outcome vary every time run simulation, proportion heads tails different coin fair.mathematical distribution based mathematical formula. Assuming coin perfectly fair, , average, get heads often get tails.distribution single observation described formula.\\[ P(Y = y) = \\begin{cases} 1/2 &\\text{}y= \\text{Heads}\\\\ 1/2 &\\text{}y= \\text{Tails} \\end{cases}\\]\nsometimes know probability heads probability tails equal 50%. case, might write:\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{}y= \\text{Heads}\\\\ \\rho_T &\\text{}y= \\text{Tails} \\end{cases}\\]Yet, know , definition, \\(\\rho_H + \\rho_T = 1\\), can rewrite asL\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{}y= \\text{Heads}\\\\ 1- \\rho_H &\\text{}y= \\text{Tails} \\end{cases}\\]Coin flipping (related scenarios two possible outcomes) common problems, notation often simplified , \\(\\rho\\) understood, convention, probability heads. case, can write mathematical distribution two canonical forms:\\[P(Y) = Bernoulli(\\rho)\\]\n\\[y_i \\sim Bernoulli(\\rho)\\]\nfive versions mean thing! first four describe mathematical probability distribution fair coin. capital \\(Y\\) within \\(P()\\) indicates random variable. fifth highlights one “draw” random variable, hence lower case \\(y\\) subscript \\(\\).probability distributions special names, use generic symbol \\(P\\) define . common probability distributions names, like “Bernoulli” case.mathematical assumptions correct, sample size increases, empirical probability distribution look like mathematical distribution.posterior distribution based beliefs expectations. displays belief things can’t see right now. may posterior distributions outcomes past, present, future.case coin toss, posterior distribution changes depending beliefs. instance, let’s say friend brought coin school asked bet . result heads, pay $5. case, posterior probability distribution might look like :full terminology mathematical (empirical posterior) probability distribution. often shorten just mathematical (empirical posterior) distribution. word “probability” understood, even present.Recall question started section: chances getting three heads row flipping fair coin? answer question, need use probability distribution data generating mechanism. Fortunately, rbinom() function allows us generate results coin flips. example:generates results 10 coin flips, result heads presented 1 tails 0. tool, can generate 1,000 draws experiment:flips independent, can consider row draw experiment. , simply count proportion experiments resulted three heads.close enough correct answer \\(1/8\\)th. increase sample size, get closer truth.first example using data generating mechanism — meaning rbinom() — answer question. see many chapters come.",
    "code": "\n# We are flipping one fair coin a hundreds times. We need to get the same result\n# each time we create this graphic because we want the results to match the\n# description in the text. Using set.seed() guarantees that the random results\n# are the same each time. We define 0 as tails and 1 as heads.\n\nset.seed(3)\n\ntibble(results = sample(c(0, 1), 100, replace = TRUE)) |> \n  ggplot(aes(x = results)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Flipping one coin a hundred times\",\n         x = \"Outcome\\nResult of Coin Flip\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = c(0, 1), \n                       labels = c(\"Heads\", \"Tails\")) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\nrbinom(n = 10, size = 1, prob = 0.5)##  [1] 1 1 0 1 1 0 0 0 0 0\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5))## # A tibble: 1,000 × 3\n##    toss_1 toss_2 toss_3\n##     <int>  <int>  <int>\n##  1      0      1      1\n##  2      0      1      1\n##  3      0      1      0\n##  4      0      0      1\n##  5      1      1      0\n##  6      1      0      1\n##  7      1      0      0\n##  8      1      0      1\n##  9      0      0      1\n## 10      0      1      1\n## # … with 990 more rows\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5)) |> \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |> \n  summarize(chance = mean(three_heads))## # A tibble: 1 × 1\n##   chance\n##    <dbl>\n## 1  0.104"
  },
  {
    "path": "probability.html",
    "id": "rolling-two-dice",
    "chapter": "5 Probability",
    "heading": "5.2.2 Rolling two dice",
    "text": "get empirical distribution rolling two dice hundred times, either hand computer simulation. result identical mathematical distribution inherent randomness real world /simulation.might consider labeling y-axis plots empirical distributions “Proportion” rather “Probability” since actual proportion, calculated real (simulated) data. keep “Probability” since want emphasize parallels mathematical, empirical posterior probability distributions.mathematical distribution tells us , fair dice, probability getting 1, 2, 3, 4, 5, 6 equal: 1/6 chance . roll two dice time sum numbers, values closest middle common values edge combinations numbers add middle values.\\[ P(Y = y) = \\begin{cases} \\dfrac{y-1}{36} &\\text{}y=1,2,3,4,5,6 \\\\ \\dfrac{13-y}{36} &\\text{}y=7,8,9,10,11,12 \\\\ 0 &\\text{otherwise} \\end{cases} \\]posterior distribution rolling two dice hundred times depends beliefs. take dice Monopoly set, reason believe assumptions underlying mathematical distribution true. However, walk crooked casino host asks play craps, might suspicious, just “flipping coin example” word “suspicious” means longer trust “population” mathematical empircal distribution drawn data . example, craps, come-roll 7 11 “natural,” resulting win “shooter” loss casino. might expect numbers occur less often fair dice. Meanwhile, come-roll 2, 3 12 loss shooter. might also expect values like 2, 3 12 occur frequently. posterior distribution might look like :Someone less suspicious casino posterior distribution looks like mathematical distribution.",
    "code": "\n# In the coin example, we create the vector ahead of time, and then assigned\n# that vector to a tibble. There was nothing wrong with that approach. And we\n# could do the same thing here. But the use of map_* functions is more powerful,\n# although it requires creating the 100 rows of the tibble at the start and then\n# doing things \"row-by_row.\"\n\nset.seed(1)\n\nemp_dist_dice <- tibble(ID = 1:100) |> \n  mutate(die_1 = map_dbl(ID, ~ sample(c(1:6), size = 1))) |> \n  mutate(die_2 = map_dbl(ID, ~ sample(c(1:6), size = 1))) |> \n  mutate(sum = die_1 + die_2) |> \n  ggplot(aes(x = sum)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 1, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Sum from rolling two dice, replicated one hundred times\",\n         x = \"Outcome\\nSum of Two Die\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = seq(2, 12, 1), labels = 2:12) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\nemp_dist_dice"
  },
  {
    "path": "probability.html",
    "id": "presidential-elections",
    "chapter": "5 Probability",
    "heading": "5.2.3 Presidential elections",
    "text": "Now let’s say building probability distributions political events, like presidential election. want know probability Democratic candidate wins X electoral votes, X comes range possible outcomes: 0 538. (total number electoral votes US elections since 1964 538.)empirical distribution case involve looking past elections United States counting number electoral votes Democrats won . empirical distribution, create tibble electoral vote results past elections. Looking elections since 1964, can observe number electoral votes Democrats received one different. Given 15 entries, difficult draw conclusions make predictions based empirical distribution.can build mathematical distribution X assumes chances Democratic candidate winning given state’s electoral votes 0.5 results state independent.assumptions mathematical distribution correct (), sample size increase, empirical distribution look similar mathematical distribution.However, data past elections enough demonstrate assumptions mathematical probability distribution work electoral votes. model assumes Democrats 50% chance receiving 538 votes. Just looking mathematical probability distribution, can observe receiving 13 17 486 votes 538 extreme almost impossible mathematical model accurate. However, empirical distribution shows extreme outcomes quite common. Presidential elections resulted much bigger victories defeats distribution seems allow .posterior distribution electoral votes popular topic, area strong disagreement, among data scientists. Consider posterior FiveThirtyEight.posterior FiveThirtyEight website August 13, 2020. created using data distribution, simply displayed differently. electoral result, height bar represents probability given event occur. However, lablels y-axis telling us specific probability outcome . OK! specific values useful. removed labels y-axes, matter?posterior Economist, also August 13, 2020. looks confusing first chose merge axes Republican Democratic electoral votes. can tell Economist less optimistic, relative FiveThirtyEight, Trump’s chances election.two models, built smart people using similar data sources, reached fairly different conclusions. Data science difficult! one “right” answer. Real life problem set.\nFIGURE 5.2: Watch makers two models throw shade Twitter! Eliot Morris one primary authors Economist model. Nate Silver charge 538. don’t seem impressed ’s work! smack talk .\nmany political science questions explore posterior distributions. can relate past, present, future.Past: many electoral votes Hilary Clinton won picked different VP?Present: total campaign donations Harvard faculty?Future: many electoral votes Democratic candidate president win 2024?",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "height",
    "chapter": "5 Probability",
    "heading": "5.2.4 Height",
    "text": "Question: height next adult male meet?three examples discrete probability distributions, meaning outcome variable can take limited set values. coin flip two outcomes. sum pair dice 11 outcomes. total electoral votes Democratic candidate 539 possible outcomes. limit, can also create continuous probability distributions infinite number possible outcomes. example, average height American male real number 0 inches 100 inches. (course, average value anywhere near 0 100 absurd. point average 68.564, 68.5643, 68.56432 68.564327, real number.)characteristics discrete probability distributions reviewed apply just much continuous probability distributions. example, can create mathematical, empirical posterior probability distributions continuous outcomes just discrete outcomes.empirical distribution involves using data National Health Nutrition Examination Survey (NHANES). instead making model ourself using mathematical formula, use actual data, can get data either simulated like “flipping coin” “Rolling two dice” scenario, used data someone else, like presidential election scenario.Mathematical distribution complete based mathematical formula assumptions like Flipping coin session assume coin perfectly fair coin probability landing heads tails equal. case, assume average hight men 175 cm, well standard deviation height around 9 cm. two values, average also called mean, standard deviation (sd), can create normal distribution using rnorm() function. normal distribution good approximation generalization height scenario.Mathematical Distribution:, Normal distribution probability distribution symmetric mean described formula.\\[y_i \\sim N(\\mu, \\sigma^2)\\].value \\(y_i\\) drawn normal distribution parameters \\(\\mu\\) mean \\(\\sigma\\) standard deviation. mathematical assumptions correct case two parameters \\(\\mu\\) \\(\\sigma\\), sample size increases, empirical probability distribution look like mathematical distribution.posterior distribution heights depends context. considering adult men America? case, posterior probably look lot like empirical distribution using NHANES data. asked distribution heights among players NBA, posterior might look like:Comments:Continuous variables myth. Nothing can represented computer truly continuous. Even something appears continuous, like height, actually can take (large) set discrete variables.Continuous variables myth. Nothing can represented computer truly continuous. Even something appears continuous, like height, actually can take (large) set discrete variables.math continuous probability distributions can tricky. Read book mathematical probability messy details. Little matters applied work.math continuous probability distributions can tricky. Read book mathematical probability messy details. Little matters applied work.important difference , discrete distributions, makes sense estimate probability specific outcome. probability rolling 9? continuous distributions, makes sense infinite number possible outcomes. continuous variables, estimate intervals.important difference , discrete distributions, makes sense estimate probability specific outcome. probability rolling 9? continuous distributions, makes sense infinite number possible outcomes. continuous variables, estimate intervals.Don’t worry distinctions discrete continuous outcomes, discrete continuous probability distributions use summarize beliefs outcomes. basic intuition cases.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "joint-distributions",
    "chapter": "5 Probability",
    "heading": "5.2.5 Joint distributions",
    "text": "Recall \\(P(\\text{coin})\\) probability distribution result coin toss. includes two parts, probability heads (\\(\\rho_h\\)) probability tails (\\(\\rho_t\\)). univariate distribution one outcome, can heads tails. one outcome, joint distribution.Joint distributions also mathematical objects cover set outcomes, distinct outcome chance occurring 0 1 sum chances must equal 1. key joint distribution measures chance events B occur. notation \\(P(, B)\\).Let’s say rolling two six-sided dice simultaneously. Die 1 weighted 50% chance rolling 6 10% chance values. Die 2 weighted 50% chance rolling 5 10% chance rolling values. Let’s roll dice 1,000 times. previous examples involving two dice, cared sum results outcomes first versus second die simulation. joint distributions, order matters; instead 11 possible outcomes x-axis distribution plot (ranging 2 12), 36. Furthermore, 2D probability distribution sufficient represent variables involved, joint distribution example displayed using 3D plot.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "conditional-distrubutions",
    "chapter": "5 Probability",
    "heading": "5.2.6 Conditional distrubutions",
    "text": "Imagine 60% people community disease. doctor develops test determine random person disease. However, test isn’t 100% accurate. 80% probability correctly returning positive person disease 90% probability correctly returning negative person disease.probability random person disease 0.6. Since person either disease doesn’t (two possibilities), probability person disease \\(1 - 0.6 = 0.4\\).random person disease, go top branch. probability infected person testing positive 0.8 test 80% sure correctly returning positive person disease.random person disease, go top branch. probability infected person testing positive 0.8 test 80% sure correctly returning positive person disease.logic, random person disease, go bottom branch. probability person incorrectly testing positive 0.1.logic, random person disease, go bottom branch. probability person incorrectly testing positive 0.1.decide go top branch random person disease. go bottom branch . called conditional probability. probability testing positive dependent whether person disease.express statistical notation? \\(P(|B)\\) thing probability given B. \\(P(|B)\\) essentially means probability know sure value B. Note \\(P(|B)\\) thing \\(P(B|)\\).summary, work three main categories probability distributions. First, p() probability distribution event . sometimes refered univariate probability distribution one random variable. Second, p(, B) joint probability distribution B. Third, p(| B) conditional probability distribution given B taken specific value. often written p(| B = b).",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "list-columns-and-map-functions",
    "chapter": "5 Probability",
    "heading": "5.3 List-columns and map functions",
    "text": "working probability models, need expand collection R tricks understanding list-columns map_* functions. Recall list different atomic vector. atomic vectors, element vector one value. Lists, however, can contain vectors, even complex objects, elements.x list two elements. first element numeric vector length 3. second element character vector length 2. use [[]] extract specific elements.number built-R functions output lists. example, ggplot objects making store plot information lists. function returns multiple values can used create list output wrapping returned object list().Notice 1--1 tibble one observation, list one element. Voila! just created list-column.function returns multiple values vector, like range() , must use list() wrapper want create list-column.list column column data list rather atomic vector. stand-alone list objects, can pipe str() examine column.can use map_* functions create list-column , much importantly, work list-column afterwards.map_* functions, like map_dbl() example, take two key arguments, .x (data acted ) .f (function act data). , .x data col_1, list-column. .f function sum(). However, can simply write map_dbl(col_1, sum). Instead, use map_* functions requires use tilde — ~ — indicate start function use dot — . — specify data goes function.map_* functions family functions, suffix specifying type object returned. map() returns list. map_dbl() returns double. map_int() returns integer. map_chr() returns character, .summarise map function map_* functions convert data (vector list) specific denoted functions formula set, always results list called “list Column”. two arguments map() function well map_*() function .x .f, .x .f placed map() functions map_*()functions like map(.x,.f), .x either list vector, .f either direct function like .f=mean, formula like .f= ~mean(.x), remember put ~ using .f formula, difference map map_* function know want outcome data specific data vector like (double, logical,character,integer) rather general list map(), can use map_* instead map organized list column.Consider detailed example:flexibility possible via use list-columns map_* functions. workflow extremely common. start empty tibble, using ID specify number rows. skeleton, step pipe adds new column, working column already exists.",
    "code": "\nx <- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx## [[1]]\n## [1]  4 16  9\n## \n## [[2]]\n## [1] \"A\" \"Z\"\nx[[1]]## [1]  4 16  9\nx <- rnorm(10)\n\n# range() returns the min and max of the argument \n\ntibble(col_1 = list(range(x))) ## # A tibble: 1 × 1\n##   col_1    \n##   <list>   \n## 1 <dbl [2]>\n# tibble() is what we use to generate a tibble, it acts sort of like the mutate(), but mutate() needs a data frame to add new column, tibble can survive on itself.\ntibble(col_1 = list(range(x))) |>\n  str()## tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -1.01 1.42\n# .x is col_1 from tibble and ~ sum(.) is the formula\ntibble(col_1 = list(range(x))) |>\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |> \n  str()## tibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -1.01 1.42\n##  $ col_2: num 0.415\ntibble(ID = 1) |> \n  mutate(col_1 = map(ID, ~range(rnorm(10)))) |>\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |> \n  mutate(col_3 = map_int(col_1, ~ length(.))) |> \n  mutate(col_4 = map_chr(col_1, ~ sum(.))) |> \n  str()## tibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n##  $ ID   : num 1\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -1.45 1.31\n##  $ col_2: num -0.143\n##  $ col_3: int 2\n##  $ col_4: chr \"-0.142989\"\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) |> \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) |> \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) |> \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))## # A tibble: 3 × 4\n##      ID draws        max min_max  \n##   <int> <list>     <dbl> <list>   \n## 1     1 <dbl [10]> 0.758 <dbl [2]>\n## 2     2 <dbl [10]> 1.06  <dbl [2]>\n## 3     3 <dbl [10]> 1.69  <dbl [2]>"
  },
  {
    "path": "probability.html",
    "id": "two-models",
    "chapter": "5 Probability",
    "heading": "5.4 Two models",
    "text": "simplest possible setting inference involves two models — meaning two possible states world — two outcomes experiment. Imagine disease — Probophobia, irrational fear probability — either don’t . don’t know diseases, assume two possibilities.also test 99% accurate given person Probophobia. Unfortunately, test 50% accurate people Probophobia. experiment, two possible outcomes: positive negative result test.Question: test positive, probability Probophobia?generally, estimating conditional probability. Conditional outcome postive test, probability Probophobia? Mathematically, want:\\[ P(\\text{Probophobia | Test = Postive} ) \\]answer question, need use tools joint conditional probability earlier Chapter. begin building, hand, joint distribution possible models (Probophobia ) possible outcomes (test positive negative). Building joint distribution involves assuming model true creating distribution outcomes might occur assumption true.example, assume Probophobia. 50% chance test positive 50% chance test negative. Similarly, assume second model true — don’t Probophobia — 1% chance test positive 99% chance negative. course, (individual) know sure happening. know disease. know test show. can use relationships construct joint distribution.first step simply create tibble consists simulated data need plot distribution. Keep mind setting two different probabilities completely separate want keep two probabilities disease results two two columns can graph using ggplot() function. ’s used rep seq functions creating table, used seq function set sequence wants, case two numbers, 0.01 (99% accuracy testing negative disease, therefore 1% testing positive disease) 0.5 (50% accuracy testing positive/negative disease), used rep functions repeat process 10,000 times probability, total 20,000 times. Note number “20,000” also represent number observations simulated data, simulated 20,000 results testing, 10,000 results -disease group 10,000 -disease group, often use capital N represent population, simulated data \\(N=20,000\\).Plot joint distribution:joint distribution displayed 3D. Instead using “jitter” feature R unstack dots, using 3D plot visualize number dots box. number people correctly test negative far greater categories. 3D plot shows total number cases section (True positive, True negative, False positive, False negative),3D bar coming combinations. Now,pay attention two rows 3D graph, trying add length 3D bar top two sections bottom two sections, equal , 10,000 case. simulate experience two independent separate world one -disease world one -disease world.Section called “Two Models” , person, two possible states world: disease disease. assumption, outcomes. call two possible states world “models,” even though simple models.addition two models, two possible results experiment given person: test positive test negative. , assumption. allow outcome. coming sections, look complex situations consider two models two possible results experiment. meantime, built unnormalized joint distribution models results. key point! Look back earlier Chapter discussions unnormalized distributions joint distributions.want analyze plots looking different slices. instance, let’s say tested positive disease. Since test always accurate, 100% certain . isolate slice test result equals 1 (meaning positive).people test positive infected result common diseases like cold. can easily create unnormalized conditional distribution :filter() transforms joint distribution conditional distribution.Turn unnormalized distribution posterior probability distribution:zoom plot, 70% people tested positive disease 30% tested positive disease. case, focusing one slice probability distribution test result positive. two disease outcomes: positive negative. isolating section, looking conditional distribution. Conditional positive test, can visualize likelihood actually disease versus .Now recalled question asked start session:\ntest positive, probability Probophobia?looking posterior graph just create, can answer question easily:\npositive test, can almost 70% sure Probophobia, however good chance 30% receive false positive, don’t worry much still third hope get wrong resultNow let’s consider manipulation posterior, another question.\nQuestion : 10 people walks testing center, 5 tested negative, 5 tested positive, probability least 6 people actually healthy? Stat 110 Animations video really good job explaining similar concepts.",
    "code": "\n# Pipes generally start with tibbles, so we start with a tibble which just\n# includes an ID variable. We don't really use ID. It is just handy for getting\n# organized. We call this object `jd_disease`, where the `jd` stands for\n# joint distribution.\n\nsims <- 10000\n\njd_disease <- tibble(ID = 1:sims, have_disease = rep(c(TRUE, FALSE), 5000)) |>\n  mutate(positive_test =\n           if_else(have_disease,\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.99)),\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.5))))\n\n\n\njd_disease## # A tibble: 10,000 × 3\n##       ID have_disease positive_test\n##    <int> <lgl>                <int>\n##  1     1 TRUE                     1\n##  2     2 FALSE                    1\n##  3     3 TRUE                     1\n##  4     4 FALSE                    1\n##  5     5 TRUE                     1\n##  6     6 FALSE                    0\n##  7     7 TRUE                     1\n##  8     8 FALSE                    1\n##  9     9 TRUE                     1\n## 10    10 FALSE                    0\n## # … with 9,990 more rows\njd_disease |> \n  filter(positive_test == 1)## # A tibble: 7,484 × 3\n##       ID have_disease positive_test\n##    <int> <lgl>                <int>\n##  1     1 TRUE                     1\n##  2     2 FALSE                    1\n##  3     3 TRUE                     1\n##  4     4 FALSE                    1\n##  5     5 TRUE                     1\n##  6     7 TRUE                     1\n##  7     8 FALSE                    1\n##  8     9 TRUE                     1\n##  9    11 TRUE                     1\n## 10    12 FALSE                    1\n## # … with 7,474 more rows\ntibble(test = 1:100000) |>\n  mutate(person1 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person2 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person3 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person4 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person5 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person6 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person7 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person8 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person9 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person10 = map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  select(!test) |> \n  \nmutate(sum = rowSums(across(person1:person10))) |>\n  \nggplot(aes(sum)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))),\n                 binwidth = 1,\n                   color = \"white\") +\n  scale_x_continuous(breaks = c(0:10)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()  "
  },
  {
    "path": "probability.html",
    "id": "three-models",
    "chapter": "5 Probability",
    "heading": "5.5 Three models",
    "text": "Imagine friend gives bag two marbles. either two white marbles, two black marbles, one color. Thus, bag contain 0% white marbles, 50% white marbles, 100% white marbles. Respectively, proportion, \\(p\\), white marbles 0, 0.5, 1.Question: chance bag contains exactly two white marbles, given selected marbles three times, everytime select white marble?\\[ P(\\text{2 White Marbles bag | White Marbles Sampled = 3} ) \\]\nJust Probophobia models, order answer question, need start simulated data graphing joint distribution sinerio need considered possible outcomes model, based joint distribution can slice part want (Conditional distribution) end making posterior graph well normalizing see probability.Step 1: Simulate data tibbleLet’s say take marble bag, record whether ’s black white, return bag. repeat three times, observing number white marbles see three trials. get three whites, two whites, one white, zero whites result trial. three models (three different proportions white marbles bag) four possible experimental results. Let’s create 3,000 draws joint distribution:Step 2: Plot joint distribution:3D visualization:y-axes scatterplot 3D visualization labeled “Number White Marbles Bag.” value y-axis model, belief world. instance, model 0, white marbles bag, meaning none marbles pull sample white.Now recalls question, essentially care fourth column joint distribution (x-axis=3) question asking us create conditional distribution given fact 3 marbles selected. Therefore, isolate slice result simulation involves three white marbles zero black ones. unnormalized probability distribution.Step 3: Plot unnormalized conditional distribution.Step 4: Plot normalize posterior distribution.\nNext, let’s normalize distribution.plot makes sense three marbles draw bag white, pretty good chance black marbles bag. can’t certain! possible draw three white even bag contains one white one black. However, impossible zero white marbles bag.Lastly let’s answer question:\nchance bag contains exactly two white marbles, given selected white marbles three times, everytime select white marble?Answer:\nPosterior Probability Distribution shows (x-axis=2), chance bag contains exactly two white marbles given select 3 white marbles three tries 85%.",
    "code": "\n# Create the joint distribution of the number of white marbles in the bag\n# (in_bag) and the number of white marbles pulled out in the sample (in_sample),\n# one-by-one. in_bag takes three possible values: 0, 1 and 2, corresponding to\n# zero, one and two white marbles potentially in the bag.\n\nset.seed(3)\nsims <- 10000\n\n# We also start off with a tibble. It just makes things easier\n\njd_marbles <- tibble(ID = 1:sims) |> \n  \n  # For each row, we (randomly!) determine the number of white marbles in the\n  # bag. We do not know why the `as.integer()` hack is necessary. Shouldn't\n  # `map_int()` automatically coerce the result of `sample()` into an integer?\n  \n  mutate(in_bag = map_int(ID, ~ as.integer(sample(c(0, 1, 2), \n                                                  size = 1)))) |>\n  \n  # Depending on the number of white marbles in the bag, we randomly draw out 0,\n  # 1, 2, or 3 white marbles in our experiment. We need `p = ./2` to transform\n  # the number of white marbles into the probability of drawing out a white\n  # marble in a single draw. That probability is either 0%, 50% or 100%.\n  \n  mutate(in_sample = map_int(in_bag, ~ rbinom(n = 1, \n                                              size = 3, \n                                              p = ./2))) \n\njd_marbles## # A tibble: 10,000 × 3\n##       ID in_bag in_sample\n##    <int>  <int>     <int>\n##  1     1      0         0\n##  2     2      1         3\n##  3     3      2         3\n##  4     4      1         1\n##  5     5      2         3\n##  6     6      2         3\n##  7     7      1         0\n##  8     8      2         3\n##  9     9      0         0\n## 10    10      1         2\n## # … with 9,990 more rows\n# The distribution is unnormalized. All we see is the number of outcomes in each\n# \"bucket.\" Although it is never stated clearly, we are assuming that there is\n# an equal likelihood of 0, 1 or 2 white marbles in the bag.\n\njd_marbles |>\n  ggplot(aes(x = in_sample, y = in_bag)) +\n    geom_jitter(alpha = 0.5) +\n    labs(title = \"Black and White Marbles\",\n         subtitle = \"More white marbles in bag mean more white marbles selected\",\n         x = \"White Marbles Selected\",\n         y = \"White Marbles in the Bag\") +\n    scale_y_continuous(breaks = c(0, 1, 2)) +\n  theme_classic()\n# The key step is the filter. Creating a conditional distribution from a joint\n# distribution is the same thing as filtering that joint distribution for a\n# specific value. A conditional distribution is a \"slice\" of the joint\n# distribution, and we take that slice with filter().\n\njd_marbles |> \n  filter(in_sample == 3) |> \n  ggplot(aes(in_bag)) +\n    geom_histogram(binwidth = 0.5, color = \"white\") +\n    labs(title = \"Unnormalized Conditional Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Count\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    theme_classic()\njd_marbles |> \n  filter(in_sample == 3) |> \n  ggplot(aes(in_bag)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Probability\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "path": "probability.html",
    "id": "n-models",
    "chapter": "5 Probability",
    "heading": "5.6 N models",
    "text": "Assume coin \\(\\rho_h\\). guarantee 11 possible values \\(\\rho_h\\): \\(0, 0.1, 0.2, ..., 0.9, 1\\). words, 11 possible models, 11 things might true world. just like situations previously discussed, except models consider.going run experiment flip coin 20 times record number heads. result tell value \\(\\rho_h\\)? Ultimately, want calculate posterior distribution \\(\\rho_h\\), written p(\\(\\rho_h\\)).Question: probability getting exactly 8 heads 20 tosses?start, useful consider things might happen , example, \\(\\rho_h = 0.4\\). Fortunately, R functions simulating random variables makes easy.First, notice many different things can happen! Even know, certain, \\(\\rho_h = 0.4\\), many outcomes possible. Life remarkably random. Second, likely result experiment 8 heads, expect. Third, transformed raw counts many times total appeared probability distribution. Sometimes, however, convenient just keep track raw counts. shape figure cases.Either way, figures show happened model — \\(\\rho_h = 0.4\\) — true.can thing 11 possible models, calculating happen true. somewhat counterfactual since one can true. Yet assumption allow us create joint distribution models might true data experiment might generate. Let’s simplify p(models, data), although keep precise meaning mind.3D version plot.diagrams, see 11 models 21 outcomes. don’t really care p(\\(models\\), \\(data\\)), joint distribution models--might--true data---experiment-might-generate. Instead, want estimate \\(p\\), unknown parameter determines probability coin come heads tossed. joint distribution alone can’t tell us . created joint distribution even conducted experiment. creation, tool use make inferences. Instead, want conditional distribution, p(\\(models\\) | \\(data = 8\\)). results experiment. results tell us probability distribution \\(p\\)?answer question, simply take vertical slice joint distribution point x-axis corresponding results experiment.animation shows want joint distributions. take slice (red one), isolate , rotate look conditional distribution, normalize (change values along current z-axis counts probabilities), observe resulting posterior.part joint distribution care . aren’t interested object looks like , example, number heads 11. portion irrelevant observed 8 heads, 11. using filter function simulation tibble created, can conclude total 465 times simulation 8 heads observed.expect, time 8 coin tosses came heads, value \\(p\\) 0.4. , numerous occasions, . quite common value \\(p\\) like 0.3 0.5 generate 8 heads. Consider:Yet distribution raw counts. unnormalized density. turn proper probability density (.e., one sum probabilities across possible outcomes sums one) just divide everything total number observations.Solution:likely value \\(\\rho_h\\) 0.4, . , much likely \\(p\\) either 0.3 0.5. 8% chance \\(\\rho_h \\ge 0.6\\).might wondering: use model? Well, let’s say toss coin 20 times get 8 heads . Given result, can ask: probability future samples 20 flips result 10 heads?three main ways go solving problem simulations.first wrong way assuming \\(\\rho_h\\) certain observed 8 heads 20 tosses. conclude 8/20 gives us 0.4. big problem ignoring uncertainty estimating \\(\\rho_h\\). lead us following code.Using Posterior distribution derived (wrong way) simulated data, probability results 10 head isabout 24.5%.second method involves sampling whole posterior distribution vector previously created. lead following correct code.32.8%may noticed, calculated value using first method, believe getting 10 heads less likely really . run casino based assumptions, lose money. important careful assumptions making. tossed coin 20 times got 8 heads. However, wrong assume \\(\\rho_h\\) = 0.4 just based result.",
    "code": "\nsims <- 10000000\n\nodds <- tibble(sim_ID = 1:sims) |>\n  mutate(heads = map_int(sim_ID, ~ rbinom(n = 1, size = 20, p = .4))) |> \n  mutate(above_ten = if_else(heads >= 10, TRUE, FALSE))\n\nodds## # A tibble: 10,000,000 × 3\n##    sim_ID heads above_ten\n##     <int> <int> <lgl>    \n##  1      1    10 TRUE     \n##  2      2     5 FALSE    \n##  3      3     2 FALSE    \n##  4      4    10 TRUE     \n##  5      5     5 FALSE    \n##  6      6    10 TRUE     \n##  7      7     7 FALSE    \n##  8      8    11 TRUE     \n##  9      9     9 FALSE    \n## 10     10     9 FALSE    \n## # … with 9,999,990 more rows\nodds |>\n  ggplot(aes(x=heads,fill=above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Wrong Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nodds |>\n  summarize(success = sum(above_ten)/sims)## # A tibble: 1 × 1\n##   success\n##     <dbl>\n## 1   0.245\np_draws <- tibble(p = rep(seq(0, 1, 0.1), 1000)) |>\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |>\n  filter(heads == 8)\n  \nodds_2nd <- tibble(p = sample(p_draws$p, size = sims, replace = TRUE)) |>\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |> \n  mutate(above_ten = if_else(heads >= 10, TRUE, FALSE)) \n\nodds_2nd## # A tibble: 10,000,000 × 3\n##        p heads above_ten\n##    <dbl> <int> <lgl>    \n##  1   0.4     7 FALSE    \n##  2   0.3     8 FALSE    \n##  3   0.5    13 TRUE     \n##  4   0.4    10 TRUE     \n##  5   0.4     6 FALSE    \n##  6   0.5     8 FALSE    \n##  7   0.5     9 FALSE    \n##  8   0.5    12 TRUE     \n##  9   0.5    10 TRUE     \n## 10   0.4     8 FALSE    \n## # … with 9,999,990 more rows\nodds_2nd |>\n  ggplot(aes(x = heads,fill = above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Right Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nodds_2nd |>\n  summarize(success = sum(above_ten)/sims)## # A tibble: 1 × 1\n##   success\n##     <dbl>\n## 1   0.351"
  },
  {
    "path": "probability.html",
    "id": "working-with-probability-distributions",
    "chapter": "5 Probability",
    "heading": "5.7 Working with probability distributions",
    "text": "probability distribution always easy work . complex object. , many contexts, don’t really care complexity. , instead providing full probability distribution, often just use summary measure, number two three captures aspects entire distribution relevant matter hand. Let’s explore issues using 538 posterior probability distribution, August 13, 2020, number electoral votes won Joe Biden. tibble 1,000,000 draws distribution:distribution sample draws distribution different things. , squint, sort thing, least purposes. example, want know mean distribution, mean draws fairly good estimate, especially number draws large enough.Recall Chapter 2 can draw randomly specified probability distributions:elements vectors “draws” specified probability distributions. applied situations, tools produce draws rather summary objects. Fortunately, vector draws easy work . Start summary statistics:Calculate 95% interval directly:Approximate 95% interval two ways:case, using mean standard deviation produces 95% interval closer true interval. cases, median scaled median absolute deviation better. Either approximation generally “good enough” work. , need know exact 95% interval, must use quantile().",
    "code": "\ndraws## # A tibble: 1,000,000 × 2\n##       ID electoral_votes\n##    <int>           <int>\n##  1     1             210\n##  2     2             277\n##  3     3             227\n##  4     4             397\n##  5     5             378\n##  6     6             428\n##  7     7             350\n##  8     8             385\n##  9     9             325\n## 10    10             463\n## # … with 999,990 more rows\nrnorm(10)##  [1]  2.234  2.232 -0.624 -0.014  2.370  0.428 -0.556  0.688 -0.632  0.556\nrunif(10)##  [1] 0.86 0.94 0.41 0.67 0.29 0.21 0.54 0.37 0.48 0.80\n# recall mean, media, standard deviation and mad functions.\n\nkey_stats <- draws |> \n  summarize(mn = mean(electoral_votes),\n            md = median(electoral_votes),\n            sd = sd(electoral_votes),\n            mad = mad(electoral_votes))\n\nkey_stats## # A tibble: 1 × 4\n##      mn    md    sd   mad\n##   <dbl> <dbl> <dbl> <dbl>\n## 1  325.   326  86.9  101.\nquantile(draws$electoral_votes, probs = c(0.025, 0.975))##  2.5% 97.5% \n##   172   483\nc(key_stats$mn - 2 * key_stats$sd, \n  key_stats$mn + 2 * key_stats$sd)## [1] 152 499\nc(key_stats$md - 2 * key_stats$mad, \n  key_stats$md + 2 * key_stats$mad)## [1] 124 528"
  },
  {
    "path": "probability.html",
    "id": "cardinal-virtues",
    "chapter": "5 Probability",
    "heading": "5.8 Cardinal Virtues",
    "text": "four Cardinal Virtues Wisdom, Justice, Courage, Temperance. data science , ultimately, moral act, use virtues guide work. Every data science project begins question.Wisdom starts creating Preceptor Table. data, , allow us answer question easily? Preceptor Table one outcome, model predictive. one (potential) outcome, model causal. explore data . can never look closely data. Key question: data close enough data want (.e., Preceptor Table) can consider coming population? , can’t proceed . Key making decision assumption validity. columns Preceptor Table match columns data?Wisdom starts creating Preceptor Table. data, , allow us answer question easily? Preceptor Table one outcome, model predictive. one (potential) outcome, model causal. explore data . can never look closely data. Key question: data close enough data want (.e., Preceptor Table) can consider coming population? , can’t proceed . Key making decision assumption validity. columns Preceptor Table match columns data?Justice starts Population Table – data want , data actually data population. row Population Table defined unique Unit/Time combination. explore two key issues Population Table. First, relationship among variables demonstrate stability, meaning model stable across different time periods? Third, rows associated data representative units might data time period? Justice concludes making assumption data generating mechanism. general mathematical formula connects outcome variable interested data ?Justice starts Population Table – data want , data actually data population. row Population Table defined unique Unit/Time combination. explore two key issues Population Table. First, relationship among variables demonstrate stability, meaning model stable across different time periods? Third, rows associated data representative units might data time period? Justice concludes making assumption data generating mechanism. general mathematical formula connects outcome variable interested data ?Courage allows us explore different models. Even though Justice provided basic mathematical structure model, still need decide variables include estimate values unknown parameters. avoid hypothesis tests. check models consistency data . select one model.Courage allows us explore different models. Even though Justice provided basic mathematical structure model, still need decide variables include estimate values unknown parameters. avoid hypothesis tests. check models consistency data . select one model.Temperance guides us use model created answer questions began . create posteriors quantities interest. modest claims make. posteriors create never “truth.” assumptions made create model never perfect. Yet decisions made flawed posteriors almost always better decisions made without .Temperance guides us use model created answer questions began . create posteriors quantities interest. modest claims make. posteriors create never “truth.” assumptions made create model never perfect. Yet decisions made flawed posteriors almost always better decisions made without .",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "wisdom",
    "chapter": "5 Probability",
    "heading": "5.8.1 Wisdom",
    "text": "\nFIGURE 5.3: Wisdom.\nWisdom helps us decide can even hope answer question data .First, start Preceptor Table. rows columns data need , , calculation quantity interest trivial? want know average height adult India, Preceptor Table include row adult column height. missing data, average easy determine, wide variety estimands, unknown values.One key aspect Preceptor Table whether need one potential outcome order calculate estimand. example, want know causal effect exposure Spanish-speakers attitude toward immigration need causal model, one estimates attitude person treatment control. Preceptor Table require two columns outcome. , hand, want predict someone’s attitude, compare one person’s attitude another’s, need Preceptor Table one column outcome.modeling (just) prediction (also) modeling causation? Predictive models care nothing causation. Causal models often also concerned prediction, means measuring quality model.Every model predictive, sense , give new data — drawn population — can create predictive forecast. subset models causal, meaning , given individual, can change value one input figure new output , , calculate causal effect looking difference two potential outcomes.prediction, care forecasting Y given X -yet-unseen data. notion “manipulation” models. don’t pretend , Joe, turn variable X value 5 value 6 just turning knob , , cause Joe’s value Y change 17 23. can compare two people (two groups people), one X equal 5 one X equal 6, see differ Y. basic assumption predictive models one possible Y Joe. , assumption, two possible values Y, one X equal 5 another X equals 6. Preceptor Table single column Y.causal inference, however, can consider case Joe \\(X = 5\\) Joe \\(X = 6\\). mathematical model can used. models can used prediction, estimating value Y yet-unseen observation specified value X. , case, instead single column Preceptor Table Y, least two (possibly many) columns, one potential outcomes consideration.difference prediction models causal models former one column outcome variable latter one.Second, look data perform exploratory data analysis, EDA. can never look data much. important variable one want understand/explain/predict. models create later chapters, variable go left-hand side mathematical equations. academic fields refer “dependent variable.” Others use terms like “regressor” “outcome.” Whatever terminology, need explore distribution variable, min/max/range, mean median, standard deviation, .Gelman, Hill, Vehtari (2020) write:important data analyzing map research question trying answer. sounds obvious often overlooked ignored can inconvenient. Optimally, means outcome measure accurately reflect phenomenon interest, model include relevant predictors, model generalize cases applied.example, regard outcome variable, model incomes necessarily tell patterns total assets. model test scores necessarily tell child intelligence cognitive development. …care variables well, especially correlated/connected outcome variable. time spend looking variables, likely create useful model.Third, key concept population. need data want — Preceptor Table — data similar enough can consider come statistical population. Wikipedia:statistics, population set similar items events interest question experiment. statistical population can group existing objects (e.g. set stars within Milky Way galaxy) hypothetical potentially infinite group objects conceived generalization experience (e.g. set opening hands poker games Las Vegas tomorrow).Mechanically, assuming Precetor Table data drawn population thing “stacking” two top . make sense, variables must mean thing — least mostly — sources. assumption validity. example, Preceptor Table concerns people vote election next week data survey taken last week, obvious can consider data coming population. , voting survey responses exactly thing. can assume — precisely everyone forecasts elections — assume “valid” measures underlying construct.assume data drawn population data Preceptor Table, can use information former make inferences latter. can combine Preceptor Table data single Population Table. can’t , can’t assume two sources come population, can’t use data answer questions. choice walk away. heart Wisdom knowing walk away. John Tukey noted:combination data aching desire answer ensure reasonable answer can extracted given body data.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "justice",
    "chapter": "5 Probability",
    "heading": "5.8.2 Justice",
    "text": "\nFIGURE 5.4: Justice.\nWisdom, Population Table. includes rows data data want . missing values, importantly potential outcomes observed. central problem inference fill question marks Population Table.two key issues explore Population Table: stability representativeness.Stability assumes relationship outcome variable covariates consistent time. Never forget temporal nature almost real data science problems. Preceptor Table focus rows today near future. data always today. must almost always assume future like past order use data past make predictions future.Stability assumes relationship outcome variable covariates consistent time. Never forget temporal nature almost real data science problems. Preceptor Table focus rows today near future. data always today. must almost always assume future like past order use data past make predictions future.Representativeness two-sided concern. want data representative population need calculate parameters. Ideally, love data randomly sampled population, almost never case. concern, just data, also Preceptor Table. data want representative entire population need careful inferences draw.Representativeness two-sided concern. want data representative population need calculate parameters. Ideally, love data randomly sampled population, almost never case. concern, just data, also Preceptor Table. data want representative entire population need careful inferences draw.Validity columns Population Table. Stability representativeness rows.last step Justice make assumption structure data generating mechanism (DGM): mathematical formula, associated error term, relates outcome variable covariates.Justice requires math. Consider model coin-tossing:\\[ H_i  \\sim B(\\rho_H, n = 20) \\]total number \\(H\\) Heads experiment \\(\\) 20 flips single coin, \\(H_i\\), distributed binomial \\(n = 20\\) unknown probability \\(\\rho_h\\) coin coming Heads.Note:cheat simplification! Bayesians specified full Bayesian machinery. really need priors unknown parameter \\(\\rho_h\\) well. complex introductory textbook, wave hands, accept default sensible parameters built R packages use, point readers advanced books, like Gelman, Hill, Vehtari (2020).cheat simplification! Bayesians specified full Bayesian machinery. really need priors unknown parameter \\(\\rho_h\\) well. complex introductory textbook, wave hands, accept default sensible parameters built R packages use, point readers advanced books, like Gelman, Hill, Vehtari (2020).Defining \\(\\rho_h\\) “probability coin comes Heads” bit fudge. calculate hand compare tools produce, won’t . Instead, calculated value closer zero. ? \\(\\rho_h\\) really “long-run percentage time coin comes Heads.” just percentage experiment.Defining \\(\\rho_h\\) “probability coin comes Heads” bit fudge. calculate hand compare tools produce, won’t . Instead, calculated value closer zero. ? \\(\\rho_h\\) really “long-run percentage time coin comes Heads.” just percentage experiment.simple case, fortunate parameter \\(\\rho_h\\) (mostly!) simple analog real world quantity. time, parameters easy interpret. complex model, especially one interaction terms, focus less parameters actual predictions.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "courage",
    "chapter": "5 Probability",
    "heading": "5.8.3 Courage",
    "text": "\nFIGURE 5.5: Courage.\nthree languages data science words, math code, important code. need explain structure model using three languages, need Courage implement model code.Courage requires us take general mathematical formula provide Justice make specific. variables include model exclude? Every data science project involves creation several models. , specify precise data generating mechanism. Using formula, R code, create fitted model. models parameters. can never know true values parameters, can create, explore, posterior distributions unknown true values.Code allows us “fit” model estimating values unknown parameters, like \\(\\rho_h\\). Sadly, can never know true values parameters. , like good data scientists, can express uncertain knowledge form posterior probability distributions. distributions, can compare actual values outcome variable “fitted” “predicted” results model. can examine “residuals,” difference fitted actual values.Every outcome sum two parts: model model:\\[outcome = model + \\ \\ \\ \\ \\ model\\]doesn’t matter outcome . result coin flip, weight person, GDP country. Whatever outcome considering always made two parts. first model created. second stuff — blooming buzzing complexity real world — part model.uncertainty driven ignorance \\(\\rho_h\\).parameter something exist real world. (, , data.) Instead, parameter mental abstraction, building block use help us accomplish true goal: replace least questions marks actual Preceptor Table. Since parameters mental abstractions, always uncertain value, however much data might collect., often , uncertainty comes forces , assumption, model. example, coin fair, expect \\(T_i\\) equal 10. , often, different, even correct \\(\\rho_h\\) equals exactly 0.5.randomness intrinsic fallen world.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "temperance",
    "chapter": "5 Probability",
    "heading": "5.8.4 Temperance",
    "text": "\nFIGURE 5.6: Temperance.\nimportant concepts statistics data science “Data Generating Mechanism.” data — data collect see — generated complexity confusion world. God’s mechanism brought data us. job build model process, create, computer, mechanism generates fake data consistent data see. DGM, can answer question might . particular, DGM, provide predictions data seen estimates uncertainty associated predictions. Justice gave us structure DGM. Courage created DGM, fitted model. Temperance guide us use.created (checked) model, now use model answer questions. Models made use, beauty. world confronts us. Make decisions must. decisions better ones use high quality models help make .Sadly, models never good like . First, world intrinsically uncertain.\nFIGURE 5.7: Donald Rumsfeld.\nknown knowns. things know know. also know known unknowns. say, know things know. also unknown unknowns, ones know know. – Donald RumsfeldWhat really care data haven’t seen yet, mostly data tomorrow. world changes, always ? doesn’t change much, maybe OK. changes lot, good model ? general, world changes . means forecasts uncertain naive use model might suggest.\nFIGURE 5.8: Three Card Monte.\nmean? Well imagine crowd playing Three Card Monte streets New York. guy running game runs demo shows cards make confident. earn money making overconfident persuading bet. odds may seem good demo round, doesn’t actually say anything likely happen real, high stakes game begins. person running game many simulations, making “victim” forget actually make conclusions odds winning. variables simply know even put lot effort making posterior probability distributions. People can using slight hand, instance.need patience order study understand unknown unknowns data. Patience also important analyze “realism” models. created mathematical probability distribution presidential elections, instance, assumed Democratic candidate 50% chance winning vote electoral college. comparing mathematical model empirical cases, however, recognize mathematical model unlikely true. mathematical model suggested getting fewer 100 votes next impossible, many past Democratic candidates empirical distribution received less 100 electoral votes.Temperance, key distinction true posterior distribution — call “Preceptor’s Posterior” — estimated posterior distribution. Recall discussion Section 5.1. Imagine every assumption made Wisdom Justice correct, correctly understand every aspect world works. still know unknown value trying estimate — recall Fundamental Problem Causal Inference — posterior created perfect. Preceptor’s Posterior. Sadly, even estimated posterior , close Preceptor’s Posterior, can never sure fact, can never know truth, never certain assumptions made correct.Even worse, must always worry estimated posterior, despite work put creating , far truth. , therefore, must cautious use posterior, humble claims accuracy. Using posterior, despite fails, better using . Yet , best, distorted map reality, glass must look darkly. Use posterior humility.",
    "code": ""
  },
  {
    "path": "probability.html",
    "id": "summary-6",
    "chapter": "5 Probability",
    "heading": "5.9 Summary",
    "text": "Throughout Chapter, spent time going examples conditional distributions. However, ’s worth noting probability distributions conditional something. Even simple examples, flipping coin multiple times, assuming probability getting heads versus tails change tosses.also discussed difference empirical, mathematical, posterior probability distributions. Even though developed heuristics better understand distributions, every time make claim world, based beliefs - think world. wrong. beliefs can differ. Two reasonable people can conflicting beliefs fairness die.start chapter briefly discuss definition random variable, yet sort let go rest chapter, ’s hiding almost everywhere whenever create distribution.\nexample, two models two random variables, disease don’t disease, three models three random variables, either 0 1 2 white marbles bad. Essentially just like missing values Preceptor table random variables need know values answer question? useful understand three types distributions concept conditional distributions, almost every probability distribution conditional posterior. can leave words future discussions, generally book. implicit.keen learn probability, video featuring Professor Gary King. great way review concepts covered chapter, albeit higher level mathematics.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "one-parameter",
    "chapter": "6 One Parameter",
    "heading": "6 One Parameter",
    "text": "chapter re-drafted.Hunger Games dystopian novel children chosen via lottery fight death. Primrose Everdeen selected urn. misfortune selected? , data scientists say, sampled?Chapter 5, learned probability, framework quantifying uncertainty. chapter, learn sampling, beginning journey toward inference. sample, take units population, calculate statistics based units, make inferences unknown parameters associated population.urn certain number red certain number white beads equal size, mixed well together. proportion, \\(\\rho\\), urn’s beads red?\nFIGURE 6.1: urn red white beads.\nOne way answer question perform exhaustive count: remove bead individually, count number red beads, count number white beads, divide number red beads total number beads. Call ratio \\(\\rho\\), proportion red beads urn. However, long tedious process. Therefore, use sampling! Consider two questions:get 17 red beads random sample size 50 taken mixed urn, proportion \\(\\rho\\) beads urn red?probability, using urn, draw 8 red beads use shovel size 20?begin chapter, look real sampling activity: urn. , simulate urn example using R code. help us understand standard error ways uncertainty factors predictions. attempt estimate single parameter, proportion red beads urn.Use tidyverse package.",
    "code": "\nlibrary(tidyverse)"
  },
  {
    "path": "one-parameter.html",
    "id": "sampling-activity",
    "chapter": "6 One Parameter",
    "heading": "6.1 Real sampling activity",
    "text": "\nFIGURE 6.2: urn red white beads.\n",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "using-the-shovel-method-once",
    "chapter": "6 One Parameter",
    "heading": "6.1.1 Using the shovel method once",
    "text": "Instead performing exhaustive count, let’s insert shovel urn remove \\(5 \\cdot 10 = 50\\) beads. taking sample total population beads.\nFIGURE 6.3: Inserting shovel urn.\n\nFIGURE 6.4: Removing 50 beads urn.\nObserve 17 50 sampled beads red thus \\(17/50 = 0.34 = 34\\%\\) shovel’s beads red. can view proportion beads red shovel guess proportion beads red entire urn. exact exhaustive count beads urn, guess 34% took much less time energy make.Recall \\(\\rho\\) true value proportion red beads. one \\(\\rho\\). guesses proportion red beads known \\(\\hat{\\rho}\\) (pronounced p hat), \\(\\hat{\\rho}\\) estimated value \\(\\rho\\) comes taking sample. many possible \\(\\hat{\\rho}\\)’s. often differ estimates. different \\(\\hat{\\rho}\\) even though agree one \\(\\rho\\).Start activity beginning, placing 50 beads back urn. second sample include exactly 17 red beads? Maybe, probably .repeated activity many times? guess proportion urn’s beads red, \\(\\hat{\\rho}\\), exactly 34% every time? Surely .Let’s repeat exercise help 33 groups friends understand value \\(\\hat{\\rho}\\) varies across 33 independent trials.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "student-shovels",
    "chapter": "6 One Parameter",
    "heading": "6.1.2 Using the shovel 33 times",
    "text": "33 groups friends following:Use shovel remove 50 beads .Count number red beads compute proportion 50 beads red.Return beads urn.Mix contents urn let previous group’s results influence next group’s.33 groups friends make note proportion red beads sample collected. group marks proportion 50 beads red appropriate bin hand-drawn histogram seen .\nFIGURE 6.5: Constructing histogram proportions.\nHistograms allow us visualize distribution numerical variable. particular, center values falls values vary. partially completed histogram first 10 33 groups friends’ results can seen figure .\nFIGURE 6.6: Hand-drawn histogram first 10 33 proportions.\nObserve following details histogram:low end, one group removed 50 beads urn proportion red 0.20 0.25.high end, another group removed 50 beads urn proportion 0.45 0.5 red.However, frequently occurring proportions 0.30 0.35 red, right middle distribution.distribution somewhat bell-shaped.tactile_sample_urn saves results 33 groups friends.group, given names, number red_beads obtained, corresponding proportion 50 beads red, called prop_red. also group_ID variable gives 33 groups unique identifier. row can viewed one instance replicated activity: using shovel remove 50 beads computing proportion beads red.Let’s visualize distribution 33 proportions using geom_histogram() binwidth = 0.05. computerized complete version partially completed hand-drawn histogram saw earlier. Setting boundary = 0.4 indicates want binning scheme one bins’ boundary 0.4. color = \"white\" modifies color boundary visual clarity.",
    "code": "## # A tibble: 33 × 4\n##    group           red_beads prop_red group_ID\n##    <chr>               <dbl>    <dbl>    <int>\n##  1 Yuki, Harry            21     0.42        1\n##  2 Aaron, Mike            11     0.22        2\n##  3 Maeve, Josh            18     0.36        3\n##  4 Ellie, Terrance        17     0.34        4\n##  5 Damani, Melissa        15     0.3         5\n##  6 Griffin, Mary          18     0.36        6\n##  7 Siobhan, Jane          11     0.22        7\n##  8 Claire, Cloud          16     0.32        8\n##  9 Nam, Joshua            19     0.38        9\n## 10 Conrad, Vlad           16     0.32       10\n## # … with 23 more rows\ntactile_sample_urn |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # Add scale_y_continuous with breaks by 1, as the default shows the y-axis\n  # from 1 to 10 with breaks at .5. Breaks by 1 is better for this plot, as all\n  # resulting values are integers.\n  \n  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  \n  # The call expression() is used to insert a mathematical expression, like\n  # p-hat. The paste after expression allows us to paste text prior to said\n  # expression.\n  \n  labs(x = expression(paste(\"Proportion of 50 beads that were red \", hat(rho))),\n       y = \"Count\",\n       title = \"Proportions Red in 33 Samples\") "
  },
  {
    "path": "one-parameter.html",
    "id": "what-did-we-just-do",
    "chapter": "6 One Parameter",
    "heading": "6.1.3 What did we just do?",
    "text": "just demonstrated activity statistical concept sampling. want know proportion red beads urn, urn population. Performing exhaustive count red white beads time-consuming. Therefore, much practical extract sample 50 beads using shovel. Using sample 50 beads, estimated proportion urn’s beads red 34%.Moreover, mixed beads use shovel, samples random independent. sample drawn random, samples different . example sampling variation. example, instead selecting 17 beads first sample selected just 11? mean population proportion beads 11/50 22%? ! performed 33 trials can look histogram, see peak distribution occurs \\(.35 < \\hat{\\rho} < .4\\) , likely proportion red beads entire population also fall near range.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "virtual-sampling",
    "chapter": "6 One Parameter",
    "heading": "6.2 Virtual sampling",
    "text": "just performed tactile sampling activity. used physical urn beads physical shovel. hand develop intuition ideas behind sampling. section, mimic physical sampling virtual sampling, using computer.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "shovel-one-time",
    "chapter": "6 One Parameter",
    "heading": "6.2.1 Using the virtual shovel once",
    "text": "Virtual sampling requires virtual urn virtual shovel. Create tibble named urn. rows urn correspond exactly contents actual urn.Observe urn 1,000 rows, meaning urn contains\n1,000 beads. first variable bead_ID used identification variable. None beads actual urn marked numbers. second variable color indicates whether particular virtual bead red white.Note section, used variable bead_ID keep track bead urn, last section used group_ID keep track samples drawn 33 individual teams. better strategy naming variables ID, much likely us get confused later .virtual urn needs virtual shovel. use slice_sample() list-column mapping wizardry learned Section 5.3 take sample 50 beads virtual urn.usual, map functions list-columns powerful confusing. str() function good way explore tibble list-column.two levels. one row tibble sample. far, drawn one sample. Within row, second level, tibble sample. tibble two variables: trial_ID color. advantage using slice_sample(), selects columns urn, whereas sample() can sample single column. identifying individual bead may irrelevant urn scenario, problems useful additional data individual.Now let’s add column indicates number red beads sample taken shovel.work? R evaluates color == red, treats TRUE values like number 1 FALSE values like number 0. summing number TRUEs FALSEs equivalent summing 1’s 0’s. end, operation counts number beads color equals “red”.Finally, calculate proportion red dividing numb_red (number red beads observed shovel), shovel size (using shovel size 50).Careful readers note numb_red changing example . reason, course, block re-runs shovel exercise, slice_sample return random number red beads. wanted number block, need use set.seed() time, always providing seed time.Let’s now perform virtual analog 33 groups students use sampling shovel!",
    "code": "\n# set.seed() ensures that the beads in our virtual urn are always in the same\n# order. This ensures that the figures in the book match their written\n# descriptions. We want 40% of the beads to be red.\n\nset.seed(10)\n\nurn <- tibble(color = c(rep(\"red\", 400), \n                        rep(\"white\", 600))) |>\n  \n  # sample_frac() keeps all the rows in the tibble but rearranges their order.\n  # We don't need to do this. A virtual urn does not care about the order of the\n  # beads. But we find it aesthetically pleasing to mix them up.\n  \n  sample_frac() |> \n  mutate(bead_ID = 1:1000) \n\nurn  ## # A tibble: 1,000 × 2\n##    color bead_ID\n##    <chr>   <int>\n##  1 white       1\n##  2 white       2\n##  3 red         3\n##  4 red         4\n##  5 white       5\n##  6 white       6\n##  7 white       7\n##  8 white       8\n##  9 white       9\n## 10 white      10\n## # … with 990 more rows\n# Define trial_ID as one instance of us sampling 50 beads from the urn. When\n# trial_ID is called within map(), we are performing slice_sample() upon our urn\n# once, and taking a sample of 50 beads. \n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50)))## # A tibble: 1 × 2\n##   trial_ID shovel           \n##      <dbl> <list>           \n## 1        1 <tibble [50 × 2]>\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  str()## tibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ trial_ID: num 1\n##  $ shovel  :List of 1\n##   ..$ : tibble [50 × 2] (S3: tbl_df/tbl/data.frame)\n##   .. ..$ color  : chr [1:50] \"white\" \"white\" \"white\" \"red\" ...\n##   .. ..$ bead_ID: int [1:50] 812 903 227 283 229 160 523 893 66 277 ...\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  \n# To count the number of red beads in each shovel, we can use a lesser \n# known property of the sum() function: By passing in a comparison \n# expression, sum() will count the number of occurrences within a vector. \n# In this case, we count the total number occurrences of the word red\n# in the color column of shovel.\n\n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\")))## # A tibble: 1 × 3\n##   trial_ID shovel            numb_red\n##      <dbl> <list>               <int>\n## 1        1 <tibble [50 × 2]>       20\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 50)## # A tibble: 1 × 4\n##   trial_ID shovel            numb_red prop_red\n##      <dbl> <list>               <int>    <dbl>\n## 1        1 <tibble [50 × 2]>       23     0.46"
  },
  {
    "path": "one-parameter.html",
    "id": "shovel-33-times",
    "chapter": "6 One Parameter",
    "heading": "6.2.2 Using the virtual shovel 33 times",
    "text": "tactile sampling exercise Section 6.1, 33 groups students use shovel, yielding 33 samples size 50 beads. used 33 samples compute 33 proportions.Let’s use virtual sampling replicate tactile sampling activity virtual format. ’ll save results data frame called virtual_samples.Let’s visualize variation histogram:Since binwidth = 0.05, create bins boundaries 0.30, 0.35, 0.45, . Recall \\(\\hat{\\rho}\\) equal proportion beads red sample.Observe occasionally obtained proportions red less 30%. hand, occasionally obtained proportions greater 45%. However, frequently occurring proportions 35% 45%. differences proportions red? sampling variation.Now compare virtual results tactile results previous section. Observe histograms somewhat similar center variation, although identical. slight differences due random sampling variation. Furthermore, observe distributions somewhat bell-shaped.\nFIGURE 6.7: Comparing 33 virtual 33 tactile proportions red. Note , though figures differ slightly, centered around .35 .45. shows , sampling distributions, frequently occuring proportion red 35% 45%.\nvisualization allows us see results differed tactile virtual urn results. can see, variation results. cause concern, always expected sampling variation results.",
    "code": "\nset.seed(9)\n\n virtual_samples <- tibble(trial_ID = 1:33) |>\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |>\n    mutate(prop_red = numb_red / 50) \n\nvirtual_samples## # A tibble: 33 × 4\n##    trial_ID shovel            numb_red prop_red\n##       <int> <list>               <int>    <dbl>\n##  1        1 <tibble [50 × 2]>       21     0.42\n##  2        2 <tibble [50 × 2]>       19     0.38\n##  3        3 <tibble [50 × 2]>       17     0.34\n##  4        4 <tibble [50 × 2]>       15     0.3 \n##  5        5 <tibble [50 × 2]>       17     0.34\n##  6        6 <tibble [50 × 2]>       21     0.42\n##  7        7 <tibble [50 × 2]>        9     0.18\n##  8        8 <tibble [50 × 2]>       21     0.42\n##  9        9 <tibble [50 × 2]>       16     0.32\n## 10       10 <tibble [50 × 2]>       20     0.4 \n## # … with 23 more rows\nvirtual_samples |> \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # To use mathematical symbols in titles and labels, use the expression()\n  # function, as here.\n  \n  labs(x = expression(hat(rho)),\n       y = \"Count\",\n       title = \"Distribution of 33 proportions red\") +\n  \n  # Label the y-axis in an attractive fashion. Without this code, the axis\n  # labels would include 2.5, which makes no sense because all the values are\n  # integers.\n  \n  scale_y_continuous(breaks = seq(2, 10, 2))"
  },
  {
    "path": "one-parameter.html",
    "id": "shovel-1000-times",
    "chapter": "6 One Parameter",
    "heading": "6.2.3 Using the virtual shovel 1,000 times",
    "text": "\nFIGURE 6.8: much sampling, little time.\nAlthough took 33 samples urn previous section, never ! advantage modern technology can use virtual simulation many times choose, restrictions resources. longer days recruit friends tirelessly sample physical urn. now data scientists! 33 samples useless us. Instead, use simulations hundreds thousands times create mathematical models can combine knowledge answer questions. section ’ll examine effects sampling urn 1,000 times.can reuse code , making sure replace 33 trials 1,000.Now 1,000 values prop_red, representing proportion 50 beads red sample. Using code earlier, let’s visualize distribution 1,000 replicates prop_red histogram:empty spaces among bars? Recall , 50 beads, 51 possible values \\(\\hat{\\rho}\\): 0, 0.02, 0.04, …, 0.98, 1. value 0.31 0.47 impossible, hence gaps.frequently occurring proportions red beads occur, , 35% 45%. Every now observe proportions much higher lower. occurs increase number trials, tails develop distribution likely witness extreme \\(\\hat{\\rho}\\) values. symmetric, bell-shaped distribution shown histogram well approximated normal distribution.Now good understanding virtual sampling, can apply knowledge examine effects changing virtual shovel size.",
    "code": "\nset.seed(9)\n\n virtual_samples <- tibble(trial_ID = 1:1000) |>\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |>\n    mutate(numb_beads = map_int(shovel, ~ length(.$color))) |> \n    mutate(prop_red = numb_red / numb_beads) \nvirtual_samples |> \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(hat(rho)),\n       y = \"Count\",\n       title = \"Distribution of 1,000 proportions red\") "
  },
  {
    "path": "one-parameter.html",
    "id": "different-shovels",
    "chapter": "6 One Parameter",
    "heading": "6.2.4 The effect of different shovel sizes",
    "text": "\nFIGURE 6.9: happens use different sized shovels sample?\nInstead just one shovel, imagine three choices shovels extract sample beads : shovels size 25, 50, 100. Using newly developed tools virtual sampling, let’s unpack effect different sample sizes. Start creating tibble 1,000 rows, row representing instance us sampling urn chosen shovel size. , compute resulting 1,000 replicates proportion red. Finally, plot distribution using histogram.repeat process shovel size 50.choose bin width .04 histograms easily compare different shovel sizes. Using smaller bin size result gaps bars, shovel size 50 possible \\(\\hat{\\rho}\\) values shovel size 25.Finally, perform process 1000 replicates map histogram using shovel size 100.easy comparison, present three resulting histograms single row matching x y axes:\nFIGURE 6.10: Comparing distributions proportion red different sample sizes (25, 50, 100). important takeaway center becomes concentrated sample size increases, indicating smaller standard deviation guesses.\nObserve sample size increases, histogram becomes taller narrower. variation proportion red sample decreases. Remember: large variation means wide range values can achieved, smaller variations concentrated around specific value.also witnessed famous theorem, mathematically proven truth, called Central Limit Theorem. loosely states sample means based larger larger sample sizes, sampling distribution sample means becomes normally shaped narrow. words, sampling distribution increasingly follows normal distribution variation sampling distributions gets smaller, quantified standard errors.variation decrease sample size increases? use large sample size like 100 500, sample much representative population simply population included. result, proportion red sample (\\(\\hat{\\rho}\\)) closer population proportion (\\(\\rho\\)). hand, smaller samples much variation old friend chance. much likely extreme samples representative population.Let’s attempt visualize concept variation different way. sample size, let’s plot proportion red 1,000 samples. 3 different shovel sizes, 3,000 total points, point representing instance sampling urn specific shovel size.graph illustrates exact concept histogram. smallest shovel size significant variance sample sample, samples take wide variety sample proportions! However, increase sample size, points become concentrated, less variance.also third way understand variation! can numerically explicit amount variation three sets 1,000 values prop_red using standard deviation. standard deviation summary statistic measures amount variation within numerical variable. three sample sizes, let’s compute standard deviation 1,000 proportions red. sample size increases, sample sample variation decreases, guesses true proportion urn’s beads red get precise. larger shovel, precise result.\nFIGURE 6.11: Variance appears everywhere data science.\nLet’s take step back variance. reality code needs better optimized, bad practice make separate tibble sample size. make comparisons easier, let’s attempt put 3 shovel sizes tibble using mapping.us completely understand mapping, fret! tidyr package provides expand_grid() function neat alternative. can use expand_grid() new variable, shovel_size, create tibble organize results. Instead using 1,000 trials, let’s use 3 get feel function.sets stage simulating three samples three different shovel sizes. Similar code can used., changed second line use shovel_size rather trial_ID mapping variable since can longer hard code shovel size call slice_sample(). Expand 1,000 simulations value shovel_size finish calculation standard deviation., approximately, result saw , 1 re-factored tibble instead 3 separate ones. can also functions like expand_grid() future make code concise.Now framework, ’s need limit sizes 25, 50, 100. try integers 1 100? can use code, except ’ll now set shovel_size = 1:100.Now, standard deviation prop_red shovel sizes 1 100. Let’s plot value see changes shovel gets larger:\nFIGURE 6.12: Comparing standard deviations proportions red 100 different shovels. standard deviation decreases rate square root shovel size. red line shows standard error.\nred line represents important statistical concept: standard error (SE). shovel size increases, thus sample size increases, find standard error decreases. confusing right now, fear ! delve explanation standard error next section.\nFIGURE 6.13: poets philosophers confused : don’t worry! won’t problem set.\npower running many analyses using map functions list columns: , tell standard deviation decreasing shovel size increased, looking shovel sizes 25, 50, 100, wasn’t clear quickly decreasing.",
    "code": "\n# Within slice_sample(), n = 25 represents our shovel of size 25. We also divide\n# by 25 to compute the proportion red.\n\nvirtual_samples_25 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 25))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 25)\n\nvirtual_samples_25 |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 25 beads that were red\", \n       title = \"25\") \nvirtual_samples_50 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 50)\n\n\nvirtual_samples_50  |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 50 beads that were red\", \n       title = \"50\")  \nvirtual_samples_100 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 100))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 100)\n\n\nvirtual_samples_100 |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 100 beads that were red\", \n       title = \"100\") \n# Use bind_rows to combine the data from our three saved virtual sampling\n# objects. Use mutate() in each to clarify the n as the necessary number\n# of samples taken. This makes our data easier to interpret and prevents\n# duplicate elements.\n\nvirtual_prop <- bind_rows(virtual_samples_25 |> \n                            mutate(n = 25), \n                          virtual_samples_50 |> \n                            mutate(n = 50), \n                          virtual_samples_100 |> \n                            mutate(n = 100))\n\n# Plot our new object with the x-axis showing prop_red. Add elements binwidth,\n# boundary, and color for stylistic clarity. Use labs() to add an x-axis label\n# and title. Facet_wrap() splits the graph into multiple plots by the variable\n# (~n).\n\ncomparing_sampling_distributions <- ggplot(virtual_prop, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of shovel's beads that are red\", \n       title = \"Comparing distributions of proportions red for three different shovel sizes.\") +\n  facet_wrap(~ n) \n\n# Inspect our new faceted graph. \n\ncomparing_sampling_distributions\nvirtual_prop |>\n  ggplot(aes(x = n, y = prop_red, color = as.factor(n))) +\n  geom_jitter(alpha = .15) + \n  labs(title = \"Results of 1,000 samples for 3 different shovel sizes.\",\n       subtitle = \"As shovel size increases, variation decreases.\",\n       y = \"Proportion red in sample\",\n       color = \"Shovel size\") +\n  # We do not need an x axis, because the color of the points denotes the shovel size. \n   theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\ntibble(trial_ID = 1:1000) |>\n  mutate(shovel_ID = map(trial_ID, ~c(25, 50, 100))) |>\n  unnest(shovel_ID) |>\n  mutate(samples = map(shovel_ID, ~slice_sample(urn, n = .))) |>\n  mutate(num_red = map_int(samples, ~sum(.$color == \"red\"))) |>\n  mutate(prop_red = num_red/shovel_ID)## # A tibble: 3,000 × 5\n##    trial_ID shovel_ID samples            num_red prop_red\n##       <int>     <dbl> <list>               <int>    <dbl>\n##  1        1        25 <tibble [25 × 2]>        7     0.28\n##  2        1        50 <tibble [50 × 2]>       26     0.52\n##  3        1       100 <tibble [100 × 2]>      37     0.37\n##  4        2        25 <tibble [25 × 2]>        8     0.32\n##  5        2        50 <tibble [50 × 2]>       15     0.3 \n##  6        2       100 <tibble [100 × 2]>      46     0.46\n##  7        3        25 <tibble [25 × 2]>        6     0.24\n##  8        3        50 <tibble [50 × 2]>       21     0.42\n##  9        3       100 <tibble [100 × 2]>      40     0.4 \n## 10        4        25 <tibble [25 × 2]>        9     0.36\n## # … with 2,990 more rows\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100))## # A tibble: 9 × 2\n##   trial_ID shovel_size\n##      <int>       <dbl>\n## 1        1          25\n## 2        1          50\n## 3        1         100\n## 4        2          25\n## 5        2          50\n## 6        2         100\n## 7        3          25\n## 8        3          50\n## 9        3         100\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red/shovel_size) ## # A tibble: 9 × 5\n##   trial_ID shovel_size shovel             numb_red prop_red\n##      <int>       <dbl> <list>                <int>    <dbl>\n## 1        1          25 <tibble [25 × 2]>         7     0.28\n## 2        1          50 <tibble [50 × 2]>        19     0.38\n## 3        1         100 <tibble [100 × 2]>       39     0.39\n## 4        2          25 <tibble [25 × 2]>         8     0.32\n## 5        2          50 <tibble [50 × 2]>        19     0.38\n## 6        2         100 <tibble [100 × 2]>       34     0.34\n## 7        3          25 <tibble [25 × 2]>         7     0.28\n## 8        3          50 <tibble [50 × 2]>        19     0.38\n## 9        3         100 <tibble [100 × 2]>       36     0.36\nexpand_grid(trial_ID = c(1:1000), shovel_size = c(25, 50, 100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red/shovel_size) |>\n  group_by(shovel_size) |> \n  summarize(st_dev_p_hat = sd(prop_red))## # A tibble: 3 × 2\n##   shovel_size st_dev_p_hat\n##         <dbl>        <dbl>\n## 1          25       0.0967\n## 2          50       0.0682\n## 3         100       0.0453\nshovels_100 <- expand_grid(trial_ID = c(1:1000), shovel_size = c(1:100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / shovel_size) |> \n  group_by(shovel_size) |> \n  summarize(st_dev_p_hat = sd(prop_red))\n\nglimpse(shovels_100)## Rows: 100\n## Columns: 2\n## $ shovel_size  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n## $ st_dev_p_hat <dbl> 0.484, 0.341, 0.276, 0.241, 0.214, 0.204, 0.177, 0.175, 0…"
  },
  {
    "path": "one-parameter.html",
    "id": "standard-errors",
    "chapter": "6 One Parameter",
    "heading": "6.3 Standard error",
    "text": "\nFIGURE 6.14: Standard errors just way old people talk confidence intervals.\nStandard errors (SE) quantify effect sampling variation estimates. words, quantify much can expect calculated proportions shovel’s beads red vary one sample another sample another sample, . general rule, sample size increases, standard error decreases.standard error standard deviation sample statistic (aka point estimate), proportion. example, standard error mean refers standard deviation distribution sample means taken population.relationship standard error standard deviation , given sample size, standard error equals standard deviation divided square root sample size. Accordingly, standard error inversely proportional sample size. larger sample size, smaller standard error.sounds confusing, don’t worry! . can explain depth, important understand terminology.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "terminology-and-notation",
    "chapter": "6 One Parameter",
    "heading": "6.3.1 Terminology and notation",
    "text": "\nFIGURE 6.15: Let Yoda’s wisdom dull pain terminology section.\nconcepts underlying terminology, notation, definitions tie directly concepts underlying tactile virtual sampling activities. simply take time practice master .First, population set relevant units. population’s size upper-case \\(N\\). sampling activities, population collection \\(N\\) = 1,000 identically sized red white beads urn. simplest possible population. examples adult men US, classrooms school, wheelbarrows Massachusetts, values blood pressure, read five minute intervals, entire life. Often, population extends time, blood pressure readings , therefore, amorphous. Consider people run governor US state since 1900, people run governor 2050. also populations.Second, population parameter numerical summary quantity population unknown, wish knew. example, quantity mean, population parameter interest population mean. mathematically denoted Greek letter \\(\\mu\\) pronounced “mu.” earlier sampling urn activity, however, since interested proportion urn’s beads red, population parameter population proportion, denoted \\(\\rho\\).Third, census exhaustive enumeration counting \\(N\\) units population order compute population parameter’s value exactly. sampling activity, correspond counting number beads \\(N = 1000\\) red computing population proportion \\(\\rho\\) red exactly. number \\(N\\) individuals observations population large case urn, census can quite expensive terms time, energy, money. census impossible populations includes future, like blood pressure next year candidates governor 2040. truth , even theory, calculate .Fourth, sampling act collecting sample population can , want , perform census. sample size lower case \\(n\\), opposed upper case \\(N\\) population’s size. Typically sample size \\(n\\) much smaller population size \\(N\\). sampling activities, used shovels varying slots extract samples size \\(n\\) = 1 \\(n\\) = 100.Fifth, point estimate, also known sample statistic, measure computed sample estimates unknown population parameter. sampling activities, recall unknown population parameter proportion red beads mathematically denoted \\(\\rho\\). point estimate sample proportion: proportion shovel’s beads red. words, guess proportion urn’s beads red. point estimate parameter \\(\\rho\\) \\(\\hat{\\rho}\\). “hat” top \\(\\rho\\) indicates estimate unknown population proportion \\(\\rho\\).Sixth, sample said representative roughly looks like population. words, sample’s characteristics good representation population’s characteristics? sampling activity, samples \\(n\\) beads extracted using shovels representative urn’s \\(N\\) = 1000 beads?Seventh, sample generalizable results based sample can generalize population. sampling activity, can generalize sample proportion shovels entire urn? Using mathematical notation, akin asking \\(\\hat{\\rho}\\) “good guess” \\(\\rho\\)?Eighth, biased sampling occurs certain individuals observations population higher chance included sample others. say sampling procedure unbiased every observation population equal chance sampled. red beads much smaller white beads, therefore prone falling shovel, sample biased. sampling activities, since mixed \\(N = 1000\\) beads prior group’s sampling since equally sized beads equal chance sampled, samples unbiased.Ninth, sampling procedure random sample randomly population unbiased fashion. sampling activities, correspond sufficiently mixing urn use shovel.\nFIGURE 6.16: Fear look like Spongebob reading section. re-cap right now!\ngeneral:sampling sample size \\(n\\) done random, thenthe sample unbiased representative population size \\(N\\), thusany result based sample can generalize population, thusthe point estimate “good guess” unknown population parameter, thusinstead performing census, can draw inferences population using sampling.Specific sampling activity:extract sample \\(n=50\\) beads random, words, mix equally sized beads using shovel, thenthe contents shovel unbiased representation contents urn’s 1000 beads, thusany result based shovel’s beads can generalize urn, thusthe sample proportion \\(\\hat{\\rho}\\) \\(n=50\\) beads shovel red “good guess” population proportion \\(\\rho\\) \\(N=1000\\) beads red, thusinstead manually going 1,000 beads urn, can make inferences urn using results shovel.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "sampling-definitions",
    "chapter": "6 One Parameter",
    "heading": "6.3.2 Statistical definitions",
    "text": "Now, important statistical definitions related sampling. refresher 1,000 repeated/replicated virtual samples size \\(n\\) = 25, \\(n\\) = 50, \\(n\\) = 100 Section 6.2, let’s display figure showing difference proportions red according different shovel sizes.\nFIGURE 6.17: Previously seen three distributions sample proportion \\(\\hat{ ho}\\).\ntypes distributions special name: sampling distributions; visualization displays effect sampling variation distribution point estimate; case, sample proportion \\(\\hat{\\rho}\\). Using sampling distributions, given sample size \\(n\\), can make statements values typically expect.example, observe centers three sampling distributions: roughly centered around \\(0.4 = 40\\%\\). Furthermore, observe somewhat likely observe sample proportions red beads \\(0.2 = 20\\%\\) using shovel 25 slots, almost never observe proportion 20% using shovel 100 slots. Observe also effect sample size sampling variation. sample size \\(n\\) increases 25 50 100, variation sampling distribution decreases thus values cluster tightly around center around 40%. quantified variation using standard deviation sample proportions, seeing standard deviation decreases square root sample size:\nFIGURE 6.18: Previously seen comparing standard deviations proportions red 100 different shovels\nsample size increases, standard deviation proportion red beads decreases. type standard deviation another special name: standard error",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "what-is-a-standard-error",
    "chapter": "6 One Parameter",
    "heading": "6.3.3 What is a “standard error”?",
    "text": "“standard error” (SE) term measures accuracy sample distribution represents population use standard deviation. Specifically, SE used refer standard deviation sample statistic (aka point estimate), mean median. example, “standard error mean” refers standard deviation distribution sample means taken population.statistics, sample mean deviates actual mean population; deviation standard error mean.Many students struggle differentiate standard error standard deviation. relationship standard error standard deviation , given sample size, standard error equals standard deviation divided square root sample size. Accordingly, standard error inversely proportional sample size; larger sample size, smaller standard error statistic approach actual value.data points involved calculations mean, smaller standard error tends . standard error small, data said representative true mean. cases standard error large, data may notable irregularities. Thus, larger sample size = smaller standard error = representative truth.help reinforce concepts, let’s re-display previous figure using new sampling terminology, notation, definitions:\nFIGURE 6.19: Three sampling distributions sample proportion \\(\\hat{ ho}\\). Note increased concentration bins around .4 sample size increases.\nFurthermore, let’s display graph standard errors \\(n = 1\\) \\(n = 100\\) using new terminology, notation, definitions relating sampling.\nFIGURE 6.20: Standard errors sample proportion based sample sizes 1 100\nRemember key message last table: sample size \\(n\\) goes , “typical” error point estimate go , quantified standard error.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "moral-of-the-story",
    "chapter": "6 One Parameter",
    "heading": "6.3.4 The moral of the story",
    "text": "know two pieces information data, ? First, need measure center distribution. include mean median, shows center data points. Second, need measure variability distribution. understand center, must understand different (spread) data points one another. Thus, need measure like sd() MAD. summary statistics necessary understanding distribution. two figures encompass need know distribution? ! , allowed two numbers keep, valuable.mean median good estimate center posterior standard error mad good estimate variability posterior, +/- 2 standard errors covering 95% outcomes.standard error measures accuracy sample distribution compared population using standard deviation. Specifically, standard deviation data points divided square root sample size. , find larger sample sizes = lower standard errors = accurate representative guesses.really drive home point: standard error just fancy term uncertainty something don’t know. Standard error == (uncertain) beliefs.\nFIGURE 6.21: wondering much need know, follow helpful guide information learned chapter!\nhierarchy represents knowledge need understand standard error (SE). bottom, math. ’s foundation understanding, doesn’t need take away lesson. go , simplify topic. top pyramid basic levels understanding help remember future.know estimate plus minus two standard errors, know 95% confidence interval. valuable information. Standard error really just measure uncertain something know, thing estimating. recall SE, remember , , ’s complicated concept can distilled : way old people talk confidence intervals.Recall \\(\\hat{\\rho}\\) estimated value p comes taking sample. can billions billions \\(\\hat{\\rho}\\)’s. look large group \\(\\hat{\\rho}\\)’s, create distribution results represent possible values p based findings, compute standard error account uncertainty predictions. 95% confidence interval prediction == estimate plus minus two standard errors.regards fifth layer hierarchy, may wonder:“thought MADs thing standard deviations. Now say things standard errors. ?”MADs standard deviations , less, thing. measures variability distribution. cases, similar values. standard error also standard deviation. Specifically, standard deviation distribution estimates, distribution estimates , less, posterior. Therefore, can use MAD, like standard error, describe distribution variability distribution.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "cardinal-virtues-1",
    "chapter": "6 One Parameter",
    "heading": "6.4 Cardinal Virtues",
    "text": "Recall questions asked beginning Chapter:get 17 red beads random sample size 50 taken mixed urn, proportion \\(\\rho\\) beads urn red?probability, using urn, draw 8 red beads use shovel size 20?Use Cardinal Virtues guide thinking.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "wisdom-1",
    "chapter": "6 One Parameter",
    "heading": "6.4.1 Wisdom",
    "text": "Wisdom requires creation Preceptor Table, examination data, determination whether can (reasonably!) assume two come population.\nFIGURE 6.22: see another Precetor Table section.\nPreceptor Table table rows columns, , data missing, can easily answer questions. Note beads ID numbers printed . numbering arbitrary. ID just reminds us actual units consideration, even can tell apart, color. also include ID help visualize fact don’t know total number beads urn, question never tells us! 1,000 beads like physical urn earlier, million beads. ellipse bottom Preceptor Table denotes uncertainty regarding urn size.one outcome column, “Color,” causal model, need (least) two potential outcomes. Predictive models require one outcome.know color every bead, calculating proportion beads red, \\(\\rho\\), simple algebra. know \\(\\rho\\) can simulate answers questions.data , unfortunately, provides color 50 beads. , ID numbers. keeping track beads sample beads helpful.last step Wisdom consider whether can consider units Preceptor Table units data drawn population. case, many sampling scenarios, trivial may make assumption. rows data also rows Preceptor Table, may assume drawn distribution.also consider beads get sampled, others . consequence sampling mechanism. hope sampling mechanism near equal access members population, else samples biased. Almost samples bias, must make judgement call see data close enough data want (.e., Preceptor Table) can consider coming population. sample 50 beads taken mixed urn, hopefully near equal chance selecting bead, samples representative population.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "justice-1",
    "chapter": "6 One Parameter",
    "heading": "6.4.2 Justice",
    "text": "Justice starts construction Population Table. use table explore issues validity, stability representativeness. make assumption form data generating mechanism.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "population-table-2",
    "chapter": "6 One Parameter",
    "heading": "6.4.2.1 Population Table",
    "text": "use Population Table acknowledge wider source collected data .includes rows three sources: data units want (Preceptor Table), data units (actual data), data units care (rest population, included data Preceptor Table).PopulationKnown, specific urnTime sample - 2 years1?PopulationKnown, specific urnTime sample - 3 weeks200redPopulationKnown, specific urnTime sample - 10 days976?...............DataKnown, specific urnTime sample2whiteDataKnown, specific urnTime sample200redDataKnown, specific urnTime sample1080white...............PopulationKnown, specific urnTime sample + 10 days1?PopulationKnown, specific urnTime sample + 3 weeks200redPopulationKnown, specific urnTime sample + 2 years2025?...............Preceptor TableKnown, specific urnNow1?Preceptor TableKnown, specific urnNow200redPreceptor TableKnown, specific urnNow2078?...............PopulationKnown, specific urnNow + 10 days1?PopulationKnown, specific urnNow + 3 weeks200redPopulationKnown, specific urnNow + 2 years2300? specific row represents one subject, individual beads urn scenario. thousands even millions beads, provide 3 examples category, use elipse denote many subjects yet record.Population Table minimum 3 types columns: covariates, time, outcome(s):construct Preceptor Table answer question, must select covariates want subjects . answer questions, theory collect data subjects desired covariates. However, life tough, data get always ideal. urn scenario one covariate: location. Rather intuitively, urns can drastically different contents, easily answer question ’s important identify location one specific urn find proportion red.draw sample exact urn question asks us , data collect comes directly Preceptor Table, subjects population location (“Known, specific urn”). Preceptor Table Population categories essentially identical. perfect scenario us, rarely occurs real life.Population Tables always column time. answering question must specify time ’s asked, life changes time.must acknowledge sample urn taken time, contents urn past (data) different contents urn want answer question now (Preceptor Table). , wider population collected data : time collecting sample, anytime collecting .Finally, Population Tables outcome. Sometimes multiple outcome columns, like case casual models ideally want data subject treatment. urn problem, two outcomes: bead ID bead color.know covariates data match covariates Preceptor Table, Data, Preceptor Table, wider Population come urn. , can use bead ID tool notate members population data , ones unknown. always know color bead 200 sampled . also track bead 1 across Preceptor Table Population, never know value never sampled. Beads 978, 1080, 2025, 2078, 2300 change time simply different beads urn different times. know, someone sneakily dumping beads urn week. Either way, sample beads, don’t know colors.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "validity-stability-and-representativeness",
    "chapter": "6 One Parameter",
    "heading": "6.4.2.2 Validity, stability, and representativeness",
    "text": "Now created Population Table, can analyze key issues: validity, stability, representativeness.Validity involves columns data set. meaning columns consistent across different data sources? urn scenario, bead color sampled data bead color Preceptor Table mean thing? answer yes, validity can assumed easily. beads urn old color worn , may difficult conclude whether color column always means thing.Stability involves time. model — meaning mathematical formula value parameters — stable time? Realistically, urn always time. However, someone dumps red beads urn take sample? assume stability, proportion red beads urn, \\(\\rho\\), instant dump different proportion red urn . assume one tampering urn, assert stability across time periods.Representativeness involves data rows, specifically rows data versus rows might . rows data representative rows data? sample proportion similar actual population proportion, ideally want data random, unbiased selection population, using considerable sample size. context problem, sampling mechanism using shovel size 50 sample beads urn beads thoroughly mixed enough consider sample representative population, can move .",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "the-dgm",
    "chapter": "6 One Parameter",
    "heading": "6.4.2.3 The DGM",
    "text": "final aspect Justice assuming/creating mathematical formula — data generating mechanism, DGM — mimics process data comes us. DGM sampling scenarios two possible values often denoted follows:\\[ T_i  \\sim B(\\rho, n = 50) \\]total number red beads selected sample 50 beads, \\(T_i\\), equal one draw binomial distribution \\(n = 50\\) observations unknown probability \\(\\rho\\) proportion red beads urn.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "courage-1",
    "chapter": "6 One Parameter",
    "heading": "6.4.3 Courage",
    "text": "’ve evaluated data questions chosen mathematical formula. Now ’s time work code create model can simulate real world.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "bayesian-framework",
    "chapter": "6 One Parameter",
    "heading": "6.4.3.1 Bayesian framework",
    "text": "Bayesian data scientists make Bayesian models. means make specific assumptions consider different things either fixed variable compared data science frameworks. One important distinctions Bayesian data science, don’t know values parameters. Consider DGM created Justice:\\[ T_i  \\sim B(\\rho, n = 50) \\]non-Bayesian frameworks concerned probability distribution observed data, care much probability distribution \\(\\rho\\) assume fixed. \\(\\rho\\) fixed, equation becomes one simple binomial distribution. Think standard 2 dimensional plot.Us Bayesians consider observed data fixed. don’t consider alternate realities observed data different due sampling variation. Instead, concerned probability distribution parameter. urn scenario, \\(\\rho\\) variable, create separate binomial distribution possible value \\(\\rho\\). Think 3 dimensional joint distribution like created Section 5.6.essential understand joint distribution posterior, two concepts Bayesians use solve problems. provide quick quick review , including statistical notation may helpful .joint distribution, \\(p(y|\\theta)\\), models outcome \\(y\\) given one unknown parameter(s), \\(\\theta\\). equation illustrates exact concept addressed discussing distinctions Bayesian science: parameters variable, create separate distributions potential value. Combining distributions together creates joint distribution 3 dimensional plotted.posterior, \\(p(\\theta|y)\\), probability distribution parameter(s) \\(\\theta\\), created using data \\(y\\) updates beliefs. referenced posterior many times , definition change meaning.urn scenario, obtaining posterior involves first creating many binomial distributions possible population proportion. joint distribution, 3 dimensional model. select distribution corresponds data: 17 red beads sampled. can represent posterior following:\\[\\text{Prob}(\\text{models} | \\text{data} = 17)\\]equivalent taking 2 dimensional slice 3 dimensional model. left probability distribution parameter, \\(\\rho\\).",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "stan_glm",
    "chapter": "6 One Parameter",
    "heading": "6.4.3.2 stan_glm()",
    "text": "Given data set use mathematical formula work , next step write code. use rstanarm package, provides user friendly interface work statistical language Stan.rstanarm Stan appealing powerful. Functions stan_glm() can everything hand Chapter 5 lines code. use professional statistical library, objects make become complex. Chapter, provide steps answering questions. Chapter 7 provide detailed explanation objects make. clear, need fully understand section code works. introduction, formal lesson.Recall assumed binomial model data generating mechanism. stan_glm() denote family = binomial. addition type distribution, also need analyze outcome predictor variables involved. outcome quantity measuring, case total number red beads sample. predictors, use argument formula = red ~ 1, means model outcome based unknown proportion red beads urn, \\(\\rho\\).pass data binomial format: 1’s represent number successes (red beads drawn), 0’s represent number failures (white beads drawn). , pass tibble 17 red beads 33 white beads data.use refresh = 0 suppress behavior printing console, seed = 10 get output every time run code. resulting model :learn meaning output Chapter 7. fit_1 object, easy answer two sorts questions: posterior probability distribution \\(\\rho\\) predictions new draws urn. key new functions posterior_epred() former posterior_predict() latter.Let’s recreate posterior using posterior_epred():wish create distribution conditional covariate, want pass tibble 1 row newdata argument. empty tibble 0 rows, instead pass tibble junk data. constant = 1 completely meaningless. Now plot result:successfully created posterior distribution can finally answer question started chapter :get 17 red beads random sample size 50 taken mixed urn, proportion \\(\\rho\\) beads urn red?Look distribution created! can see bulk area posterior occurs approximately \\(\\rho\\) .28 .42, answer question likely 28% 42% beads urn red. Another reader may believe bulk area posterior occurs \\(\\rho\\) .25 .45, may conclude likely 25% 45% beads red. perfectly correct! range large enough acknowledge uncertainty regarding exact value \\(\\rho\\) acceptable.Although likely probability (highest bar histogram) occurs \\(\\rho\\) around .32, answer single number. posterior distribution just : distribution. sample, many different results proportion red beads entire urn. Certain proportions, like extremes close 0% 100%, essentially impossible due sample value 34%. hand, just easily sampled 16 18 beads urn, sample proportions 32% 36% plausible.means , can provide range possibilities (can estimate possibilities occur frequently), can never say know total number red beads certainty. know chance \\(\\rho\\) .28 .42, chance \\(\\rho\\) .15 .24 .42 .56, almost chance \\(\\rho\\) .15 .56. posterior can visualize probabilities .Another important question remains:4,000 rows stan_glm() tibble?default, stan_glm() sample posterior 2 sets 2,000 iterations. needed can change default number iterations using iter argument, reasons . us may still want know sample posterior first place. use entire posterior? answer posterior theoretical beast, makes difficult work .example, wanted know probability \\(\\rho\\) .3 .4? answer using pure posterior, need calculate area distribution \\(.3 < \\rho < .4\\). difficult seems, posterior distribution, individual observations work ’s continuous!Instead, can work draws posterior. enough draws create close approximation posterior models counts observations. approximation; exactly posterior, close enough purposes. can easily convert posterior distribution posterior probability distribution, making area graph sum 1. posterior probability distribution often used visual aid, percentages easy conceptualize raw numbers. One way convert posterior distribution probability distribution group value \\(\\rho\\) turn counts probabilities:can also accomplish similar effect passing aes(y = after_stat(count/sum(count)) geom_histogram() plotting. Oftentimes, like answering probability \\(\\rho\\) .3 .4, can work posterior distribution end. Just divide number draws meet condition (.3 .4), total number draws.approximately 54% chance \\(\\rho\\) .3 .4. Give draws posterior can show world!\nFIGURE 6.23: Time quick musical interlude. ’ve earned .\n",
    "code": "\nlibrary(rstanarm)\n\nfit_1 <- stan_glm(formula = red ~ 1, \n                  data = tibble(red = c(rep(1, 17), \n                                        rep(0, 33))),\n                  family = binomial,\n                  refresh = 0,\n                  seed = 10) \nfit_1## stan_glm\n##  family:       binomial [logit]\n##  formula:      red ~ 1\n##  observations: 50\n##  predictors:   1\n## ------\n##             Median MAD_SD\n## (Intercept) -0.7    0.3  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nppd_for_p <- posterior_epred(fit_1, \n                newdata = tibble(constant = 1)) |> \n  as_tibble() |>\n  rename(p = `1`)\n  \n# posterior_epred() will unhelpfully name the column of our tibble to \"1\". We\n# have two options: either refer to the column name as `1`, or rename the column\n# to make it less confusing. We will rename the column to \"p\" in this chapter, but you\n# will oftentimes see `1` in later chapters.\n\nppd_for_p## # A tibble: 4,000 × 1\n##        p\n##    <dbl>\n##  1 0.176\n##  2 0.165\n##  3 0.173\n##  4 0.246\n##  5 0.258\n##  6 0.258\n##  7 0.243\n##  8 0.242\n##  9 0.307\n## 10 0.300\n## # … with 3,990 more rows\nppd_for_p |> \n  ggplot(aes(x = p)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Distribution is centered at .34\",\n         x = \"Proportion p of Red Beads in Urn\",\n         y = \"Probability\") + \n  \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nppd_for_p |>\n  round(digits = 2) |>\n  group_by(p) |>\n  summarize(prob = n()/nrow(ppd_for_p)) |>\n  arrange(desc(prob))## # A tibble: 46 × 2\n##        p   prob\n##    <dbl>  <dbl>\n##  1  0.35 0.0615\n##  2  0.32 0.0605\n##  3  0.36 0.0588\n##  4  0.34 0.0585\n##  5  0.33 0.058 \n##  6  0.31 0.054 \n##  7  0.37 0.0535\n##  8  0.39 0.047 \n##  9  0.3  0.0462\n## 10  0.38 0.0442\n## # … with 36 more rows\nsum(ppd_for_p$p > .3 & ppd_for_p$p < .4)/nrow(ppd_for_p)## [1] 0.54"
  },
  {
    "path": "one-parameter.html",
    "id": "temperance-1",
    "chapter": "6 One Parameter",
    "heading": "6.4.4 Temperance",
    "text": "fitted model object fit_1, can now make predictions future answer questions.Recall second question started :probability, using urn, draw 8 red beads use shovel size 20?",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "using-the-posterior",
    "chapter": "6 One Parameter",
    "heading": "6.4.4.1 Using the posterior",
    "text": "Whenever someone asks question, need decide posterior probability distribution make easy answer question. case, know posterior probability distribution number red beads shovel size 20, question likelihood drawing 8 (value) easy answer.posterior probability distribution probability tricky thing. much easier just estimate posterior probability distribution outcome — number red beads 20 — work distribution order answer probability-type questions.predict future unknown samples, use posterior_predict(). pass posterior created using stan_glm() first argument, want estimate number red draws shovel size 20, use pass tibble 20 rows newdata.4,000 rows represent one instance us predicting future sample urn, column represents color bead shovel slot. create new column called total use rowSums() calculate total number red beads drawn sample. Finally, graph resulting distribution.successfully created posterior probability distribution number red beads drawn shovel size 20. answer question, us may wondering made predictions using posterior_predict() instead posterior_epred(). Let’s examine happens use posterior_epred() instead.\nFIGURE 6.24: Using posterior_epred()\nhappened? posterior_epred() shows distribution entire population, continuous. expected predictions can fractional, posterior_epred() returns draws posterior (can fractional) contingent covariate. scenario covariates create expected predictions , posterior_epred() just returns posterior, re-scaled 0 20 beads instead 0 1 . shape distributions identical:hand, posterior_predict() models posterior distribution future individuals. scenario, model binomial distribution discrete random variable. bars appear real numbers 1 16, predicting probability individual samples. draw fractions beads sample. Using posterior_predict() essentially replicates DGM, taking many virtual draws urn summarizing results.summary, use posterior_predict() predict outcome individual(s) future, use posterior_epred() model probability across entire population using posterior. answer question, want know probability outcomes using single shovel size 20. use posterior_predict() model taking individual samples many times, can analyze probabilities. confusing fret! plenty practice 2 functions remainder Primer.Now let’s attempt actually answer question:probability, using urn, draw 8 red beads use shovel size 20?posterior_predict() takes predictive draws us, can simply count number draws 8 red beads, divide total number draws.approximately 25% chance draw 8 red beads sample size 20.visualize probability graphically, reuse posterior, add new column called above_eight TRUE total > 8.can can set fill histogram above_eight == TRUE visualize probability drawing 8 red beads.red bars illustrate area specific section curve, compared entire area curve. question requires looking new area curve. someones asks question, two things. First, providing instructions posterior create. , results shovel 20 slots. Second, asking question area curve specific region. , region number red beads greater 8 highlighted red. Therefore, area curve red get estimate.See Chapter 7 thorough discussion use rstanarm. package main tool rest Primer.",
    "code": "\nposterior_predict(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble()## # A tibble: 4,000 × 20\n##      `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`  `13`\n##    <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n##  1     0     0     0     0     1     0     0     0     1     0     0     0     1\n##  2     0     0     0     0     1     0     1     0     0     0     1     0     0\n##  3     0     1     0     0     0     0     0     0     0     0     1     0     0\n##  4     0     1     0     1     1     0     0     1     0     0     0     0     1\n##  5     0     0     0     1     0     0     1     0     0     0     0     1     0\n##  6     0     0     0     0     0     1     1     0     0     0     0     0     1\n##  7     0     1     1     1     0     0     1     1     0     0     0     0     1\n##  8     1     1     1     1     0     0     1     0     1     1     0     0     0\n##  9     1     0     1     1     0     0     0     0     0     1     0     1     1\n## 10     0     0     1     1     0     0     0     0     0     0     0     0     0\n## # … with 3,990 more rows, and 7 more variables: `14` <int>, `15` <int>,\n## #   `16` <int>, `17` <int>, `18` <int>, `19` <int>, `20` <int>\nppd_reds_in_20 <- posterior_predict(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |> \n  select(total)\n\n\nppd_reds_in_20   |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in 20-slot shovel\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\") +  \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\npost_epred <- posterior_epred(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |> \n  select(total)\n\npost_epred  |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n    labs(title = \"Posterior probability distribution using posterior_epred()\",\n         subtitle = \"In our scenario, using posterior_epred() is incorrect\",\n         x = \"Number of red beads\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\npost_epred |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n  labs(x = \"Number of red beads sampled out of 20\",\n       y = \"Probability\") + \n  ppd_for_p  |>\n  ggplot(aes(x = p)) + \n  geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n  labs(x = \"Proportion red in urn\",\n       y = \"Probability\") + \n  plot_annotation(title = 'Expected prediction for sample of size 20 on left, posterior on right.',\n                  subtitle = \"The two distributions have an identical shape.\")\n# Same code as earlier, included as a refresher. \nppd_reds_in_20 <- posterior_predict(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |> \n  select(total)\n\n# Calculating the probability\nsum(ppd_reds_in_20$total > 8)/length(ppd_reds_in_20$total)## [1] 0.25\nppd_reds_in_20 <- posterior_predict(fit_1, \n                    newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |>\n  select(total) |>\n  mutate(above_eight = ifelse(total > 8, TRUE, FALSE))\n\nppd_reds_in_20## # A tibble: 4,000 × 2\n##    total above_eight\n##    <dbl> <lgl>      \n##  1     4 FALSE      \n##  2     3 FALSE      \n##  3     5 FALSE      \n##  4     6 FALSE      \n##  5     3 FALSE      \n##  6     4 FALSE      \n##  7     6 FALSE      \n##  8     3 FALSE      \n##  9     7 FALSE      \n## 10     6 FALSE      \n## # … with 3,990 more rows\nppd_reds_in_20   |> \n  \n  # Set fill as above_eight. \n  \n  ggplot(aes(x = total, fill = above_eight)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n  \n  # Scale_fill_manual()  is calling grey for the first color and red for the\n  # second color. This is going to highlight the portion of the curve that we\n  # want to highlight in red.\n\n  scale_fill_manual(values = c('grey50', 'red')) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in 20-slot shovel\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\",\n         fill = \"More than 8 Red Beads Drawn?\") +  \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "path": "one-parameter.html",
    "id": "assesing-uncertainty",
    "chapter": "6 One Parameter",
    "heading": "6.4.4.2 Assesing uncertainty",
    "text": "process Cardinal Virtues process may seem tedious, use good reason. Wisdom determined data need answer question, Justice analyzed whether data can answer question. skipped steps, risk creating models “bad data” Courage Temperance, draw misleading conclusions.successfully answered questions using Cardinal Virtues, must also realize limitations models. Earlier claimed 26% chance draw 8 red beads sample size 20. However, must modest claims. question particular urn, urns? Clearly, predictions based upon posterior specific urn, conclusions can applied particular urn; conclusions generalizable urns.Furthermore, certain prediction Temperance 26% samples shovel size 20 using urn produce 8 red beads?First let’s model situation:\\[ T_i  \\sim B(\\rho, n = 20) \\]total number red beads selected sample 20 beads, \\(T_i\\), modeled binomial distribution \\(n = 20\\) observations unknown probability \\(\\rho\\) proportion red beads urn.must realize 26% estimate based entire posterior. uncertain exact value \\(\\rho\\), posterior_predict() takes uncertainty account creating model. However, fact actual value \\(\\rho\\), even don’t know .actual value \\(\\rho\\) 30%, can calculate correct answer question using binomial probability distribution function:maybe actual value \\(\\rho\\) 40%:Sadly, answer question 11%, 40%, value , even extreme value. never know correct answer, must settle best estimate 26% factors uncertainty regarding true value \\(\\rho\\). Long story short, models never truth.",
    "code": "\npbinom(q = 8, size = 20, prob = .3, lower.tail = FALSE)## [1] 0.11\npbinom(q = 8, size = 20, prob = .4, lower.tail = FALSE)## [1] 0.4"
  },
  {
    "path": "one-parameter.html",
    "id": "bad-practices",
    "chapter": "6 One Parameter",
    "heading": "6.4.5 Bad practices",
    "text": "Although process long, successfully gone cardinal virtues answered questions like real data scientists. section address pitfalls avoid.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "confidence-intervals",
    "chapter": "6 One Parameter",
    "heading": "6.4.5.1 Confidence intervals",
    "text": "Let’s go back first question:get 17 red beads random sample size 50 taken mixed urn, proportion \\(\\rho\\) beads urn red?decided , answered question using confidence intervals statistics. First calculate standard error:\\[  SE = \\frac{\\sigma\\text{ data}}{\\sqrt{\\text{sample size}}} = \\frac{.4785}{\\sqrt{50}} \\approx .067\\]calculate numerator taking standard deviation dataset 50 observation, 1’s represent red beads 0’s represent white beads:can use standard error create 95% confidence interval:\\[  CI = \\bar{x} \\hspace{.1cm} \\pm 2SE = .34 \\hspace{.1cm} \\pm .134\\]95% confidence, proportion red beads urn 21% 47%.correct, quite difficult conceptualize. boss needs quick answer, means can use confidence interval save us work earlier! However, must careful confidence intervals. chosen answer first question using , unable answer questions Temperance. confidence interval gives us general range uncertainty, answer question requires knowledge value parameter, confidence interval us little good. data scientists create posterior distribution quantify uncertainty answer questions.",
    "code": "\nsd(c(rep(1, 17), rep(0, 33)))## [1] 0.48"
  },
  {
    "path": "one-parameter.html",
    "id": "hypothesis-tests",
    "chapter": "6 One Parameter",
    "heading": "6.4.5.2 Hypothesis tests",
    "text": "Statisticians, like use confidence intervals, also use hypothesis tests quickly try answer questions. Recall view hypothesis tests: Amateurs test. Professionals summarize. Traditionally, scientific papers much interested estimating \\(\\rho\\). interested testing specific hypotheses. mean ?Let’s look possible hypothesis urn paradigm: equal number red white beads urn. null hypothesis, denoted \\(H_0\\), theory testing, alternative hypothesis, denoted \\(H_a\\), represents opposite theory. Therefore, hypothesis designed :\\(H_0\\): equal number red white beads urn.\\(H_a\\): equal number red white beads urn.Can reject hypothesis? Convention: 95% confidence interval excludes null hypothesis, reject . , mean posterior estimate (plus minus 2 standard errors) excluded possibility red white beads equal (\\(\\rho = .5\\)) can reject null hypothesis. previous section determined 95% confidence interval 21% 47%. 50% outside interval, reject null hypothesis, conclude unlikely proportion beads urn 50%.testing theory \\(\\rho = .45\\) instead, null hypothesis fall within confidence interval. mean accept null hypothesis, just don’t reject . can’t reject null hypothesis test essentially useless, idea whether theory true. scenario know possibility \\(\\rho = .45\\). ’re just back started! never test — unless boss demands test. Use judgment, make models, summarize knowledge world, use summary make decisions.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "sampling-case-study",
    "chapter": "6 One Parameter",
    "heading": "6.5 Case study: Polls",
    "text": "Let’s now switch gears realistic sampling scenario: poll. practice, pollsters take 1,000 repeated samples previous sampling activities, rather take single sample ’s large possible.December 4, 2013, National Public Radio US reported poll President Obama’s approval rating among young Americans aged 18-29 article, “Poll: Support Obama Among Young Americans Eroding.” poll conducted Kennedy School’s Institute Politics Harvard University. quote article:voting large numbers 2008 2012, young Americans souring President Obama.According new Harvard University Institute Politics poll, just 41 percent millennials — adults ages 18-29 — approve Obama’s job performance, lowest-ever standing among group 11-point drop April.Let’s tie elements real life poll new article “tactile” “virtual” urn activity Sections 6.1 6.2 using terminology, notations, definitions learned Section 6.3. ’ll see sampling activity urn idealized version pollsters trying real life.First, (Study) Population \\(N\\) individuals observations interest?Urn: \\(N\\) = 1000 identically sized red white beadsObama poll: \\(N\\) = ? young Americans aged 18-29Second, population parameter?Urn: population proportion \\(\\rho\\) beads urn red.Obama poll: population proportion \\(\\rho\\) young Americans approve Obama’s job performance.Third, census look like?Urn: Manually going \\(N\\) = 1000 beads exactly computing population proportion \\(\\rho\\) beads red.Obama poll: Locating \\(N\\) young Americans asking approve Obama’s job performance. case, don’t even know population size \\(N\\) !Fourth, perform sampling obtain sample size \\(n\\)?Urn: Using shovel \\(n\\) slots.Obama poll: One method get list phone numbers young Americans pick \\(n\\) phone numbers. poll’s case, sample size poll \\(n = 2089\\) young Americans.Fifth, point estimate (AKA sample statistic) unknown population parameter?Urn: sample proportion \\(\\hat{\\rho}\\) beads shovel red.Obama poll: sample proportion \\(\\hat{\\rho}\\) young Americans sample approve Obama’s job performance. poll’s case, \\(\\hat{\\rho} = 0.41 = 41\\%\\), quoted percentage second paragraph article.Sixth, sampling procedure representative?Urn: contents shovel representative contents urn? mixed urn sampling, can feel confident .Obama poll: sample \\(n = 2089\\) young Americans representative young Americans aged 18-29? depends whether sampling random (samples rarely )Seventh, samples generalizable greater population?Urn: sample proportion \\(\\hat{\\rho}\\) shovel’s beads red “good guess” population proportion \\(\\rho\\) urn’s beads red? Given sample representative, answer yes.Obama poll: sample proportion \\(\\hat{\\rho} = 0.41\\) sample young Americans supported Obama “good guess” population proportion \\(\\rho\\) young Americans supported Obama time 2013? words, can confidently say roughly 41% young Americans approved Obama time poll? , depends whether sampling random.Eighth, sampling procedure unbiased? words, observations equal chance included sample?Urn: Since bead equally sized mixed urn using shovel, bead equal chance included sample hence sampling unbiased.Obama poll: young Americans equal chance represented poll? , depends whether sampling random.Ninth lastly, sampling done random?Urn: long mixed urn sufficiently sampling, samples random.Obama poll: sample conducted random? can’t answer question without knowing sampling methodology used Kennedy School’s Institute Politics Harvard University. ’ll discuss end section.words, poll Kennedy School’s Institute Politics Harvard University can thought instance using shovel sample beads urn. Furthermore, another polling company conducted similar poll young Americans roughly time, likely get different estimate 41%. due sampling variation.Let’s now revisit sampling paradigm Subsection 6.3.1:general:sampling sample size \\(n\\) done random, thenthe sample unbiased representative population size \\(N\\), thusany result based sample can generalize population, thusthe point estimate “good guess” unknown population parameter, thusinstead performing census, can infer population using sampling.Specific urn:extract sample \\(n = 50\\) beads random, words, mix equally sized beads using shovel, thenthe contents shovel unbiased representation contents urn’s 1000 beads, thusany result based shovel’s beads can generalize urn, thusthe sample proportion \\(\\hat{\\rho}\\) \\(n = 50\\) beads shovel red “good guess” population proportion \\(\\rho\\) \\(N = 1000\\) beads red, thusinstead manually going 1000 beads urn, can infer urn using shovel.Specific Obama poll:way contacting randomly chosen sample 2089 young Americans polling approval President Obama 2013, thenthese 2089 young Americans unbiased representative sample young Americans 2013, thusany results based sample 2089 young Americans can generalize entire population young Americans 2013, thusthe reported sample approval rating 41% 2089 young Americans good guess true approval rating among young Americans 2013, thusinstead performing expensive census young Americans 2013, can infer young Americans 2013 using polling.can see, critical sample obtained Kennedy School’s Institute Politics Harvard University truly random order infer young Americans’ opinions Obama. sample truly random? ’s hard answer questions without knowing sampling methodology used.example, Kennedy School’s Institute Politics Harvard University conducted poll using mobile phone numbers? People without mobile phones left therefore represented sample. flaw example censoring, exclusion certain datapoints due issue data collection. results incomplete observation increases prediction uncertainty estimand, Obama’s approval rating among young Americans. Ensuring samples random easy sampling urn exercises; however, real life situation like Obama poll, much harder .",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "summary-7",
    "chapter": "6 One Parameter",
    "heading": "6.6 Summary",
    "text": "Key lesson:truth! true value \\(\\rho\\) know! want create posterior probability distribution summarizes knowledge. care posterior probability distribution p. center distribution around mean median proportion sample. sd (mad) posterior standard deviation divided square root sample size! Note thing standard deviation repeated samples.Key lesson:journey reality, predictions, standard error predictions, posterior probability distribution p. sequence:p (truth) \\(\\Rightarrow\\) \\(\\hat{\\rho}\\) (estimate) \\(\\Rightarrow\\) standard error \\(\\hat{\\rho}\\) (black box math mumbo jumbo computer simulation magic) \\(\\Rightarrow\\) posterior probability distribution p (beliefs truth).journey shows beliefs truth develop work. begin p; p truth, true unknown value estimating. \\(\\hat{\\rho}\\) estimate p. can millions millions \\(\\hat{\\rho}\\)’s. Next, must take standard error estimates (\\(\\hat{\\rho}\\)’s) account uncertainty predictions. Finally – thing need – create posterior probability distribution p. distribution used answer key questions p. highlights:",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "tactial-sampling",
    "chapter": "6 One Parameter",
    "heading": "6.6.1 Tactial sampling",
    "text": "Sampling allows us make guesses unknown, difficult--obtain value looking smaller subset data generalizing larger population.Sampling preferable urn example counting 1000 beads urn intensive tedious.Sampling preferable real-world often impossible sample “beads” (people) population. sampling, see variations results. known sampling variation expected, especially draw samples random (unbiased).",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "virtual-sampling-1",
    "chapter": "6 One Parameter",
    "heading": "6.6.2 Virtual sampling",
    "text": "creating virtual analog urn shovel, able look even samples observe effects sampling size results.samples yield even distributions resemble bell.Larger sample sizes decrease standard deviation, meaning resulting proportions red closer one another sample sizes smaller. means larger samples = lower SD = precise guesses.want repeat task multiple times, like comparing distributions 3 sample sizes, use mapping functions like expand_grid() cut repetative code.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "standard-error",
    "chapter": "6 One Parameter",
    "heading": "6.6.3 Standard error",
    "text": "Standard error just fancy term uncertainty something don’t know. Standard error \\(\\approx\\) (uncertain) beliefs.standard error measures accuracy sample distribution compared population using standard deviation.find larger sample sizes \\(\\implies\\) lower standard errors \\(\\implies\\) accurate estimates.know two pieces information data, need measure center distribution (like mean median) measure variability distribution (like sd MAD).SE refers standard deviation sample statistic (aka point estimate), mean median. Therefore, “standard error mean” refers standard deviation distribution sample means taken population.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "cardinal-virtues-2",
    "chapter": "6 One Parameter",
    "heading": "6.6.4 Cardinal Virtues",
    "text": "stan_glm() can create joint distribution estimate posterior probability distribution, conditional data passed data argument. much easier way create posterior distribution, explored detail Chapter 7.use posterior distribution answer questions.",
    "code": ""
  },
  {
    "path": "one-parameter.html",
    "id": "case-study-polls",
    "chapter": "6 One Parameter",
    "heading": "6.6.5 Case study: Polls",
    "text": "Polling complex real life application binomial distribution.Real life sampling extremely prone error. emphasized enough. often forced choose precision accuracy real-life sampling.chapter, performed tactile virtual sampling exercises infer unknown parameter also presented case study sampling real life polls. case, used sample proportion \\(\\hat{\\rho}\\) estimate population proportion \\(\\rho\\). However, just limited scenarios related proportions. words, can use sampling estimate population parameters using point estimates well.continue journey, recall case Primrose Everdeen represents: matter realistic model , predictions never certain.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "two-parameters",
    "chapter": "7 Two Parameters",
    "heading": "7 Two Parameters",
    "text": "Chapter 6, learned inference. created joint distribution models consideration data might observed. observed data, went joint distribution conditional distribution possible models given data , fact, observe. conditional distribution, suitably normalized, posterior probability distribution space possible models. distribution, can answer question might (reasonably) ask.pain ass whole process ! professionals actually go steps every time work data science problem? ! absurd. Instead, professionals use standard tools , automated fashion, take care steps, taking us directly assumptions data posterior:\\[\\text{Prob}(\\text{models} | \\text{data} = \\text{data observed})\\]Even , however, relative likelihood different models important. Models invisible, mental entities physical presence unicorns leprechauns. world , make test predictions. People better models make better predictions. matters.addition change tools, two key differences chapter. First, Chapter 6 used models just one parameter: number red beads, can also transform parameter, \\(p\\), number red beads divided 1,000. model Chapter 6 binomial, one unknown parameter \\(p\\) models. chapter, two unknown parameters: mean \\(\\mu\\) height US standard deviation, \\(\\sigma\\), normally distributed error term.Second, Chapter 6 dealt limited set specific models: 1,001 , precise. procedure used just saw Chapter 5. chapter, hand, continuous parameters.point, understand reason making models , primarily, making models fun (although )! reason , confronted question, face decisions. must decide variables X Y. must choose datasets , B C. Confronted decision, need make model world help us.real world complex. substantive decision problem includes great deal complexity requires even context. time get level detail now. , simplify. going create model height adult men. use model answer four questions:average height men?average height men?probability next man meet taller 180 centimeters?probability next man meet taller 180 centimeters?probability , among next 4 men meet, tallest least 10 cm taller shortest?probability , among next 4 men meet, tallest least 10 cm taller shortest?posterior probability distribution height 3rd tallest man next 100 meet?posterior probability distribution height 3rd tallest man next 100 meet?hope chapter , answering four questions, ’ll gain better thorough understanding professionals data science.Data science ultimately moral act, use four Cardinal Virtues — Wisdom, Justice, Courage Temperance — organize approach.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "wisdom-2",
    "chapter": "7 Two Parameters",
    "heading": "7.1 Wisdom",
    "text": "",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "preceptor-table-1",
    "chapter": "7 Two Parameters",
    "heading": "7.1.1 Preceptor Table",
    "text": "rows columns data need , , calculation number interest trivial? want know average height adult India, Preceptor Table include row adult India column height. scenario, want know average height men, “men” includes males Earth age 18.One key aspect Preceptor Table whether need one potential outcome order calculate estimand. Mainly: modeling (just) prediction (also) modeling causation? need causal model, one estimates attitude treatment control? causal model, Preceptor Table require two columns outcome. case, modeling causation; thus, need two outcome columns.Predictive models care nothing causation. Causal models often also concerned prediction, means measuring quality model. , looking prediction., ideal table look like? Assuming predicting height every male planet Earth moment time, height data every male person 18 years age. means almost 4 billion rows, one male person, includes column height.sample Preceptor Table:table extend way person 4 billion--something. table, questions answered basic math. inference necessary. Now ’ve seen ideal dataset, actual data look like?",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "eda-for-nhanes",
    "chapter": "7 Two Parameters",
    "heading": "7.1.2 EDA for nhanes",
    "text": "quest find suitable data, find nhanes dataset National Health Nutrition Examination Survey conducted 2009 2011 Centers Disease Control Prevention, examines health pieces data children adults United States.nhanes includes 15 variables, including physical attributes like weight height. Let’s restrict attention subset, focusing age, gender height.Now, let’s examine random sample data:Notice decimal height column ch7. height <dbl> <int>.Let’s also run glimpse() new data.lookout anything suspicious. NA’s data set? types data columns, .e. age characterized integer instead double? females males?can never look data closely.addition glimpse(), can run skim(), skimr package, calculate summary statistics.TABLE 7.1: Data summaryVariable type: characterVariable type: numericInteresting! 353 missing values height subset data. Just using glimpse() show us . Let’s filter NA’s using drop_na(). delete rows value variable missing. want examine height men (boys, females), let’s limit data include adult males.Let’s plot data using geom_density() geom_histogram().’ll focusing subset nhanes data, designed answer questions. , instead using data, ’ll just use 50 randomly selected observations. (set.seed() function ensures 50 observations selected every time code run.)data — sample adult American men decade ago — allow us answer questions, however roughly? notion population comes play.",
    "code": "\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(skimr)\nglimpse(nhanes)## Rows: 10,000\n## Columns: 15\n## $ survey         <int> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2…\n## $ gender         <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male…\n## $ age            <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58…\n## $ race           <chr> \"White\", \"White\", \"White\", \"Other\", \"White\", \"White\", \"…\n## $ education      <fct> High School, High School, High School, NA, Some College…\n## $ hh_income      <fct> 25000-34999, 25000-34999, 25000-34999, 20000-24999, 350…\n## $ weight         <dbl> 87, 87, 87, 17, 87, 30, 35, 76, 76, 76, 68, 78, 75, 39,…\n## $ height         <dbl> 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170, …\n## $ bmi            <dbl> 32, 32, 32, 15, 31, 17, 21, 27, 27, 27, 24, 24, 26, 19,…\n## $ pulse          <int> 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80,…\n## $ diabetes       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ general_health <int> 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, NA…\n## $ depressed      <fct> Several, Several, Several, NA, Several, NA, NA, None, N…\n## $ pregnancies    <int> NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA,…\n## $ sleep          <int> 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA,…\nnhanes |> \n  select(age, gender, height)## # A tibble: 10,000 × 3\n##      age gender height\n##    <int> <chr>   <dbl>\n##  1    34 Male     165.\n##  2    34 Male     165.\n##  3    34 Male     165.\n##  4     4 Male     105.\n##  5    49 Female   168.\n##  6     9 Male     133.\n##  7     8 Male     131.\n##  8    45 Female   167.\n##  9    45 Female   167.\n## 10    45 Female   167.\n## # … with 9,990 more rows\nnhanes |> \n  select(age, gender, height) |> \n  slice_sample(n = 5)## # A tibble: 5 × 3\n##     age gender height\n##   <int> <chr>   <dbl>\n## 1     2 Female   95.5\n## 2    15 Male    180. \n## 3    23 Female  162. \n## 4    45 Female  155. \n## 5    48 Female  164.\nnhanes |> \n  select(age, gender, height) |> \n  glimpse()## Rows: 10,000\n## Columns: 3\n## $ age    <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9,…\n## $ gender <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fema…\n## $ height <dbl> 165, 165, 165, 105, 168, 133, 131, 167, 167, 167, 170, 182, 169…\nnhanes |> \n  select(age, gender, height) |> \n  skim()\nch7 <- nhanes |> \n  filter(gender == \"Male\", age >= 18) |> \n  select(height) |> \n  drop_na()\nch7 |>\n  ggplot(aes(x = height)) + \n  geom_density(color = \"black\",\n               fill = \"red\",\n               alpha = .2) +\n  \n  # You can have multiple geom layers in a plot by simply adding them one after\n  # the other.\n  \n  # Be careful though! Often times you'll need to give each additional layer its\n  # own mapping argument in order for it to show up properly.\n  \n  geom_histogram(mapping = aes(y = ..density..),\n                 bins = 15,\n                 color = \"black\",\n                 fill = \"blue\",\n                 alpha = .2) +\n  \n  # In order to use Greek letters and other mathematical expressions in plot\n  # labels and titles, you'll need to use the expression() function, with\n  # appropriate arguments.\n  \n  labs(x = expression(mu),\n       y = \"Density\",\n       title = \"Height (cm) in NHANES Dataset\",\n       color = \"Sex\") +\n  theme_classic()\nch7_all <- nhanes |>\n  filter(gender == \"Male\", age >= 18) |>\n  select(height) |>\n  drop_na() \n\nset.seed(9)\n\nch7 <- ch7_all |> \n  slice_sample(n = 50)"
  },
  {
    "path": "two-parameters.html",
    "id": "population",
    "chapter": "7 Two Parameters",
    "heading": "7.1.3 Population",
    "text": "One important components Wisdom concept “population.”population set people data — participants CDC’s Health Nutrition Examination Survey conducted 2009 2011. dataset. set voters like data. rows Preceptor Table. population larger — potentially much larger — set individuals include data data want. Generally, population much larger either data data want.case, want predict average height males today, people 2009-2011! also want predict height males outside United States, group excluded dataset. reasonable generate conclusions world group? likely, . However, limited data work determine far willing generalize groups.judgment call, matter Wisdom, whether may assume data data want (.e., Preceptor Table) drawn population.Note judgement call differs discussion representativeness Justice. Wisdom, ask questions data collected, says data. Justice, ask questions (already knowing data collected) whether inferences made using data can apply broader scope population. Suppose, instance, men sampled NHANES NBA players. Wisdom tell us heights likely much greater average person. Justice asks whether data representative average person, modifications imposed fix potential bias.social sciences, never perfect relationship data question trying answer. Data American males past thing data American males today. data men France Mexico.Yet, data relevant. Right? certainly better nothing.Using -perfect data generally better using data .-perfect data always better? ! problem estimating median height 5th grade girls Tokyo, doubt data relevant. Wisdom recognizes danger using non-relevant data build model mistakenly using model way make situation worse. data won’t help, don’t use data, don’t build model. Better just use common sense experience. find better data.now, accept data works.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "justice-2",
    "chapter": "7 Two Parameters",
    "heading": "7.2 Justice",
    "text": "looked data decided “close enough” questions creating model help us come better answers, move Justice.Justice emphasizes key concepts:Population Table, structure includes row every unit population. generally break rows Population Table three categories: data units want (Preceptor Table), data units actually (actual data), data units care (rest population, included data Preceptor Table).data representative population?meaning columns consistent, .e., can assume validity?\nmake assumption data generating mechanism.inspect representativeness validity Population Table. Representativeness focuses rows table, validity focuses columns. Let’s explore means depth.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "the-population-table",
    "chapter": "7 Two Parameters",
    "heading": "7.2.1 The Population Table",
    "text": "Population Table shows rows three sources: Preceptor Table, actual data, population (outside data).Preceptor Table rows contain information want know order answer questions. rows contain entries covariates (sex year) contain outcome results (height). trying answer questions male population 2021, sex entries rows read “Male” year entries rows read “2021”.actual data rows contain information know. rows contain entries covariates outcomes. case, actual data comes study conducted males 2009-2011, sex entries rows read “Male” year entries rows either read “2009”, “2010”, “2011”.population rows contain data. subjects fall desired population, data. , rows missing.Preceptor TableMale2021?Preceptor TableMale2021?............Actual DataMale2009180Actual DataMale2011160Actual DataMale2010168............Population???Population???Population???",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "representativeness-1",
    "chapter": "7 Two Parameters",
    "heading": "7.2.2 Representativeness",
    "text": "’ve stated , representativeness involves rows. specifically, rows data representative rows data? Ideally, data random, unbiased selection population, answer question yes.nhanes data, case? time investigate.According CDC, individuals invited participate NHANES based randomized process. First, United States divided number geographical groups (ensure counties areas). groups, counties randomly selected participate. county randomly selected, members households county notified upcoming survey, must volunteer time participate.\nclear process goes several layers randomization (promising!). said, many counties excluded end process. also possible certain groups communities less representative greater population, though know certain.also fact participation voluntary. Perhaps certain individuals (immobile, elderly, anxious) less likely participate. Perhaps individuals hospitalized get opportunity participate. impacts data!Regardless, can assume process ensures people United States somewhat equal chance surveyed. Thus, data representative population.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "validity-1",
    "chapter": "7 Two Parameters",
    "heading": "7.2.3 Validity",
    "text": "Validity involves columns. specifically, whether columns mean thing. Consider following example: predicting height two different datasets. datasets measure height centimeters, may assume validity — columns identical meaning. However, validity much complex appears first glance.Consider method measurements. first dataset, participants asked remove shoes? allowed keep thick socks? case second dataset, columns technically represent truth.even smaller differences impact validity. instance, known — course day — average human’s spine compresses inch time wake time go sleep! first set data collected morning, second set data collected evening, predictions may .best can, need investigate possible reasons may able say columns mean thing. find issue, may need adjust one set data match . instance, knew height taken evening one sample (measurements taken morning), may add inch height findings. ensure validity.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "functional-form",
    "chapter": "7 Two Parameters",
    "heading": "7.2.4 Functional form",
    "text": "However, little mathematical notation make modeling assumptions clear, bring precision approach. case:\\[ y_i =  \\mu + \\epsilon_i \\]\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) height male \\(\\). \\(\\mu\\) average height males population. \\(\\epsilon_i\\) “error term,” difference height male \\(\\) average height males.\\(\\epsilon_i\\) normally distributed mean 0 standard deviation \\(\\sigma\\). mean 0 relates concept accuracy; assuming data representative enough accurate, can expect average error 0. standard deviation, hand, relates concept precision; smaller \\(\\sigma\\) , precise data , larger \\(\\sigma\\) , less precise data .simplest model can construct. Note:model two unknown parameters: \\(\\mu\\) \\(\\sigma\\). can anything else need estimate values parameters. Can ever know exact value? ! Perfection lies God’s R code. , using Bayesian approach similar used Chapters 5 6, able create posterior probability distribution parameter.model wrong, models.model wrong, models.parameter care \\(\\mu\\). parameter substantively meaningful interpretation. meaning \\(\\sigma\\) difficult describe, also don’t particular care value. Parameters like \\(\\sigma\\) context nuisance auxiliary parameters. still estimate posterior distributions, don’t really care posteriors look like.parameter care \\(\\mu\\). parameter substantively meaningful interpretation. meaning \\(\\sigma\\) difficult describe, also don’t particular care value. Parameters like \\(\\sigma\\) context nuisance auxiliary parameters. still estimate posterior distributions, don’t really care posteriors look like.\\(\\mu\\) average height men sample. can calculate directly. 175.61. estimation required! Instead, \\(\\mu\\) average height men population. Recall discussions Chapter 6 population universe people/units/whatever seek draw conclusions. level, seems simple. deeper level, subtle. example, walking around Copenhagen, population really care , order answer three questions, set adult men might meet today. population adult men US 2010. close enough? better nothing? want assume men nhanes (data ) men meet Copenhagen today (data want ) drawn population. case different details matter.\\(\\mu\\) average height men sample. can calculate directly. 175.61. estimation required! Instead, \\(\\mu\\) average height men population. Recall discussions Chapter 6 population universe people/units/whatever seek draw conclusions. level, seems simple. deeper level, subtle. example, walking around Copenhagen, population really care , order answer three questions, set adult men might meet today. population adult men US 2010. close enough? better nothing? want assume men nhanes (data ) men meet Copenhagen today (data want ) drawn population. case different details matter.\\(\\sigma\\) estimate standard deviation errors, .e., variability height accounting mean.\\(\\sigma\\) estimate standard deviation errors, .e., variability height accounting mean.Consider:\\[\\text{outcome} = \\text{model} + \\text{model}\\]\ncase, outcome height individual male. variable, also called “response,” trying understand /explain /predict. model creation, mixture data parameters, attempt capture underlying structure world generates outcome.difference outcome model? definition, model, blooming buzzing complexity real world. model always incomplete won’t capture everything. Whatever model misses thrown error term.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "courage-2",
    "chapter": "7 Two Parameters",
    "heading": "7.3 Courage",
    "text": "data science, deal words, math, code, important code. need Courage create model, take leap faith can make ideas real.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "stan_glm-1",
    "chapter": "7 Two Parameters",
    "heading": "7.3.0.1 stan_glm",
    "text": "Bayesian models hard create R. rstanarm package provides tools need, importantly function stan_glm().first argument stan_glm() data, case filtered ch7 tibble contains 50 observations. mandatory argument formula want use build model. case, since predictor variables, formula height ~ 1.Details:may take time. Bayesian models, especially ones large amounts data, can take longer might like. Indeed, computational limits main reason Bayesian approaches — , extent, still — little used. creating models, often want use cache = TRUE code chunk option. saves result model don’t recalculate every time knit document.may take time. Bayesian models, especially ones large amounts data, can take longer might like. Indeed, computational limits main reason Bayesian approaches — , extent, still — little used. creating models, often want use cache = TRUE code chunk option. saves result model don’t recalculate every time knit document.data argument, like usage R, input data.data argument, like usage R, input data.don’t set refresh = 0, model puke many lines confusing output. can learn output reading help page stan_glm(). output provides details fitting process runs well diagnostics final result. details beyond scope book.don’t set refresh = 0, model puke many lines confusing output. can learn output reading help page stan_glm(). output provides details fitting process runs well diagnostics final result. details beyond scope book.always assign result call stan_glm() object, . convention, name object often include word “fit” indicate fitted model object.always assign result call stan_glm() object, . convention, name object often include word “fit” indicate fitted model object.direct connection mathematical form model created Justice code use fit model Courage. height ~ 1 code equivalent \\(y_i = \\mu\\).direct connection mathematical form model created Justice code use fit model Courage. height ~ 1 code equivalent \\(y_i = \\mu\\).default value family gaussian, need include call . Justice section, assumption \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) equivalent using gaussian. \\(\\epsilon_i\\) different distribution, need use different family. saw example situation end Chapter 6 performed urn analysis using stan_glm() setting family = binomial.default value family gaussian, need include call . Justice section, assumption \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) equivalent using gaussian. \\(\\epsilon_i\\) different distribution, need use different family. saw example situation end Chapter 6 performed urn analysis using stan_glm() setting family = binomial.Although can use standalone function set.seed() order make code reproducible, convenient use seed argument within stan_glm(). Even though fitting process, unavoidably, contains random component, get exact answer set seed = 9 rerun code. matter number use seed.Although can use standalone function set.seed() order make code reproducible, convenient use seed argument within stan_glm(). Even though fitting process, unavoidably, contains random component, get exact answer set seed = 9 rerun code. matter number use seed.",
    "code": "\nlibrary(rstanarm)\nfit_obj <- stan_glm(data = ch7, \n                    formula = height ~ 1, \n                    family = gaussian, \n                    refresh = 0,\n                    seed = 9)"
  },
  {
    "path": "two-parameters.html",
    "id": "printed-model",
    "chapter": "7 Two Parameters",
    "heading": "7.3.0.1.1 Printed model",
    "text": "several ways examine fitted model. simplest print . Recall just typing x prompt writing print(x).first line telling us model used, case stan_glm().second line tells us model using Gaussian, normal, distribution. discussed distribution Section 2.8.1.4. typically use default unless working lefthand variable extremely non-normal, e.g., something takes two values like 0/1 TRUE/FALSE. Since height (roughly) normally distributed, Gaussian distribution good choice.third line gives us back formula provided. creating model predicting height constant — just simplest model can create. Formulas R constructed two parts. First, left side tilde (“~” symbol) “response” “dependent” variable, thing trying explain. Since model height, height goes lefthand side. Second, “explanatory” “independent” “predictor” variables righthand side tilde. often many variables , simplest possible model, one, single constant. (number 1 indicates constant. mean think everyone height 1.)fourth fifth lines output tell us 50 observations one predictor (constant). , terminology bit confusing. mean suggest \\(\\mu\\) “constant?” means , although \\(\\mu\\)’s value unknown, fixed. change person person. 1 formula corresponds parameter \\(\\mu\\) mathematical definition model.knew information fit model. R records fit_obj don’t want forget . second half display gives summary parameter values. can look just second half detail argument:see output two parameters model: “(Intercept)” “sigma”. can confusing! Recall thing care \\(\\mu\\), average height population. Preceptor Table — row every adult male population care missing data — \\(\\mu\\) trivial calculate, uncertainty. know named parameter \\(\\mu\\). R sees 1 formula. fields statistics, constant term called “intercept.” , now three things — \\(\\mu\\) (math), 1 (code), “(Intercept)” (output) — refer exact concept. last time terminology confusing.point, stan_glm() — rather print() method objects created stan_glm() — problem. full posteriors \\(\\mu\\) \\(\\sigma\\). simple printed summary. can’t show entire distribution. , best numbers provide? right answer question! , choice provide “Median” posterior “MAD_SD”.Anytime distribution, whether posterior probability otherwise, important single number associated measure location. data? two common choices measure mean median. use median posterior distributions can often quite skewed, making mean less stable measure.Anytime distribution, whether posterior probability otherwise, important single number associated measure location. data? two common choices measure mean median. use median posterior distributions can often quite skewed, making mean less stable measure.second important number summarizing distribution concerns spread. far data spread around center? common measure used standard deviation. MAD SD, scaled median absolute deviation, another. variable normal distribution, standard deviation MAD SD similar. MAD SD much robust outliers, used . (Note MAD SD measure referred mad till now. measure calculated mad() command R. Terminology confusing, usual.)second important number summarizing distribution concerns spread. far data spread around center? common measure used standard deviation. MAD SD, scaled median absolute deviation, another. variable normal distribution, standard deviation MAD SD similar. MAD SD much robust outliers, used . (Note MAD SD measure referred mad till now. measure calculated mad() command R. Terminology confusing, usual.)can also change number digits shown:Now understand meaning Median MAD_SD display, can interpret actual numbers. median intercept, 175.6, median posterior distribution \\(\\mu\\), average height men population. median sigma, 8.5, median posterior distribution true \\(\\sigma\\), can roughly understood variability height men, account estimate \\(\\mu\\).MAD SD parameter measure variability posterior distributions parameter. spread ? Speaking roughly, 95% mass posterior probability distribution located within +/- 2 MAD SDs median. example, 95% confident true value \\(\\mu\\) somewhere 173.2 178.",
    "code": "\nfit_obj## stan_glm\n##  family:       gaussian [identity]\n##  formula:      height ~ 1\n##  observations: 50\n##  predictors:   1\n## ------\n##             Median MAD_SD\n## (Intercept) 175.6    1.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 8.5    0.9   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nprint(fit_obj, detail = FALSE)##             Median MAD_SD\n## (Intercept) 175.6    1.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 8.5    0.9\nprint(fit_obj, detail = FALSE, digits = 1)##             Median MAD_SD\n## (Intercept) 175.6    1.2 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 8.5    0.9"
  },
  {
    "path": "two-parameters.html",
    "id": "plotting-the-posterior-distributions",
    "chapter": "7 Two Parameters",
    "heading": "7.3.0.1.2 Plotting the posterior distributions",
    "text": "Instead math heads, can display posterior probability distributions. Pictures speak math mumbles. Fortunately, getting draws posteriors easy:4,000 rows draws estimated posteriors, column. like vectors result calling functions like rnorm() rbinom(). can create plot similar way:Although possible variable names like “(Intercept)”, recommended. Avoid weird names! stuck , place backticks. Even better, rename , .Note title includes word “Posterior” complete term “Posterior Probability Distribution.” practice going forward. “Posterior” means “Posterior Distribution” , posterior distribution sum range 1 “posterior probability distribution.” plots, “posterior” implies “posterior probability distribution.”Always go back first principles. truth, unknown number, fact world. knew everything, Preceptor Table, inference necessary. Algebra suffice. Alas, imperfect world, choice data scientist. always uncertain. summarize knowledge unknown numbers posterior probability distributions, “posteriors” short., \\(\\sigma\\) usually nuisance parameter. don’t really care value , rarely plot .",
    "code": "\nfit_obj |> \n  as_tibble()## # A tibble: 4,000 × 2\n##    `(Intercept)` sigma\n##            <dbl> <dbl>\n##  1          175.  8.34\n##  2          175.  8.16\n##  3          174.  8.26\n##  4          174.  8.25\n##  5          173.  9.43\n##  6          175.  9.24\n##  7          174.  8.47\n##  8          174.  8.26\n##  9          177.  8.71\n## 10          173.  9.06\n## # … with 3,990 more rows\nfit_obj |> \n  as_tibble() |> \n  ggplot(aes(x = `(Intercept)`)) +\n  \n# Recall that after_stat() allows us to work with \"stat variables\" that have\n# been calculated by ggplot, such as \"count\" and \"density\".\n  \n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Average Male Height\",\n         subtitle = \"American men average around 176 cm in height\",\n         x = expression(mu), \n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n# The same plot as above, but for sigma instead of mu.\n\nfit_obj |> \n  as_tibble() |> \n  ggplot(aes(x = sigma)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Standard Deviation of Male Height\",\n         subtitle = \"The standard deviation of height is around 7 to 11 cm\",\n         x = expression(sigma),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "path": "two-parameters.html",
    "id": "testing",
    "chapter": "7 Two Parameters",
    "heading": "7.3.1 Testing",
    "text": "",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "temperance-2",
    "chapter": "7 Two Parameters",
    "heading": "7.4 Temperance",
    "text": "model. can ? Let’s answer four questions started.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "question-1",
    "chapter": "7 Two Parameters",
    "heading": "7.4.1 Question 1",
    "text": "average height men?Preceptor Table, one row every man alive, actual height, calculate number easily. Just take average 3 billion rows! Alas, actual Preceptor Table, vast majority heights missing. Question marks make simple algebra impossible. , unknown number, need estimate posterior probability distribution. Objects created stan_glm() make easy .use posterior_epred() many times. two key arguments object, fitted model object returned stan_glm(), newdata, tibble contains covariate values associated unit (units) want make forecast. (case, newdata can tibble intercept-model make use covariates. don’t really need variable named constant, including harm.) epred posterior_epred() stands expected prediction. words, pick random adult male “expect” height . also call expected value.use as_tibble() convert matrix returned posterior_epred(). tibble 1 column 4,000 rows. column, unhelpfully named 1, 4,000 draws posterior probability distribution expected height random male. Recall earlier chapters posterior probability distribution draws posterior probability distribution , less, thing. , rather, posterior probability distribution, ownself, hard work . Draws distribution, hand, easy manipulate. use draws answer questions.Converting 4,000 draws posterior probability distribution straightforward.rest Primer filled graphics like one. make dozens . fundamental structure algebra real number. fundamental structure data science posterior probability distribution. need able create interpret .",
    "code": "\nnewobs <- tibble(constant = 1)\n\npe <- posterior_epred(object = fit_obj,\n                      newdata = newobs) |> \n        as_tibble()\n\npe## # A tibble: 4,000 × 1\n##      `1`\n##    <dbl>\n##  1  175.\n##  2  175.\n##  3  174.\n##  4  174.\n##  5  173.\n##  6  175.\n##  7  174.\n##  8  174.\n##  9  177.\n## 10  173.\n## # … with 3,990 more rows\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Adult Male Height\",\n         subtitle = \"Note that the plot is very similar to the one created with the parameters\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "path": "two-parameters.html",
    "id": "question-2",
    "chapter": "7 Two Parameters",
    "heading": "7.4.2 Question 2",
    "text": "probability next adult male meet taller 180 centimeters?two fundamentally different kinds unknowns care : expected values (previous question) predicted values. former, interested specific individual. individual value irrelevant. predicted values, care, average, specific person. former, use posterior_epred(). latter, relevant function posterior_predict(). functions return draws posterior probability distribution, unknown number underlies posterior different.Recall mathematics:\\[ y_i =  \\mu + \\epsilon_i \\]expected values averages, can ignore \\(\\epsilon_i\\) term formula. expected value \\(\\epsilon_i\\) zero since, assumption, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). However, can’t ignore \\(\\epsilon_i\\) predicting height single individual., straightforward turn draws posterior probability distribution graphic:Note:posterior individual much wider posterior expected value.posterior individual much wider posterior expected value.Eyeballing, seems like 1 3 chance next man meet, randomly chosen man, taller 180 cm.Eyeballing, seems like 1 3 chance next man meet, randomly chosen man, taller 180 cm.can calculate exact probability manipulating tibble draws directly.can calculate exact probability manipulating tibble draws directly.30% draws posterior probability distribution greater 180 cm, 30% chance next individual taller 180 cm., key conceptual difficulty population. problem actually involves walking around London, wherever, today. data involve America 2010. things! totally different. Knowing whether data “close enough” problem want solve heart Wisdom. Yet decision made start process, decision create model first place. Now created model, look virtue Temperance guidance using model. data never perfect match world face. need temper confidence act humility. forecasts never good naive use model might suggest. Reality surprise us. need take model’s claims family-sized portion salt.",
    "code": "\nnewobs <- tibble(constant = 1)\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble()\n\npp## # A tibble: 4,000 × 1\n##      `1`\n##    <dbl>\n##  1  179.\n##  2  164.\n##  3  172.\n##  4  172.\n##  5  167.\n##  6  184.\n##  7  184.\n##  8  159.\n##  9  182.\n## 10  168.\n## # … with 3,990 more rows\npp |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Height of Random Male\",\n         subtitle = \"Uncertainty for a single individual is much greater than for the expected value\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nsum(pp$`1` > 180)/length(pp$`1`)## [1] 0.31"
  },
  {
    "path": "two-parameters.html",
    "id": "question-3",
    "chapter": "7 Two Parameters",
    "heading": "7.4.3 Question 3",
    "text": "probability , among next 4 men meet, tallest least 10 cm taller shortest?Bayesian models beautiful , via magic simulation, can answer (almost!) question. question four random individuals, need posterior_predict() give us four sets draws four identical posterior probability distributions. Start new newobs:need predict X individuals, need tibble X rows, regardless whether rows otherwise identical.Note need add mutate_all(.numeric) end pipe. caused bug — least awkwardness — whereby variable type provided posterior_predict() ppd rather dbl. Using mutate_all(.numeric) makes column type double. Avoid working columns type ppd. lead heartache.next step calculate number interest. can , directly, draw height tallest shortest 4 random men. However, drawn 4 random men, can calculate numbers, difference .steps serve template much analysis later. often hard create model directly thing want know. easy way create model estimates height difference directly. easy, however, create model allows random draws.Give us enough random draws, tibble store , can estimate world.random draws posterior distribution care , graphing posterior probability distribution -old, -old.85% chance , meeting 4 random men, tallest least 10 cm taller shortest. Exact calculation:",
    "code": "\nnewobs <- tibble(constant = rep(1, 4))\n\nnewobs## # A tibble: 4 × 1\n##   constant\n##      <dbl>\n## 1        1\n## 2        1\n## 3        1\n## 4        1\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble() |> \n        mutate_all(as.numeric)\npp## # A tibble: 4,000 × 4\n##      `1`   `2`   `3`   `4`\n##    <dbl> <dbl> <dbl> <dbl>\n##  1  177.  170.  169.  164.\n##  2  178.  167.  164.  185.\n##  3  177.  180.  181.  170.\n##  4  174.  169.  166.  169.\n##  5  176.  191.  186.  165.\n##  6  173.  164.  160.  183.\n##  7  181.  199.  172.  157.\n##  8  169.  181.  174.  174.\n##  9  185.  180.  179.  170.\n## 10  177.  184.  164.  164.\n## # … with 3,990 more rows\n# First part of the code is the same as we did above.\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble() |> \n        mutate_all(as.numeric) |> \n  \n        # Second part of the code requires some trickery.\n  \n        rowwise() |> \n        mutate(tallest = max(c_across())) |> \n        mutate(shortest = min(c_across())) |> \n        mutate(diff = tallest - shortest) \n        \npp        ## # A tibble: 4,000 × 7\n## # Rowwise: \n##      `1`   `2`   `3`   `4` tallest shortest  diff\n##    <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> <dbl>\n##  1  167.  169.  159.  177.    177.     159.  17.6\n##  2  179.  187.  178.  170.    187.     170.  16.8\n##  3  169.  183.  168.  172.    183.     168.  15.2\n##  4  158.  179.  175.  161.    179.     158.  20.9\n##  5  182.  182.  158.  170.    182.     158.  24.5\n##  6  166.  153.  189.  165.    189.     153.  36.0\n##  7  169.  172.  190.  175.    190.     169.  20.3\n##  8  172.  164.  178.  197.    197.     164.  32.1\n##  9  190.  184.  177.  171.    190.     171.  18.7\n## 10  161.  192.  165.  193.    193.     161.  32.4\n## # … with 3,990 more rows\npp |> \n  ggplot(aes(x = diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Max Height Difference Among Four Men\",\n         subtitle = \"The expected value for this difference would be much more narrow\",\n         x = \"Height Difference in Centimeters\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(breaks = seq(0, 50, 10),\n                       labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) \nsum(pp$diff > 10) / length(pp$diff)## [1] 0.85"
  },
  {
    "path": "two-parameters.html",
    "id": "question-4",
    "chapter": "7 Two Parameters",
    "heading": "7.4.4 Question 4",
    "text": "posterior probability distribution height 3rd tallest man next 100 meet?approach work almost question.Explore pp object. 101 columns: one hundred 100 individual heights one column 3rd tallest among . done hard work, plotting easy:",
    "code": "\nnewobs <- tibble(constant = rep(1, 100))\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) |> \n  rowwise() |> \n  mutate(third_tallest = sort(c_across(), \n                              decreasing = TRUE)[3])\npp |> \n  ggplot(aes(x = third_tallest, y = after_stat(count / sum(count)))) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Posterior for Height of 3rd Tallest Man from Next 100\",\n         subtitle = \"Should we have more or less certainty about behavior in the tails?\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) "
  },
  {
    "path": "two-parameters.html",
    "id": "three-levels-of-knowledge",
    "chapter": "7 Two Parameters",
    "heading": "7.4.5 Three Levels of Knowledge",
    "text": "answering questions , can easy falsely believe delivering truth. case. fact, three primary levels knowledge need understand order account uncertainty.three primary levels knowledge possible knowledge scenario include: Truth (Preceptor Table), DGM Posterior, Posterior.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "the-truth",
    "chapter": "7 Two Parameters",
    "heading": "7.4.5.1 The Truth",
    "text": "know Truth (capital “T”), know Preceptor Table. knowledge, can directly answer question precisely. can calculate individual’s height, summary measure might interested , like average height different ages countries.level knowledge possible omniscient power, one can see every outcome every individual every treatment. Truth show, given individual, actions control, actions treatment, little factor impacted decisions.Truth represents highest level knowledge one can — , questions merely require algebra. need estimate treatment effect, different treatment effects different groups people. need predict — know.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "dgm-posterior",
    "chapter": "7 Two Parameters",
    "heading": "7.4.5.2 DGM posterior",
    "text": "DGM posterior next level knowledge, lacks omniscient quality Truth. posterior posterior calculate perfect knowledge data generating mechanism, meaning correct model structure exact parameter values. often falsely conflated “posterior,” subject error model structure parameter value estimations.DGM posterior posterior — estimate parameters based data predict future latest relevant information possible. difference , calculate posteriors unknown value DGM posterior, expect posteriors perfect.",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "our-posterior",
    "chapter": "7 Two Parameters",
    "heading": "7.4.5.3 Our posterior",
    "text": "Unfortunately, posterior possesses even less certainty! real world, don’t perfect knowledge DGM: model structure exact parameter values. mean?go boss, tell best guess. informed estimate based relevant data possible. data, created posterior average height males.mean certain average height lies probable outcome posterior? course ! tell boss, shocking find actual average height less estimate.lot assumptions make process building model, processes Wisdom, subject error. Perhaps data match future well hoped. Ultimately, try account uncertainty estimates. Even safeguard, aren’t surprised bit .",
    "code": ""
  },
  {
    "path": "two-parameters.html",
    "id": "zero-one-outcomes",
    "chapter": "7 Two Parameters",
    "heading": "7.5 0/1 Outcomes",
    "text": "Variables well-behaved, continuous ranges easiest handle. started height simple. Sadly, however, many variables like height. Consider gender, variable nhanes takes two possible values: “Male” “Female”. way like construct model explains predicts height, like build model explains predicts gender. want answer questions like:probability random person 180 cm tall female?Wisdom suggests start looking data. models use numbers, need create new variable, female, 1 Females 0 Males.just fit linear model, ? Consider:Recall default value family gaussian, need include . Initially, fitted model seems OK.Comparing two individuals differ height 1 cm, expect taller individual 3% lower probability female. unreasonable. problems show extremes. Consider fitted values across range data.Using 1 Female 0 Male allows us interpret fitted values probability person female male. handy natural interpretation. problem linear model arises , case, model suggests values outside 0 1. values , definition, impossible. People 190 cm tall -25% chance female.Justice suggests different functional form, one restricts fitted values acceptable range. Look closely math:\\[  p(\\text{Female} = 1) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{height})}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{height})} \\]inverse logistic function, don’t worry details. Mathematical formulas never Google search away. Instead, note range restricted. Even \\(\\beta_0 + \\beta_1 \\text{height}\\) large number, ratio bound 1. Similarly, matter negative \\(\\beta_0 + \\beta_1 \\text{height}\\) , ratio can never smaller 0. model can , ever, produce impossible values.Whenever two categories outcome, use family = binomial.Courage allows us use tools fitting logistic regression fitting linear models.One major difference linear logistic models parameters latter much harder interpret. mean, substantively, \\(\\beta_1\\) -0.26? topic advanced course.Fortunately, parameters care . epiphenomenon, unicorns imagination. Instead, want answers questions, Temperance — functions rstanarm — guide. Recall question:probability random person 180 cm tall female?1 20 chance 180 centimeter tall person female.Note x y axes probabilities. Whenever create posterior probability distribution , definition, y-axis probability. x-axis unknown number know. unknown number can anything — weight average male, height 3rd tallest 100 men, probability 180 cm tall person female. probability just another number. interpretation always.Another major difference logistic models posterior_epred() posterior_predict() return different types objects. posterior_epred() returns probabilities, . posterior_predict(), hand, returns predictions, name suggests. words, returns zeros ones. Consider another question:group 100 people 180 centimeters tall, many women?show just first 4 columns convenience. column 4,000 draws posterior predictive distribution gender person 180 cm tall. (Since 100 people height, columns draws distribution.)can manipulate object row--row basis.total number women row. Manipulating draws row--row basis common.5 6 women likely number consistent answer first question. found random person 180 cm tall 5% 6% chance female. , 100 people, 5 6 seems reasonable total. expected value posterior_epred(), although provide sense center predictive distribution , tell us much range possible outcomes. , need posterior_predict().",
    "code": "\nch7_b <- nhanes |> \n  select(age, gender, height) |>\n  mutate(female = ifelse(gender == \"Female\", 1, 0)) |> \n  filter(age >= 18) |> \n  select(female, height) |> \n  drop_na()\n\nch7_b## # A tibble: 7,424 × 2\n##    female height\n##     <dbl>  <dbl>\n##  1      0   165.\n##  2      0   165.\n##  3      0   165.\n##  4      1   168.\n##  5      1   167.\n##  6      1   167.\n##  7      1   167.\n##  8      0   170.\n##  9      0   182.\n## 10      0   169.\n## # … with 7,414 more rows\nch7_b |> \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Men are taller than women\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(0, 1),\n                     labels = c(\"Male\", \"Female\"))\nfit_gender_linear <- stan_glm(data = ch7_b,\n                              formula = female ~ height,\n                              family = gaussian,\n                              refresh = 0,\n                              seed = 82) \nprint(fit_gender_linear, digits = 2)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      female ~ height\n##  observations: 7424\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept)  6.22   0.07 \n## height      -0.03   0.00 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.36   0.00  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nch7_b |> \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  geom_smooth(aes(y = fitted(fit_gender_linear)),\n              method = \"lm\",\n              formula = y ~ x,\n              se = FALSE) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Some fitted values are impossible\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(-0.5, 0, 0.5, 1, 1.5),\n                     labels = c(\"-50%\", \"0% (Male)\", \n                                \"50%\", \"100% (Female)\",\n                                \"150%\"))\nfit_2 <- stan_glm(data = ch7_b,\n                  formula = female ~ height,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 27)\nprint(fit_2, digits = 3)## stan_glm\n##  family:       binomial [logit]\n##  formula:      female ~ height\n##  observations: 7424\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept) 43.389  0.999\n## height      -0.257  0.006\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nnewobs <- tibble(height = 180)\n\npe <- posterior_epred(fit_2, newdata = newobs) |> \n  as_tibble()\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for p(female | height = 180 cm)\",\n         subtitle = \"There is a 5-6% chance that a person this tall is female\",\n         x = \"Probability\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) \nnewobs <- tibble(height = rep(180, 100))\n\npp <- posterior_predict(fit_2, newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) \n\npp[, 1:4]## # A tibble: 4,000 × 4\n##      `1`   `2`   `3`   `4`\n##    <dbl> <dbl> <dbl> <dbl>\n##  1     0     0     0     0\n##  2     0     0     0     0\n##  3     0     0     0     0\n##  4     0     0     0     0\n##  5     0     0     0     0\n##  6     0     0     0     0\n##  7     0     0     0     0\n##  8     0     0     0     0\n##  9     0     0     0     0\n## 10     1     0     0     0\n## # … with 3,990 more rows\npp <- posterior_predict(fit_2, newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) |> \n  rowwise() |> \n  mutate(total = sum(c_across()))\n\npp[, c(\"1\", \"2\", \"100\", \"total\")]## # A tibble: 4,000 × 4\n## # Rowwise: \n##      `1`   `2` `100` total\n##    <dbl> <dbl> <dbl> <dbl>\n##  1     0     0     0     5\n##  2     0     0     0     3\n##  3     0     0     0     7\n##  4     0     0     0     8\n##  5     0     0     0     6\n##  6     0     0     0     5\n##  7     0     0     0     5\n##  8     0     0     0     2\n##  9     0     0     0     6\n## 10     0     0     0     4\n## # … with 3,990 more rows\npp |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Number of Women among 100 People 180 cm Tall\",\n         subtitle = \"Consistent with probability estimate above\",\n         x = \"Number of Women\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) "
  },
  {
    "path": "two-parameters.html",
    "id": "summary-8",
    "chapter": "7 Two Parameters",
    "heading": "7.6 Summary",
    "text": "next five chapters follow process just completed . start decision make. luck, data guide us. (Without data, even best data scientist struggle make progress.) Wisdom asks us: “data close enough decision face make using data helpful?” Often times, answer “.”start build model, Justice guide us. model descriptive causal? mathematical relationship dependent variable trying explain independent variables can use explain ? assumptions making distribution error term?set model framework, need Courage implement model code. Without code, math world useless. created model, need understand . posterior distributions unknown parameters? seem sensible? interpret ?Temperance guides final step. model, can finally get back decision motivated exercise first place. can use model make statements world, confirm model consistent world use model make predictions numbers know.Let’s practice process another dozen times.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "three-parameters",
    "chapter": "8 Three Parameters",
    "heading": "8 Three Parameters",
    "text": "Models parameters. Chapter 6 created models single parameter \\(p\\), proportion red beads urn. Chapter 7, used models two parameters: \\(\\mu\\) (average height population, generically known model “intercept”) \\(\\sigma\\) (variation height population). — can guess going? — build models three parameters: \\(\\sigma\\) (serves role throughout book) two “coefficients.” models relate continuous predictor outcome, two parameters labeled \\(\\beta_0\\) \\(\\beta_1\\). models estimate two averages, parameters \\(\\beta_1\\) \\(\\beta_2\\). notation confusing, least different academic fields use inconsistent schemes. Follow Cardinal Virtues tackle problem step step.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "wisdom-3",
    "chapter": "8 Three Parameters",
    "heading": "8.1 Wisdom",
    "text": "",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "preceptor-table-2",
    "chapter": "8 Three Parameters",
    "heading": "8.1.1 Preceptor Table",
    "text": "Wisdom begins considering questions desire answer data set given. chapter, going ask series questions involving train commuters’ ages, party affiliations, incomes, political ideology, well causal effect exposure Spanish-speakers attitude toward immigration. questions pertain train commuters US today. Given types questions, Preceptor Table :Commuter 123Democrat50000Liberal38Commuter 218Republican150000Not Liberal77.....................Commuter 100049Republican100000Liberal48Commuter 100138Democrat200000Liberal97.....................Recall: Preceptor Table smallest possible table , missing data, questions easy answer. answer questions — like “, today, average age train commuters US?” — need row every train commuter.Notice one questions causal effect: change immigration attitudes caused exposed Spanish-speakers? Answering causal questions requires (least) two potential outcomes: immigration attitude receive treatment exposed Spanish-speakers .created Preceptor Table, now look data : trains data set primer.data package.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "eda-for-trains",
    "chapter": "8 Three Parameters",
    "heading": "8.1.2 EDA for trains",
    "text": "Always explore data. Recall discussion Chapter 4. Enos (2014) randomly placed Spanish-speaking confederates nine train platforms around Boston, Massachusetts. Exposure Spanish-speakers – treatment – influenced attitudes toward immigration. reactions measured changes answers three survey questions. Load necessary libraries look data.data include information respondent’s gender, political affiliations, age, income . treatment indicates whether subject control treatment group. key outcomes attitudes toward immigration (att_start) (att_end) experiment. Type ?trains read help page information variable.Let’s restrict attention subset variables need answer questions, specified Preceptor Table. age age respondent; party political party affiliation; liberal whether liberal ; income income respondent; treatment specifies whether respondent given treatment exposed Spanish-speakers train station; att_end tell us respondent’s attitude immigration treatment, .always smart look random slices data:att_end measure person’s attitude toward immigration. higher number means conservative, .e., exclusionary stance toward immigration United States.Pay attention variable types. make sense? Perhaps. certainly grounds suspicion. att_end double rather integer? values data appear integers, benefit variables doubles. party character variable treatment factor variable? intentional choices made creator tibble, .e., us. , mistakes. likely, choices mixture sensible arbitrary. Regardless, responsibility notice . can’t make good model without looking closely data using.TABLE 8.1: Data summaryVariable type: characterVariable type: factorVariable type: logicalVariable type: numericskim() shows us different values treatment factor. Unfortunately, character variables like party. ranges age att_end seem reasonable. Recall participants asked three questions immigration issues, allowed answer indicated strength agreement scale form 1 5, higher values indicating agreement conservative viewpoints. att_end sum responses three questions, liberal possible value 3 conservative 15.",
    "code": "\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(skimr)\nlibrary(tidyverse)\nglimpse(trains)## Rows: 115\n## Columns: 15\n## $ treatment           <fct> Treated, Treated, Treated, Treated, Control, Treat…\n## $ att_start           <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6,…\n## $ att_end             <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7,…\n## $ gender              <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Femal…\n## $ race                <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"Whit…\n## $ liberal             <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FAL…\n## $ party               <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", …\n## $ age                 <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24…\n## $ income              <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 875…\n## $ line                <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framing…\n## $ station             <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"…\n## $ hisp_perc           <dbl> 0.026, 0.015, 0.019, 0.019, 0.019, 0.023, 0.030, 0…\n## $ ideology_start      <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3,…\n## $ ideology_end        <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3,…\n## $ income_in_thousands <dbl> 135, 105, 135, 300, 135, 88, 88, 135, 105, 135, 10…\nch8 <- trains |> \n  select(age, att_end, party, income, treatment, liberal)\nch8 |> \n  slice_sample(n = 5)## # A tibble: 5 × 6\n##     age att_end party    income treatment liberal\n##   <int>   <dbl> <chr>     <dbl> <fct>     <lgl>  \n## 1    38      11 Democrat  87500 Treated   FALSE  \n## 2    31       9 Democrat  87500 Control   TRUE   \n## 3    45      11 Democrat 300000 Treated   FALSE  \n## 4    52      13 Democrat 135000 Treated   FALSE  \n## 5    67       9 Democrat 135000 Control   TRUE\nch8 |> \n  glimpse()## Rows: 115\n## Columns: 6\n## $ age       <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40, 53, …\n## $ att_end   <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 13, 8,…\n## $ party     <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Democrat\"…\n## $ income    <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 135000…\n## $ treatment <fct> Treated, Treated, Treated, Treated, Control, Treated, Contro…\n## $ liberal   <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE,…\nch8 |> \n  skim()"
  },
  {
    "path": "three-parameters.html",
    "id": "population-1",
    "chapter": "8 Three Parameters",
    "heading": "8.1.3 Population",
    "text": "data limited. 115 observations, 2012 involving train commuters Boston. Can assume data data Preceptor Table drawn population? judgment, along advice colleagues, can guide .truth . data real enough, created Preceptor Table. Whether population can assume data Preceptor Table might drawn TRUE/FALSE question.Preceptor Table US train commuters today. However, data involves train commuters Boston area 2012. , must ask data Boston 2012 can drawn population train commuters US today.key concept idea “population.” larger population data (conceptually) drawn? interested age individuals data set, need inference. know everyone’s ages already. need tools like stan_glm() seek understand individuals data.issue always: data data want come population? connection two, progress impossible. , case, willing consider population US residents last decade (including today), willing assume single population Preceptor Table data set drawn, can use data create model answer questions.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "population-table-3",
    "chapter": "8 Three Parameters",
    "heading": "8.1.4 Population Table",
    "text": "determined Preceptor Table data drawn population, can produce Population Table.Population Table includes rows three sources: Preceptor Table, actual data, remainder population.rows Preceptor Table contain information want know order answer questions. rows contain entries covariates (city year) contain outcome results. trying answer questions train commuter population 2021, city entries rows vary year entries rows read “2021”.actual data rows contain information know. rows contain entries covariates outcomes. case, actual data comes study conducted train commuters around Boston, MA 2012, city entries rows read “Boston, MA” year entries rows read “2012”.population rows contain data. subjects fall desired population, data. , rows missing. However, may include, example, data range years 2012, since considered little change overtime, also consider data representative drawn larger population train commuters years near 2012.population table includes commuters Preceptor Table information ideally answer questions data specific Boston, MA. Additionally, population table also includes groups people within population data Preceptor table drawn don’t .Population?2008??????Population?2009??????...........................DataBoston, MA201243Democrat150000Liberal6?DataBoston, MA201252Republican50000Not Liberal?2...........................Population?2013??????Population?2014??????...........................Preceptor TableWilmington, DE202145Republican45000Not Liberal??Preceptor TableAtlanta, GA202165Republican15000Not Liberal??...........................Population?2025??????Population?2027??????Preceptor Table rows chronologically ordered. population shows greater population making assumptions — year start earlier data year end later Preceptor Table. shows expansive population inferring .",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "age-party",
    "chapter": "8 Three Parameters",
    "heading": "8.2 age ~ party",
    "text": "want build model use model make claims world. questions relationship age party following:expected age Democrat train station?group three Democrats three Republicans, age difference oldest Democrat youngest Republican?can answer similar questions creating model uses party affiliation predict age",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "justice-3",
    "chapter": "8 Three Parameters",
    "heading": "8.2.1 Justice",
    "text": "\nFIGURE 8.1: Justice\nJustice consists four topics: validity, stability, representativeness, data generating mechanism (DGM).understand validity regards Population Table, must first recognize inherent flaw experiment design: two units receive exactly treatment. doesn’t ring true, consider Spanish speaking train experiment. units Spanish-speaking platform received treatment, right? , actually! Every unit heard Spanish different decibel level. Every unit also heard Spanish different amount time, depending arrived station. Thus, two units can treatment — , hearing Spanish platform — different versions treatment. crucial define one’s estimand precisely: interested difference potential outcomes Spanish spoken 10 minutes 60 dB versus control, can ignore possible columns Population Table.must consider stability model. Stability means relationship columns three categories rows: data, Preceptor table, larger population drawn. something like height, much easier assume stability greater period time. Changes global height occur extremely slowly, height stable across span 20 years reasonable assume. something like political ideology, much harder make assertion data collected 2010 stable data collected 2025. confronted uncertainty, can consider making timeframe smaller. However, still need assume stability 2014 (time data collection) today. Stability allows us ignore issue time.Next, let’s consider representativeness. Representativeness well sample represents larger population interested generalizing . train experiment allow us calculate causal effect people commute cars? Can calculate causal effect people New York City? generalize broader populations consider experimental estimates applicable beyond experiment. Generally: chance certain type person experiment, make assumption person.Now let’s discuss kind DGM using. purposes, choosing linear logistic, however, several different types distributions dealing moment. Let’s recall difference linear logistic models, dependent upon outcome variable. outcome variable continuous, use linear model, outcome variable binary, two options, use logistic model. case, outcome variable age continous, linear model.can also consider type DGM. DGM age dependent variable predictive, causal, simple reason nothing, time, can change age. X years old. matter changed party registration Democrat Republican vice versa. age age. dealing non-causal DGM, focus predicting things. underlying mechanism connects age party less important brute statistical fact connection. Predictive models care little causality.good way looking Preceptor Table, seen . Unlike previous table Chapter 7, now two columns addition ID. Since data include Republicans Democrats world, every row filled . now know working predictive DGM. Recall:\\[\\text{outcome} = \\text{model} + \\text{model}\\]words, event depends explicitly described model well influences unknown us. Everything happens world result various factors, can consider model (know influences, data ).Let’s make DGM. mathematics:\\[ y_i = \\beta_1 republican_i + \\beta_2 democrat_i + \\epsilon_i\\]\n\\[republican_i, democrat_i \\\\{0,1\\}\\]\n\\[republican_i +  democrat_i = 1\\]\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]Don’t panic dear poets philosophers, whole thing easier looks. follow form outcome result model model.left-hand side outcome, \\(y_i\\), variable explained. case, age individual population.right-hand side contains two parts, contained within model, isn’t.First, part contained model, consists parameter data points. betas two parameters: \\(\\beta_1\\) average age Republicans population \\(\\beta_2\\) average age Democrats population. \\(republican_i\\) \\(democrat_i\\) explanatory variables take values 1 0. shown model, \\(\\beta_1 republican_i\\) \\(\\beta_2 democrat_i\\) two similar terms added together make model term consists parameter data point. person \\(\\) Republican \\(republican_i = 1\\) \\(democrat_i = 0\\). person \\(\\) Democrat \\(republican_i = 0\\) \\(democrat_i = 1\\). words, values mutually exclusive – Democrat, also Republican.second part right-hand side, \\(\\epsilon_i\\) (“epsilon”), represents unexplained part outcome called error term. includes factors influence someone’s age related party affiliation. words, \\(\\epsilon_i\\) influence age factored model. assume error follows normal distribution expected value 0 (meaning 0 average) simply difference outcome model predictions.things note model:small \\(\\)’s index number observations. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modeled factors person \\(\\).small \\(\\)’s index number observations. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained modeled non-modeled factors person \\(\\).model claim world works, just 115 individuals data people population seek draw inferences.model claim world works, just 115 individuals data people population seek draw inferences.Although terminology differs across academic fields, common term describe model like “regression.” “regressing” age party order see associated . formula “regression formula”, model “regression model.” terminology also apply model height Chapter 7.Although terminology differs across academic fields, common term describe model like “regression.” “regressing” age party order see associated . formula “regression formula”, model “regression model.” terminology also apply model height Chapter 7.model Chapter 7 sometimes called “intercept-” (interesting) parameter intercept. “two intercept” model , instead estimating average whole population, estimating two averages.model Chapter 7 sometimes called “intercept-” (interesting) parameter intercept. “two intercept” model , instead estimating average whole population, estimating two averages.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "courage-3",
    "chapter": "8 Three Parameters",
    "heading": "8.2.2 Courage",
    "text": "\nFIGURE 8.2: Courage\nCourage allows us translate math code.get posterior distributions three parameters, use stan_glm(), just Chapter 7.variable tilde, age, outcome.variable tilde, age, outcome.explanatory variable party. variable two values, ‘Democrat’ ‘Republican’.explanatory variable party. variable two values, ‘Democrat’ ‘Republican’.Recall model linear. Since using linear model, family use gaussian.Recall model linear. Since using linear model, family use gaussian.also added -1 end equation, indicating want intercept, otherwise added default.also added -1 end equation, indicating want intercept, otherwise added default.resulting output:partyDemocrat corresponds \\(\\beta_1\\), average age Democrats population. partyRepublican corresponds \\(\\beta_2\\), average age Republicans population. Since don’t really care posterior distribution \\(\\sigma\\), won’t discuss . Graphically:unknown parameters \\(\\beta_1\\) (partyDemocrat) \\(\\beta_2\\) (partyRepublican) still unknown. can never know true average age Democrats population. can calculate posterior probability distribution parameter. Comments:Democrats seem slightly older Republicans. true sample , almost (quite!) definition, true posterior probability distributions.Democrats seem slightly older Republicans. true sample , almost (quite!) definition, true posterior probability distributions.estimate average age Democrats population much precise Republicans five times many Democrats Republicans sample. central lesson Chapter 6 data related parameter, narrower posterior distribution .estimate average age Democrats population much precise Republicans five times many Democrats Republicans sample. central lesson Chapter 6 data related parameter, narrower posterior distribution .great deal overlap two distributions. surprised , truth, average age Republicans population greater Democrats? really. don’t enough data sure either way.great deal overlap two distributions. surprised , truth, average age Republicans population greater Democrats? really. don’t enough data sure either way.phrase “population” great deal work said , precisely, mean “population.” set people commuter platforms days 2012 experiment done? set people platforms, including ones never visited? set Boston commuter? Massachusetts residents? US residents? include people today, can draw inferences 2012? explore questions every model create.phrase “population” great deal work said , precisely, mean “population.” set people commuter platforms days 2012 experiment done? set people platforms, including ones never visited? set Boston commuter? Massachusetts residents? US residents? include people today, can draw inferences 2012? explore questions every model create.parameters \\(\\beta_1\\) \\(\\beta_2\\) can interpreted two ways. First, like parameters, part model. need estimate . , many cases, don’t really care value parameter . exact value \\(\\sigma\\), example, really matter. Second, parameters substantive interpretaion, \\(\\beta_1\\) \\(\\beta_2\\) average ages population. often case! Fortunately, models, can use functions like posterior_epred() posterior_predict() answer questions.parameters \\(\\beta_1\\) \\(\\beta_2\\) can interpreted two ways. First, like parameters, part model. need estimate . , many cases, don’t really care value parameter . exact value \\(\\sigma\\), example, really matter. Second, parameters substantive interpretaion, \\(\\beta_1\\) \\(\\beta_2\\) average ages population. often case! Fortunately, models, can use functions like posterior_epred() posterior_predict() answer questions.Consider table shows sample 8 individuals. fitted values Republicans Democrats, model produces one fitted value condition. table shows just sample 8 individuals captures wide range residuals, making difficult predict age new individual. can get better picture unmodeled variation sample plot three variables individuals data.following three histograms show actual outcomes, fitted values, residuals people trains:three plots structured like equation table . value left plot sum one value middle plot plus one right plot.actual age distribution looks like normal distribution. centered around 43, standard deviation 12 years.actual age distribution looks like normal distribution. centered around 43, standard deviation 12 years.middle plot fitted values shows two adjacent spikes, represent estimates Democrats Republicans.middle plot fitted values shows two adjacent spikes, represent estimates Democrats Republicans.Since residuals plot represents difference two plots, distribution looks like first plot.Since residuals plot represents difference two plots, distribution looks like first plot.",
    "code": "\nfit_1 <- stan_glm(age ~ party - 1, \n                    data = trains, \n                    family = gaussian,\n                    seed = 17,\n                    refresh = 0)\nfit_1## stan_glm\n##  family:       gaussian [identity]\n##  formula:      age ~ party - 1\n##  observations: 115\n##  predictors:   2\n## ------\n##                 Median MAD_SD\n## partyDemocrat   42.6    1.2  \n## partyRepublican 41.2    2.7  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 12.3    0.8  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nfit_1 |> \n  as_tibble() |> \n  select(-sigma) |> \n  mutate(Democrat = partyDemocrat, Republican = partyRepublican) |>\n  pivot_longer(cols = Democrat:Republican,\n               names_to = \"parameter\",\n               values_to = \"age\") |> \n  ggplot(aes(x = age, fill = parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Average Age\",\n         subtitle = \"More data allows for a more precise posterior for Democrats\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\noutcome <- ch8 |> \n  ggplot(aes(age)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Age\",\n         y = \"Count\") \n\nfitted <- tibble(age = fitted(fit_1)) |> \n  ggplot(aes(age)) +\n    geom_bar() +\n    labs(x = \"Fitted Values\",\n         y = NULL) +\n    scale_x_continuous(limits = c(20, 70)) \n\nres <- tibble(resids = residuals(fit_1)) |> \n  ggplot(aes(resids)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Residuals\",\n         y = NULL) \n  \n\noutcome + fitted + res +\n  plot_annotation(title = \"Decomposition of Height into Fitted Values and Residuals\")"
  },
  {
    "path": "three-parameters.html",
    "id": "temperance-3",
    "chapter": "8 Three Parameters",
    "heading": "8.2.3 Temperance",
    "text": "\nFIGURE 8.3: Temperance\nRecall first questions began section:probability , Democrat shows train station, 50 years old?far tried model people data set whose real age already knew. helpful understand model, ultimate goal understand real world, people don’t yet know much . Temperance guides us make meaningful predictions become aware known unknown limitations.Start simple question, chances random Democrat 50 years old? First, create tibble desired input model. case tibble variable named “party” contains single observation value “Democrat”. bit different Chapter 7.Use posterior_predict() create draws posterior scenario. Note new posterior distribution consideration . unknown parameter, call \\(D_{age}\\), age Democrat. age randomly selected Democrat population next Democrat meet next Democrat interview train platform. definition “population” determines appropriate interpretation. Yet, regardless, \\(D_{age}\\) unknown parameter. one — like \\(\\beta_1\\), \\(\\beta_2\\), \\(\\sigma\\) — already created posterior probability distribution. need posterior_predict().posterior_predict() takes two arguments: model simulations run, tibble indicating many parameters want run simulations. case, model one Courage tibble one just created.might expect can use as_tibble() directly object returned posterior_predict(). Sadly, obscure technical reasons, won’t quite work. , need incantation mutate_all(.numeric) make sure resulting tibble well-behaved. command ensures every column tibble simple numeric vector, want.result draws posterior distribution age Democrat. important understand concrete person trains dataset - algorithm posterior_predict() simply uses existing data trains estimate posterior distribution.posterior distribution, can answer (almost) reasonable question. case, probability next Democrat 50 around 28%.Recall second question:group three Democrats three Republicans, age difference oldest Democrat youngest Republican?start creating tibble desired input. Note name column (“party”) observations (“Democrat”, “Republican”) must always exactly original data set. tibble well model can used arguments posterior_predict():6 columns: one person. posterior_predict() name columns, arranged order specified persons newobs: D, D, D, R, R, R. determine expected age difference, add code works posterior draws:plotting code similar seen :words, expect oldest Democrat 22 years older youngest Republican, surprised oldest Democrat actually younger youngest Republican group 6.",
    "code": "\nnew_obs <- tibble(party = \"Democrat\")\npp <- posterior_predict(fit_1, newdata = new_obs) |>\n    as_tibble() |>\n    mutate_all(as.numeric)\n\nhead(pp, 10)## # A tibble: 10 × 1\n##      `1`\n##    <dbl>\n##  1  53.3\n##  2  40.6\n##  3  37.0\n##  4  39.6\n##  5  47.0\n##  6  43.8\n##  7  53.1\n##  8  49.6\n##  9  51.9\n## 10  44.0\npp |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for a Random Democrat's Age\",\n         subtitle = \"Individual predictions are always more variable than expected values\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nsum(pp$`1` > 50) / nrow(pp)## [1] 0.27\nnewobs <- tibble(party = c(\"Democrat\", \"Democrat\", \"Democrat\", \n                        \"Republican\", \"Republican\",\"Republican\"))\n\nposterior_predict(fit_1, newdata = newobs) |>\n    as_tibble() |>\n    mutate_all(as.numeric)## # A tibble: 4,000 × 6\n##      `1`   `2`   `3`   `4`   `5`   `6`\n##    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1  41.9  58.1  60.5  42.9  40.6  49.7\n##  2  66.0  53.5  27.3  43.0  39.8  33.9\n##  3  13.3  41.1  35.1  26.5  42.4  27.6\n##  4  54.5  42.4  21.4  38.7  63.1  35.4\n##  5  13.9  17.5  37.0  44.9  41.3  55.6\n##  6  49.0  37.9  48.6  32.2  32.8  45.1\n##  7  51.3  48.6  36.3  20.8  53.0  26.6\n##  8  27.6  27.9  35.0  43.3  49.9  41.1\n##  9  38.4  26.1  33.8  41.5  58.5  28.9\n## 10  57.4  36.2  42.5  52.2  51.2  40.6\n## # … with 3,990 more rows\npp <- posterior_predict(fit_1, newdata = newobs) |>\n    as_tibble() |>\n    mutate_all(as.numeric) |> \n  \n    # We don't need to rename the columns, but doing so makes the subsequest\n    # code much easier to understand. We could just have worked with columns 1,\n    # 2, 3 and so on. Either way, the key is to ensure that you correctly map\n    # the covariates in newobs to the columns in the posterior_predict object.\n  \n    set_names(c(\"dem_1\", \"dem_2\", \"dem_3\", \n                \"rep_1\", \"rep_2\", \"rep_3\")) |> \n    rowwise() |> \n  \n  # Creating three new columns. The first two are the highest age among\n  # Democrats and the lowest age among Republicans, respectively. The third one\n  # is the difference between the first two.\n  \n  mutate(dems_oldest = max(c_across(dem_1:dem_3)),\n         reps_youngest = min(c_across(rep_1:rep_3)),\n         age_diff = dems_oldest - reps_youngest)\n\npp## # A tibble: 4,000 × 9\n## # Rowwise: \n##    dem_1 dem_2 dem_3 rep_1 rep_2 rep_3 dems_oldest reps_youngest age_diff\n##    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>       <dbl>         <dbl>    <dbl>\n##  1  81.0  30.7  40.6  61.6  23.1  30.6        81.0          23.1     57.8\n##  2  59.8  22.1  55.5  18.8  45.5  56.8        59.8          18.8     41.0\n##  3  29.6  38.1  45.7  42.4  23.2  42.4        45.7          23.2     22.5\n##  4  58.8  57.4  72.0  34.7  15.2  23.6        72.0          15.2     56.8\n##  5  25.8  34.0  30.2  52.5  44.1  47.1        34.0          44.1    -10.0\n##  6  40.6  35.7  41.2  56.1  28.3  28.2        41.2          28.2     13.0\n##  7  34.9  51.6  28.1  37.5  38.8  30.2        51.6          30.2     21.4\n##  8  47.9  45.3  46.9  30.5  40.8  39.4        47.9          30.5     17.4\n##  9  76.8  42.5  55.1  27.2  53.8  40.7        76.8          27.2     49.6\n## 10  24.5  35.4  64.9  37.2  33.5  58.2        64.9          33.5     31.4\n## # … with 3,990 more rows\npp |>  \n  ggplot(aes(x = age_diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Age Difference\",\n         subtitle = \"Oldest of three Democrats compared to youngest of three Republicans\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"
  },
  {
    "path": "three-parameters.html",
    "id": "addendum",
    "chapter": "8 Three Parameters",
    "heading": "8.2.4 Addendum",
    "text": "Instead parameterizing model without intercept, used one. case, math :\\[ y_i = \\beta_0  + \\beta_1 democratic_i + \\epsilon_i\\]\ninterpretations parameters different prior model. \\(\\beta_0\\) now average age Republicans. interpretation \\(\\beta_1\\) original set . \\(\\beta_1\\) now difference average age Republicans Democrats.fit model, use exact code , except without -1 formula argument.intercept, 42.6, partyDemocrat estimate first model. partyRepublican estimate, previously 41.0, now -1.5, meaning difference (allowing rounding) average age Democrats Republicans.Little else models different. fitted values residuals. posterior_predict() generate posterior predictive probabilty distributions. parameterization use matter much. able interpret meaning coefficients .",
    "code": "\nstan_glm(age ~ party, \n         data = trains, \n         seed = 98,\n         refresh = 0)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      age ~ party\n##  observations: 115\n##  predictors:   2\n## ------\n##                 Median MAD_SD\n## (Intercept)     42.6    1.3  \n## partyRepublican -1.5    3.1  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 12.3    0.8  \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "path": "three-parameters.html",
    "id": "att_end-treatment",
    "chapter": "8 Three Parameters",
    "heading": "8.3 att_end ~ treatment",
    "text": ", created predictive model: someone’s party affiliation, can make better guess age absence information party. nothing causal model. Changing someone’s party registration can change age. example, build causal model. Consider two questions:average treatment effect, exposing people Spanish-speakers, attitudes toward immigration?largest causal effect still 1 10 chance occurring?Models help us answer questions better without models, follow Cardinal Virtues.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "justice-4",
    "chapter": "8 Three Parameters",
    "heading": "8.3.1 Justice",
    "text": "\nFIGURE 8.4: Justice\nfour elements Justice data science project remain : validity, stability, representativeness, model.First, must consider validity model, , let’s look treatment variable. mean somebody receive treatment? Let’s say somebody running late way train morning. Well, person may heard Spanish-speakers shorter amount time commuter arrived earlier. Similarly, Spanish-speakers may speaking louder others commuters may closer speakers commuters. Therefore, commuters hear treatment exposed Spanish-speakers less loudly commuters. Additionally, Spanish-speakers weren’t every train station 2012 data get data now, wouldn’t able hire exact Spanish-speakers. considering validity reference treatments, must determine can assume treatments exposed Spanish-speakers commuter may experience bit differently can assumed .Next, must consider stability model relationship att_end treatment 2012 2021. relationship 2012, four years Donald Trump’s election president, still ? might know sure, consider order continue make assumptions data. purposes, consider relationship stable. Even though know may changes, consider model years.Let’s now consider whether data 2012 train commuters around Boston representative 2012 train commuters US. last model, discussed issue Boston may different cities therefore representative US, now consider issue random sampling may lead representativeness issues.Let’s say even though Boston different US cities, considered Boston perfectly representative US. Great, 2012 data still representative. bias within chosen give survey, commuters approached receive surveys may random representative. individuals giving surveys younger also tended choose people approach survey similar age ? scenario like end overestimating younger train commuters population, influence answers questions. Specifically, considering relationship att_end treatment, influence results model younger individuals may similar attitudes immigration.Now, let’s determine whether DGM linear logistic. Since outcome variable att_end continuous variable since range possible values, use linear model.math model exactly math predictive model first part chapter, although change notation bit clarity.\\[ y_i = \\beta_1 treatment_i + \\beta_2 control_i + \\epsilon_i\\]\n\\[treatment_i, control_i \\\\{0,1\\}\\]\n\\[treatment_i +  control_i = 1\\]\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]Nothing changed, except meaning data items interpretations parameters.left-hand side still outcome, \\(y_i\\), however case, person’s attitude toward immigration experiment complete. \\(y_i\\) takes integer values 3 15 inclusive.right-hand side, part contained model consist terms \\(\\beta_1 treatment_i\\) \\(\\beta_2 control_i\\). two terms stand Treated Control , term consists parameter data point. \\(\\beta_1\\) average attitude toward immigration treated individuals — exposed Spanish-speakers — population. \\(\\beta_2\\) average attitude toward immigration control individuals — exposed Spanish-speakers — population.parameters. \\(x\\)’s explanatory variables take values 1 0. person \\(\\) Treated, \\(treatment_i = 1\\) \\(control_i = 0\\). person \\(\\) Control, \\(treatment_i = 0\\) \\(control_i = 1\\). words, binary variables mutually exclusive – Treated, also Control.last part, \\(\\epsilon_i\\) (“epsilon”), represents part explained model called error term. simply difference outcome model predictions. particular case, includes factors influence someone’s attitude toward immigration explained treatment status. assume error follows normal distribution expected value 0.Note formula applies everyone population, just 115 people data. index \\(\\) just go 1 115. goes 1 \\(N\\), \\(N\\) number individuals population. Conceptually, everyone att_end treatment control.small \\(\\)’s index data set. equivalent “ID” column Preceptor Table simply states outcome person \\(\\) explained predictor variables (\\(treatment\\) \\(control\\)) person \\(\\), along error term.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "courage-4",
    "chapter": "8 Three Parameters",
    "heading": "8.3.2 Courage",
    "text": "\nFIGURE 8.5: Courage\nJustice satisfied, gather Courage fit model. Note , except change variable names, code exactly , predictive model age. Predictive models causal models use math code. differences, important, lie interpretation results, creation.Note , since using linear model, set family argument “gaussian”.treatmentTreated corresponds \\(\\beta_1\\). always, R , behind scenes, estimated entire posterior probability distribution \\(\\beta_1\\). graph distribution next section. basic print method objects can’t show entire distribution, gives us summary numbers: median MAD SD. Speaking roughly, expect 95% values posterior within two MAD SD’s median. words, 95% confident true, unknowable, average attitude toward immigration among Treated population 9.2 10.8.treatmentControl corresponds \\(\\beta_2\\). analysis applies. 95% confident true value average attitude toward immigration Control population 7.9 9.1.now, used Bayesian interpretation “confidence interval.” also intuitive meaning , outside academia, almost universal. truth . don’t know, sometimes can’t know, truth. confidence interval, associated confidence level, tells us likely truth lie within specific range. boss asks confidence interval, almost certainly using interpretation., contemporary academic research, phrase “confidence interval” usually given “Frequentist” interpretation. (biggest divide statistics Bayesians Frequentist interpretations. Frequentist approach, also known “Classical” statistics, dominant 100 years. power fading, textbook uses Bayesian approach.) Frequentist, 95% confidence interval means , apply procedure used infinite number future situations like , expect true value fall within calculated confidence intervals 95% time. academia, distinction sometimes made confidence intervals (use Frequentist interpretation) credible intervals (use Bayesian interpretation). won’t worry difference Primer.Let’s look full posteriors \\(\\beta_1\\) \\(\\beta_2\\).appears affect treatment change people’s attitudes conservative immigration issues. somewhat surprising!can decompose dependent variable, att_end two parts: fitted values residuals. two possible fitted values, one Treated one Control. residuals, always, simply difference outcomes fitted values.smaller spread residuals, better job model explaining outcomes.",
    "code": "\nfit_2 <- stan_glm(att_end ~ treatment - 1, \n                      data = trains, \n                      seed = 45,\n                      refresh = 0)\n\nfit_2## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ treatment - 1\n##  observations: 115\n##  predictors:   2\n## ------\n##                  Median MAD_SD\n## treatmentTreated 10.0    0.4  \n## treatmentControl  8.5    0.3  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 2.8    0.2   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nfit_2 |> \n  as_tibble() |> \n  select(-sigma) |> \n  pivot_longer(cols = treatmentTreated:treatmentControl,\n               names_to = \"Parameter\",\n               values_to = \"attitude\") |> \n  ggplot(aes(x = attitude, fill = Parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Attitude Toward Immigration\",\n         subtitle = \"Treated individuals are more conservative\",\n         x = \"Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) + \n    theme_classic()"
  },
  {
    "path": "three-parameters.html",
    "id": "temperance-4",
    "chapter": "8 Three Parameters",
    "heading": "8.3.3 Temperance",
    "text": "\nFIGURE 8.6: Temperance\nRecall first question began section:average treatment effect, exposing people Spanish-speakers, attitudes toward immigration?Chapter 4 defined average treatment effect. One simple estimator average treatment effect difference \\(\\beta_1\\) \\(\\beta_2\\). , definition \\(\\beta_1\\) average attitude toward immigration, population, anyone, exposure treatment. , \\(\\beta_1 - \\beta_2\\) average treatment effect population, roughly 1.5. However, estimating posterior probability distribution parameter tricky, unless make use posterior distributions \\(\\beta_1\\) \\(\\beta_2\\). information, problem simple:true value average treatment effect much 2 little 1? course! likely value around 1.5, variation data smallness sample cause estimate imprecise. However, quite unlikely true average treatment effect zero.can use posterior_epred() answer question. Create tibble use done :posterior probability distribution created posterior_epred() one produced manipulating parameters directly.second question:largest effect size still 1 10 chance occurring?Create tibble can pass posterior_predict(). variables tibble passed newdata. Fortunately, tibble created just need question also.Consider result posterior_predict() two people, one treated one control. Take difference.Create graphic:case, looking distribution treatment effect single individual. different average treatment effect. particular, much variable. looking one row Preceptor Table. single individual, att_end can anywhere 3 15, treatment control. causal effect — difference two potential outcomes can, theory, anywhere -12 +12. extreme values rare, impossible.question, however, interested value 90th percentile.expect treatment effect magnitude common, , time, effects big bigger occur 10% time.",
    "code": "\nfit_2 |> \n  as_tibble() |> \n  mutate(ate = treatmentTreated - treatmentControl) |> \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nnewobs <- tibble(treatment = c(\"Treated\", \"Control\"))\n\npe <- posterior_epred(fit_2, newobs) |> \n    as_tibble() |> \n    mutate(ate = `1` - `2`)\n\npe## # A tibble: 4,000 × 3\n##      `1`   `2`   ate\n##    <dbl> <dbl> <dbl>\n##  1 10.0   8.56 1.47 \n##  2  9.94  8.33 1.61 \n##  3 10.0   8.48 1.56 \n##  4 10.6   8.28 2.36 \n##  5 10.6   8.29 2.32 \n##  6  9.26  8.81 0.449\n##  7 10.4   8.42 1.96 \n##  8 10.5   8.28 2.26 \n##  9 10.8   8.51 2.25 \n## 10 10.3   8.37 1.88 \n## # … with 3,990 more rows\npe |> \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\npp <- posterior_predict(fit_2, \n                        newdata = newobs) |>\n    as_tibble() |>\n    mutate_all(as.numeric) |> \n    mutate(te = `1` - `2`)\n  \npp## # A tibble: 4,000 × 3\n##      `1`   `2`       te\n##    <dbl> <dbl>    <dbl>\n##  1  9.80  4.31  5.49   \n##  2  7.34  6.84  0.505  \n##  3 12.0   7.13  4.89   \n##  4 14.7   9.67  5.04   \n##  5 13.5   7.54  5.91   \n##  6  9.65 14.4  -4.77   \n##  7  9.28  6.42  2.86   \n##  8 12.7   5.88  6.82   \n##  9 10.4  10.4  -0.00962\n## 10  9.68  9.59  0.0967 \n## # … with 3,990 more rows\npp |> \n  ggplot(aes(x = te)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Treatment Effect for One Person\",\n         subtitle = \"Causal effects are more variable for indvduals\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nquantile(pp$te, prob = 0.9)## 90% \n## 6.5"
  },
  {
    "path": "three-parameters.html",
    "id": "income-age",
    "chapter": "8 Three Parameters",
    "heading": "8.4 income ~ age",
    "text": "far, created models predictor variable discrete, two possible values. party either “Democrat” “Republican”. treatment either “Treated” “Control”. Often times, however, predictor variable continuous. Fortunately, exact approach works case. Consider:expect income 40-year old?",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "justice-5",
    "chapter": "8 Three Parameters",
    "heading": "8.4.1 Justice",
    "text": ", Justice, must consider validity, stability, representativeness, data generating mechanism.Let’s look validity age income. Recall validity refers whether can consider columns age income meaning data set 2012 Boston train commuters Preceptor Table. age doesn’t really change meaning time, income can impacted inflation. , $100,000 2012 doesn’t worth $100,000 now due inflation now less purchasing power. result income underestimated within model. However, since hasn’t drastic inflation dramatically changed buying power, consider income valid. like 300% inflation, however, conclusion probably different.Now, let’s consider stability model believe relationship age income changed 2012 now. , let’s consider inflation impact income. incomes increase rate inflation, income distribution different 2012. However, wages don’t tend change quickly inflation , likely change significantly can consider model stable.Next, let’s consider another issue may representativeness. now assume Boston train commuters perfectly representative US train commuters approached respond survey also random? Even true, still assume representativeness actually complete submit survey random. Instead young people chosen respond handing surveys like discussed last model, assume surveys handed randomly, younger people tended fill submit older? Well, still skew age distribution overestimate younger people population, younger people also tend lower income older people, also alter answers current questions.reason believe true, one way fix issue representativeness alter population train commuters US respond survey. , population accommodate skewed age distribution assumption younger individuals tend respond surveys higher rates older people.\nFIGURE 8.7: Justice\nmathematics continuous predictor unchanged intercept-including example explored Section 8.2.4:\\[y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]comparing two people (persons 1 2), first one year older second, \\(\\beta_1\\) expected difference incomes. algebra simple. Start two individuals.\\[y_1 = \\beta_0  + \\beta_1 age_1\\]\n\\[y_2 = \\beta_0  + \\beta_1 age_2\\]\nwant difference , subtract second first, performing subtraction sides equals sign.\\[y_1 - y_2 = \\beta_0  + \\beta_1 age_1 - \\beta_0 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 age_1 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 (age_1 - age_2)\\], person 1 one year older person 2, :\\[y_1 - y_2 = \\beta_1 (age_1 - age_2)\\\\\ny_1 - y_2 = \\beta_1 (1)\\\\\ny_1 - y_2 = \\beta_1\\]algebra demonstrates \\(\\beta_1\\) ages. difference expected income two people aged 23 24 difference two people aged 80 81. plausible? Maybe. algebra lie. create model like , assumption making.Note careful imply increasing age one year “causes” increase income. nonsense! causation without manipulation. Since impossible change someone’s age, one potential outcome. one potential outcome, causal effect defined.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "courage-5",
    "chapter": "8 Three Parameters",
    "heading": "8.4.2 Courage",
    "text": "\nFIGURE 8.8: Courage\nuse stan_glm() usual.comparing two individuals, one 30 years old one 40, expect older earn $9,000 . far certain: 95% confidence interval ranges -$3,000 $20,000.good summary models.brief! one wants listen much prattle. One sentence gives number interest. second sentence provides confidence interval.brief! one wants listen much prattle. One sentence gives number interest. second sentence provides confidence interval.rounds appropriately. one wants hear bunch decimals. Use sensible units.rounds appropriately. one wants hear bunch decimals. Use sensible units.just blindly repeat numbers printed display. one year difference age, associated $900 difference income, awkward. (think.) decade comparison sensible.just blindly repeat numbers printed display. one year difference age, associated $900 difference income, awkward. (think.) decade comparison sensible.“comparing” great phrase start summary non-causal model. Avoid language like “associated ” “leads ” “implies” anything even hints causal claim.“comparing” great phrase start summary non-causal model. Avoid language like “associated ” “leads ” “implies” anything even hints causal claim.Consider usual decomposition outcome two parts: model error term.scores different fitted values. Indeed, greater number different fitted values different outcome values! often true models continuous predictor variables, age.",
    "code": "\nfit_3 <- stan_glm(income ~ age, \n                  data = trains, \n                  seed = 28,\n                  refresh = 0)\n\nprint(fit_3, details = FALSE)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      income ~ age\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median   MAD_SD  \n## (Intercept) 103288.5  24077.6\n## age            907.8    535.8\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74121.3  4897.0\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "path": "three-parameters.html",
    "id": "temperance-5",
    "chapter": "8 Three Parameters",
    "heading": "8.4.3 Temperance",
    "text": "\nFIGURE 8.9: Temperance\nRecall question:expect income random 40 year old?Given looking expected value, use posterior_epred().Plotting always.",
    "code": "\nnewobs <- tibble(age = 40)\n\npe <- posterior_epred(fit_3, newdata = newobs) |> \n  as_tibble() \n\npe## # A tibble: 4,000 × 1\n##        `1`\n##      <dbl>\n##  1 142106.\n##  2 141530.\n##  3 141363.\n##  4 139289.\n##  5 139647.\n##  6 142876.\n##  7 139427.\n##  8 143336.\n##  9 145881.\n## 10 143052.\n## # … with 3,990 more rows\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Expected Income\",\n         subtitle = \"A 40-years old commuter earns around $140,000\",\n         x = \"Income\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"
  },
  {
    "path": "three-parameters.html",
    "id": "liberal-income",
    "chapter": "8 Three Parameters",
    "heading": "8.5 liberal ~ income",
    "text": "far chapter, considered continuous outcome variables. age, att_end income take variety values. None , truly, continuous, course. age reported integer value. att_end can , definition, take 13 distinct values. However, modeling perspective, matters 2 possible values.liberal, however, takes two values: TRUE FALSE. order model , must use binomial family. begin, always, questions:Among people income $100,000, proportion liberal?Assume group eight people, two make $100,000, two $200,000, two $300,000 two $400,000. many liberal?",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "justice-6",
    "chapter": "8 Three Parameters",
    "heading": "8.5.1 Justice",
    "text": "Let’s now consider Justice relationship liberal income.First, let’s look validity, especially pertaining liberal. know, column values “Liberal” “Liberal” convey political ideology train commuters. must determine column liberal therefore meaning liberal Boston 2012 Boston 2021. can determine , can assume validity, case, core beliefs liberal changed much 2012 2021, can determine data valid.Now, let’s consider stability. , must look relationship liberal income determine whether believe relationship changed 2012 2021. knowledge world, reason believe changed? 2012 2021, income increased liberal? Well, model relationship income liberal changed years, therefore model. However, since knowledge world reason believe something sorts happened affect relationship, can consider model stable.past three models, considered 3 possible issues representativeness model, difference Boston cities, problems random sampling, bias respond, now consider survey surveys given treatment, people may filled one surveys. People filled one surveys affect representativeness data included data filled one survey tended liberal, affect data underestimate amount liberals survey. something must consider looking representativeness, since otherwise determine data train commuters Boston 2012 representative enough train commuters US now continue using data.Let’s consider whether model linear logistic. Unlike previous models chapter, outcome variable, liberal, model, two options, “Liberal” “Liberal”. Therefore, logistic 2 possible outcomes outcome variable isn’t continuous.Recall discussion Section 7.5 logistic regression model use whenever outcome dependent variable binary/logical. math , care math. don’t, least much. Reminder:\\[p(\\text{Liberal}_i = \\text{TRUE}) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}\\]model two parameters, \\(\\beta_0\\) \\(\\beta_1\\). parameters simple interpretations, unlike parameters linear (gaussian) model.Recall fundamental structure data science problems:\\[\\text{outcome} = \\text{model} + \\text{model}\\]\nexact mathematics model — parameters, interpretations — just dross foundry inferences: unavoidable worth much time.Even math ignorable, causal versus predictive nature model . causal model predictive model? depends! causal assume can manipulate someone’s income, , , least two potential outcomes: person \\(\\)’s liberal status makes X dollars person \\(\\)’s liberal status makes Y dollars. Remember: causation without manipulation. definition causal effect difference two potential outcomes. one outcome, model can causal.many circumstances, don’t really care model causal . might want forecast/predict/explain outcome variable. case, whether can interpret influence variable causal irrelevant use variable.",
    "code": ""
  },
  {
    "path": "three-parameters.html",
    "id": "courage-6",
    "chapter": "8 Three Parameters",
    "heading": "8.5.2 Courage",
    "text": "Fitting logistic model easy. use arguments usual, family = binomial added.fit model, can look printed summary. Note use digits argument display digits printout.Fitted models tell us posterior distributions parameters formula defines model estimated. assuming model true. , discussed Chapter 6, assumption always false! model never perfectly accurate representation reality. , perfect, posterior distributions created \\(\\beta_0\\), \\(\\beta_1\\), perfect well.working linear model, often interpret meaning parameters, already done first three sections chapter. interpretations much harder logistic models math much less convenient. , won’t even bother try understand meaning parameters. However, can note \\(\\beta_1\\) negative, suggesting people higher incomes less likely liberal.",
    "code": "\nfit_4 <- stan_glm(data = ch8,\n                  formula = liberal ~ income,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 365)\nprint(fit_4, digits = 6)## stan_glm\n##  family:       binomial [logit]\n##  formula:      liberal ~ income\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median   MAD_SD  \n## (Intercept)  5.6e-01  4.3e-01\n## income      -6.0e-06  3.0e-06\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "path": "three-parameters.html",
    "id": "temperance-6",
    "chapter": "8 Three Parameters",
    "heading": "8.5.3 Temperance",
    "text": "Among people income $100,000, proportion liberal?Although model now logistic, steps answering question like linear/guassian model.pe tibble single vector. vector 4,000 draws posterior distribution proportion people, among make $100,000, liberal. population proportion thing probability single individual.Assume group eight people, two make $100,000, two $200,000, two $300,000 two $400,000. many liberal?trying predict outcome small number units, use posterior_predict(). complex questions ask, care need devote making newobs tibble. use rowwise() c_across() tricks earlier chapter.Study pp tibble. Understand component parts. first column, example, 4,000 draws posterior distribution liberal status random person income $100,000. Note draws zeroes ones. different draws seen ! also makes sense. making prediction binary variable, variable two possible values: zero one. , (reasonable!) predictions zero one.second column thing first column. 4,000 draws posterior distribution liberal status random person income $100,000. Yet also different values. thing different things, way rnorm(10) rnorm(10) thing — 10 draws standard normal distribution — different things values vary.third fourth columns different first two columns. 4,000 draws posterior distribution liberal status random person income $200,000. later columns. can answer difficult questions putting together simple building blocks, set draws posterior distribution. Recall discussion Section 5.1.total column simply sum first eight columns. created building blocks 8 columns draws four different posterior distributions, can switch focus row. Consider row 2. vector 8 numbers: 1 1 1 0 0 1 0 0. can treat vector unit analysis. might happen 8 people. first three might liberal, fourth liberal . row just one example might happen, one draw posterior distribution possible outcomes groups eight people incomes.can simplify draw taking sum, anything else might answer question confronted. Posterior distributions flexible individual numbers. can, less, just use algebra work .Graphically :always, truth. , tomorrow, meet 8 new people, specified incomes, certain number liberal. ideal Preceptor Table, just look number. data science required. Alas, don’t know truth. bets can create posterior distribution unknown value, done . need translate posterior English — “likely number liberals 2 3, total low zero high 5 also plausible. 6 liberals really surprising. 7 8 almost impossible.”two posterior probability distributions perfect? ! central message virtue Temperance. must demonstrate humility use models. Recall distinction unknown true distribution estimated distribution. first posterior distribution create understood every detail process accurately model . still know true unknown number, posterior distribution number perfect. Yet, model never perfect. making sorts assumptions behind scenes. assumptions plausible. Others less . Either way, estimated distribution graphed .central lesson Temperance : Don’t confuse estimated posterior () true posterior (want). Recognize unavoidable imperfections process. can still use estimated posterior — choice ? — cautious humble . suspect estimated posterior differs true posterior, humble cautious .",
    "code": "\nnewobs <- tibble(income = 100000)\n\npe <- posterior_epred(fit_4, \n                      newdata = newobs) |> \n  as_tibble()\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Proportion Liberal Among $100,000 Earners\",\n         subtitle = \"The population proportion is the same as the probability for any individual\",\n         x = \"Income\",\n         y = \"Probability of Being Liberal\") +\n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\nnewobs <- tibble(income = c(rep(100000, 2),\n                            rep(200000, 2),\n                            rep(300000, 2),\n                            rep(400000, 2)))\n                 \n\npp <- posterior_predict(fit_4, \n                        newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) |> \n  rowwise() |> \n  mutate(total = sum(c_across()))\n\npp## # A tibble: 4,000 × 9\n## # Rowwise: \n##      `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8` total\n##    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n##  1     0     1     1     0     0     0     0     0     2\n##  2     0     0     0     1     0     0     1     0     2\n##  3     0     0     0     1     1     0     0     0     2\n##  4     1     1     0     0     1     0     0     0     3\n##  5     0     1     0     0     0     0     0     0     1\n##  6     0     0     0     0     0     0     0     1     1\n##  7     0     1     0     0     0     1     0     0     2\n##  8     0     1     1     0     0     0     0     0     2\n##  9     0     1     1     0     0     0     0     0     2\n## 10     0     0     0     0     1     0     1     0     2\n## # … with 3,990 more rows\npp |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Number of Liberals in Group with Varied Incomes\",\n         subtitle = \"Two is the most likely number, but values from 0 to 5 are plausible\",\n         x = \"Number of Liberals\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "path": "three-parameters.html",
    "id": "summary-9",
    "chapter": "8 Three Parameters",
    "heading": "8.6 Summary",
    "text": "chapter, explored relationships different variables trains data set. built three predictive models one causal model.Similar previous chapters, first task use Wisdom . judge relevant data questions ask. reasonable consider data (e.g., income age data Boston commuters 2012) drawn population data want (e.g., income age data today entire US)? Probably?Justice necessary decide best way represent models make. little math won’t kill . use Courage translate models code. goal understand, generate posterior distributions parameters, interpret meaning. Temperance leads us final stage, using models answer questions.Key commands:Create model stan_glm().Create model stan_glm().Use posterior_epred() estimate expected values. e epred stands expected.Use posterior_epred() estimate expected values. e epred stands expected.Use posterior_predict() make forecasts individuals. variable predictions always greater variability expectations predictions can’t pretend \\(\\epsilon_i\\) zero.Use posterior_predict() make forecasts individuals. variable predictions always greater variability expectations predictions can’t pretend \\(\\epsilon_i\\) zero.draws posterior distribution outcome variable — whether expectation prediction — can manipulate draws answer question.Remember:Always explore data.Always explore data.Predictive models care little causality.Predictive models care little causality.Predictive models causal models use math code.Predictive models causal models use math code.“comparing” great phrase start summary non-causal model.“comparing” great phrase start summary non-causal model.Don’t confuse estimated posterior () true posterior (want). humble cautious use posterior.Don’t confuse estimated posterior () true posterior (want). humble cautious use posterior.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "four-parameters",
    "chapter": "9 Four Parameters",
    "heading": "9 Four Parameters",
    "text": "haste make progress — get way process building, interpreting using models — given short shrift messy details model building evaluation. chapter fills lacunae. also introduce models four parameters, including parallel slopes models.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "transforming-variables",
    "chapter": "9 Four Parameters",
    "heading": "9.1 Transforming variables",
    "text": "often convenient transform predictor variable model makes sense.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "centering",
    "chapter": "9 Four Parameters",
    "heading": "9.1.1 Centering",
    "text": "Recall model income function age.\\[ y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]fit using trains data primer.data. also using new package, broom.mixed, allows us tidy regression data plotting.nothing wrong model. Yet interpretation \\(\\beta_0\\), intercept regression, awkward. represents average income people age zero. useless! people zero age data. , even , weird think people taking commuter train Boston filling survey forms.easy, however, transform age variable makes intercept meaningful. Consider new variable, c_age, age minus average age sample. Using centered version age change predictions residuals model, make intercept easier interpret.intercept, 141,923, expected income someone c_age = 0, .e., someone average age data, around 42.",
    "code": "\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nfit_1 <- stan_glm(formula = income ~ age, \n         data = trains, \n         refresh = 0,\n         seed = 9)\n\nprint(fit_1, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 103927.5  23785.9\n## age            887.1    555.9\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74153.4  4905.9\ntrains_2 <- trains |> \n  mutate(c_age = age - mean(age))\n\nfit_1_c <- stan_glm(formula = income ~ c_age, \n                    data = trains_2, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_c, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 141922.8   6829.9\n## c_age          915.0    559.2\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74234.5  4723.3"
  },
  {
    "path": "four-parameters.html",
    "id": "scaling",
    "chapter": "9 Four Parameters",
    "heading": "9.1.2 Scaling",
    "text": "Centering — changing variable via addition/subtraction — often makes intercept easier interpret. Scaling — changing variable via multiplication/division — often makes easier interpret coefficients. common scaling method divide variable standard deviation.s_age age scaled standard deviation. change one unit s_age change one standard deviation age, 12. interpretation \\(\\beta_1\\) now:comparing two people, one 1 standard deviation worth years older , expect older person earn 11,000 dollars ., scaled without centering, intercept now back (nonsensical) meaning expected income people age 0.",
    "code": "\ntrains_3 <- trains |> \n  mutate(s_age = age / sd(age))\n\nfit_1_s <- stan_glm(formula = income ~ s_age, \n                    data = trains_3, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_s, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 103621.9  25156.0\n## s_age        10975.5   7115.7\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74432.7  4950.9"
  },
  {
    "path": "four-parameters.html",
    "id": "z-scores",
    "chapter": "9 Four Parameters",
    "heading": "9.1.3 z-scores",
    "text": "common transformation applies centering scaling. base R function scale() subtracts mean divides standard deviation. variable transformed “z-score,” meaning variable mean zero standard deviation one. Using z-scores makes interpretation easier, especially seek compare importance different predictors.two parameters easy interpret transformation.expected income someone average age, 42 study, 142,000 dollars.comparing two individuals differ age one standard deviation, 12 years study, older person expected earn 11,000 dollars younger.Note , using z-scores, often phrase comparison terms “sigmas.” One person “one sigma” older another person means one standard deviation older. simple enough, get used , also confusing since already using word “sigma” mean \\(\\sigma\\), standard deviation \\(\\epsilon_i\\). Alas, language something deal rather control. hear word “sigma” applied concepts, even sentence. Determine meaning context.",
    "code": "\ntrains_4 <- trains |> \n  mutate(z_age = scale(age))\n\nfit_1_z <- stan_glm(formula = income ~ z_age, \n                    data = trains_4, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_z, detail = FALSE)##             Median   MAD_SD  \n## (Intercept) 141737.9   6877.0\n## z_age        11041.8   7035.7\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 74306.7  5154.8"
  },
  {
    "path": "four-parameters.html",
    "id": "taking-logs",
    "chapter": "9 Four Parameters",
    "heading": "9.1.4 Taking logs",
    "text": "often helpful take log predictor variables, especially cases distribution skewed. generally take log variables values strictly positive. log negative number defined. Consider number registered voters (rv13) polling stations kenya.experienced data scientists use log rv13 rather raw value. Comments:know “true” model. say model using raw value right wrong?know “true” model. say model using raw value right wrong?Check whether choice meaningfully affects answer question. Much time, won’t. , inferences often fairly “robust” small changes model. get answer rv13 log_rv13, one cares use.Check whether choice meaningfully affects answer question. Much time, won’t. , inferences often fairly “robust” small changes model. get answer rv13 log_rv13, one cares use.Follow conventions field. everyone X, probably X, unless good reason . reason, explain prominently.Follow conventions field. everyone X, probably X, unless good reason . reason, explain prominently.professionals, presented data distributed like rv13, take log. Professionals hate (irrationally?) outliers. transformation makes distribution look normal generally considered good idea.professionals, presented data distributed like rv13, take log. Professionals hate (irrationally?) outliers. transformation makes distribution look normal generally considered good idea.Many suggestions apply every aspect modeling process.",
    "code": "\nx <- kenya |> \n  filter(rv13 > 0)\n\nrv_p <- x |> \n  ggplot(aes(rv13)) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Registered Voters\",\n         y = NULL) \n\nlog_rv_p <- x |> \n  ggplot(aes(log(rv13))) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Log of Registered Voters\",\n         y = NULL) +\n    expand_limits(y = c(0, 175))\n\nrv_p + log_rv_p +\n  plot_annotation(title = 'Registered Votes In Kenya Communities',\n                  subtitle = \"Taking logs helps us deal with outliers\")"
  },
  {
    "path": "four-parameters.html",
    "id": "adding-transformed-terms",
    "chapter": "9 Four Parameters",
    "heading": "9.1.5 Adding transformed terms",
    "text": "Instead simply transforming variables, can add terms transformed versions variable. Consider relation height age nhanes. Let’s start dropping missing values.Fit plot simple linear model:good model, obviously.Adding quadratic term makes better. (Note need () creating squared term within formula argument.)Still, made use background knowledge creating variables. know people don’t get taller age 18 . Let’s create variables capture break.point take variables receive given. captains souls. transform variables needed.",
    "code": "\nno_na_nhanes <- nhanes |> \n  select(height, age) |> \n  drop_na() \nnhanes_1 <- stan_glm(height ~ age,\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 47)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_1)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Children are shorter, but a linear fit is poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\nnhanes_2 <- stan_glm(height ~ age + I(age^2),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 33)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_2)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Quadratic fit is much better, but still poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\nnhanes_3 <- stan_glm(height ~ I(ifelse(age > 18, 18, age)),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 23)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_3)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Domain knowledge makes for better models\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")"
  },
  {
    "path": "four-parameters.html",
    "id": "transforming-the-outcome-variable",
    "chapter": "9 Four Parameters",
    "heading": "9.1.6 Transforming the outcome variable",
    "text": "Transforming predictor variables uncontroversial. matter much. Change continuous predictor variables \\(z\\)-scores won’t go far wrong. keep original form, take care interpretations. ’s good.Transforming outcome variable much difficult question. Imagine seek create model explains rv13 kenya tibble. transform ?Maybe? right answers. model rv13 outcome variable different model log(rv13) outcome. two directly comparable.Maybe? right answers. model rv13 outcome variable different model log(rv13) outcome. two directly comparable.Much advice regard taking logs predictor variables applies well.Much advice regard taking logs predictor variables applies well.See Gelman, Hill, Vehtari (2020)",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "interpreting-coefficients",
    "chapter": "9 Four Parameters",
    "heading": "9.1.7 Interpreting coefficients",
    "text": "interpret coefficients, important know difference across unit within unit comparisons. compare across unit, meaning comparing Joe George, looking causal relationship. Within unit discussions, comparing Joe treatment versus Joe control, causal. means within unit interpretation possible causal models, studying one unit two conditions. talk two potential outcomes, discussing person unit two conditions.put terms Preceptor tables, within unit comparison looking one row data: data Joe control data Joe treatment. comparing one unit, (case) one person, two conditions. across unit comparison looking multiple rows data, focus differences across columns. looking differences without making causal claims. predicting, implying causation.magnitude coefficients linear models relatively easy understand. true logistic regressions. case, use Divide--Four rule: Take logistic regression coefficient (constant term) divide 4 get upper bound predictive difference corresponding unit difference variable. means , evaluating predictor helpful logistic regression , divide coefficient four.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "selecting-variables",
    "chapter": "9 Four Parameters",
    "heading": "9.2 Selecting variables",
    "text": "decide variables include model? one right answer question.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "general-guidelines-for-selecting-variables",
    "chapter": "9 Four Parameters",
    "heading": "9.2.1 General guidelines for selecting variables",
    "text": "deciding variables keep discard models, advice keep variable X following circumstances apply:variable large well-estimated coefficient. means, roughly, 95% confidence interval excludes zero. “Large” can defined context specific model. Speaking roughly, removing variable large coefficient meaningfully changes predictions model.variable large well-estimated coefficient. means, roughly, 95% confidence interval excludes zero. “Large” can defined context specific model. Speaking roughly, removing variable large coefficient meaningfully changes predictions model.Underlying theory/observation suggests X meaningfully connection outcome variable.Underlying theory/observation suggests X meaningfully connection outcome variable.variable small standard error relative size coefficient, general practice include model improve predictions. rule thumb keep variables estimated coefficient two standard errors away zero. variables won’t “matter” much model. , coefficients, although well-estimated, small enough removing variable model affect model’s predictions much.variable small standard error relative size coefficient, general practice include model improve predictions. rule thumb keep variables estimated coefficient two standard errors away zero. variables won’t “matter” much model. , coefficients, although well-estimated, small enough removing variable model affect model’s predictions much.standard error large relative coefficient, .e., magnitude coefficient two standard errors zero, find reason include model, probably remove variable model.standard error large relative coefficient, .e., magnitude coefficient two standard errors zero, find reason include model, probably remove variable model.exception rule variable relevant answering question . example, want know ending attitude toward immigration differs men women, need include gender model, even coefficient small closer zero two standard errors.exception rule variable relevant answering question . example, want know ending attitude toward immigration differs men women, need include gender model, even coefficient small closer zero two standard errors.standard field include X regressions.standard field include X regressions.boss/client/reviewer/supervisor wants include X.boss/client/reviewer/supervisor wants include X.Let’s use trains dataset evaluate helpful certain variables creating effective model, based guidelines .",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "variables-in-the-trains-dataset",
    "chapter": "9 Four Parameters",
    "heading": "9.2.2 Variables in the trains dataset",
    "text": "look recommendations practice, let’s focus trains dataset. variables trains include gender, liberal, party, age, income, att_start, treatment, att_end. variables best include model?",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "att_end-treatment-att_start",
    "chapter": "9 Four Parameters",
    "heading": "9.2.3 att_end ~ treatment + att_start",
    "text": "First, let’s look model left hand variable, att_end, two right side variables, treatment att_start.decide variables useful? First, let’s interpret coefficients.variable tilde, att_end, outcome.variable tilde, att_end, outcome.explanatory variables treatment, says whether commuter relieved treatment control conditions, att_start, measures attitude start study.explanatory variables treatment, says whether commuter relieved treatment control conditions, att_start, measures attitude start study.95% confidence interval att_end equal coefficient– 2– plus minus two standard errors. shows estimate att_end commuters treatment control group.95% confidence interval att_end equal coefficient– 2– plus minus two standard errors. shows estimate att_end commuters treatment control group.variable treatmentControl represents offset att_end estimate (Intercept). offset group people Control group. find estimated att_end group, must add median treatmentControl (Intercept) median.variable treatmentControl represents offset att_end estimate (Intercept). offset group people Control group. find estimated att_end group, must add median treatmentControl (Intercept) median.variable att_start measures expected difference att_end every one unit increase att_start.variable att_start measures expected difference att_end every one unit increase att_start.causal effect variable treatmentControl -1. means , compared predicted att_end groups treatment, control group predicted attitude one entire point lower. can see, large well estimated coefficient. Recall means, roughly, 95% confidence interval excludes zero. calculate 95% confidence interval, take coefficient plus minus two standard errors: -1.3 -0.5. can see, 95% confidence interval exclude zero, suggesting treatment worthy variable.addition meaningful, enough justify inclusion model, variable satisfies number qualifications:\n- variable small standard error.\n- variable considered indicator variable, separates two groups significance (treatment control) like study.variable att_start, coefficient 1, also meaningful. Due fact MAD_SD value 0, need find 95% confidence interval know –intuitively– variable large well estimated coefficient.Conclusion: keep variables! treatment att_start significant, well satisfying requirements guidelines. worthy inclusion model.",
    "code": "\nfit_1_model <- stan_glm(att_end ~ treatment + att_start, \n                    data = trains, \n                    refresh = 0)\n\nfit_1_model## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ treatment + att_start\n##  observations: 115\n##  predictors:   3\n## ------\n##                  Median MAD_SD\n## (Intercept)       2.3    0.4  \n## treatmentControl -0.9    0.3  \n## att_start         0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.3    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "path": "four-parameters.html",
    "id": "income-age-liberal",
    "chapter": "9 Four Parameters",
    "heading": "9.2.4 income ~ age + liberal",
    "text": "Now, look income function age liberal, proxy political party.Great! estimate income fall category liberalFALSE, well data right hand side variables age liberalTRUE.First, let’s interpret coefficients.variable tilde, income, outcome.variable tilde, income, outcome.explanatory variables liberal, resulting value TRUE FALSE, age, numeric variable.explanatory variables liberal, resulting value TRUE FALSE, age, numeric variable.(Intercept) estimating income liberal == FALSE. Therefore, estimated income commuters liberals age = 0. estimate age showing increase income every additional year age.(Intercept) estimating income liberal == FALSE. Therefore, estimated income commuters liberals age = 0. estimate age showing increase income every additional year age.estimate liberalTRUE represents offset predicted income commuters liberal. find estimate, must add coefficient (Intercept) value. see , average, liberal commuters make less money.estimate liberalTRUE represents offset predicted income commuters liberal. find estimate, must add coefficient (Intercept) value. see , average, liberal commuters make less money.important note looking causal relationship either explanatory variables. noting differences two groups, without considering causality. known across unit comparison. compare across unit looking causal relationship.comparing liberalTRUE (Intercept), recall (Intercept) calculating income case liberal == FALSE. can see, , coefficient liberalTRUE, -32,491, shows liberals dataset make less average non-liberals dataset. coefficient large relative (Intercept) , rough mental math, see 95% confidence interval excludes 0. Therefore, liberal helpful variable!variable age, however, appear meaningful impact (Intercept). coefficient age low 95% confidence interval exclude 0.Conclusion: definitely keep liberal! age less clear. really matter preference point.",
    "code": "\nfit_2 <- stan_glm(income ~ age + liberal, \n                    data = trains, \n                    refresh = 0)\n\nfit_2## stan_glm\n##  family:       gaussian [identity]\n##  formula:      income ~ age + liberal\n##  observations: 115\n##  predictors:   3\n## ------\n##             Median   MAD_SD  \n## (Intercept) 110298.3  23690.0\n## age           1079.4    534.6\n## liberalTRUE -32490.6  13481.3\n## \n## Auxiliary parameter(s):\n##       Median  MAD_SD \n## sigma 72547.0  4894.7\n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "path": "four-parameters.html",
    "id": "liberal-iincome1e05-party",
    "chapter": "9 Four Parameters",
    "heading": "9.2.5 liberal ~ I(income/1e+05) + party",
    "text": "now look liberal function transformed income party. reason transformed income due fact , without transformation, income makes predictions according small income disparities. helpful. enhance significance income, divided variable \\(100000\\). (Note need () creating income term within formula argument.)Recall , logistic models, (Intercept) nearly impossible interpret. evaluate impact variables, need use Divide--Four rule. instructs us take logistic regression coefficient (constant term) divide 4 get upper bound predictive difference corresponding unit difference variable. means , evaluating predictor helpful, logistic regression , divide coefficient four.partyRepublican, take coefficient, 0, divide 4: -0.12. upper bound predictive difference corresponding unit difference variable. Therefore, see partyRepublican meaningful. want variable model.variable income, however, seems less promising. addition resulting value Divide--Four rule low (-0.02), MAD_SD income also equal posterior median , indicating standard error large relative magnitude coefficient. Though may choose include income reasons unrelated impact model, appear worthy inclusion basis predictive value alone.Conclusion: variable party significant; include model. Unless strong reason include income, exclude .",
    "code": "\nfit_3 <- stan_glm(liberal ~ I(income/1e+05) + party, \n                    data = trains, \n                    refresh = 0)\n\nfit_3## stan_glm\n##  family:       gaussian [identity]\n##  formula:      liberal ~ I(income/1e+05) + party\n##  observations: 115\n##  predictors:   3\n## ------\n##                 Median MAD_SD\n## (Intercept)      0.7    0.1  \n## I(income/1e+05) -0.1    0.1  \n## partyRepublican -0.5    0.1  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.5    0.0   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg"
  },
  {
    "path": "four-parameters.html",
    "id": "final-thoughts",
    "chapter": "9 Four Parameters",
    "heading": "9.2.6 Final thoughts",
    "text": "Now looked three cases variables decided whether included, let’s discuss concept selecting variables generally.variables decided keep discard necessarily variables keep discard, variables data scientist keep discard. much easier keep variable build case discarding variable. Variables helpful even significant, particularly indicator variables may separate data two groups want study.process selecting variables – though guidelines – complicated. many reasons include variable model. main reasons exclude variable variable isn’t significant variable large standard error.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "comparing-models-in-theory",
    "chapter": "9 Four Parameters",
    "heading": "9.3 Comparing models in theory",
    "text": "Deciding variables include model subset larger question: decide model, set possible models, choose?Consider two models explain attitudes immigration among Boston commuters.seem like good models! results make sense. People liberal liberal attitudes immigration, expect att_end scores lower. also expect people provide similar answers two surveys administered week two apart. makes sense higher (conservative) values att_start also higher values att_end.choose models?",
    "code": "\nfit_liberal <- stan_glm(formula = att_end ~ liberal,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nprint(fit_liberal, detail = FALSE)##             Median MAD_SD\n## (Intercept) 10.0    0.3  \n## liberalTRUE -2.0    0.5  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 2.7    0.2\nfit_att_start <- stan_glm(formula = att_end ~ att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 85)\n\nprint(fit_att_start, detail = FALSE)##             Median MAD_SD\n## (Intercept) 1.6    0.4   \n## att_start   0.8    0.0   \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.4    0.1"
  },
  {
    "path": "four-parameters.html",
    "id": "better-models-make-better-predictions",
    "chapter": "9 Four Parameters",
    "heading": "9.3.1 Better models make better predictions",
    "text": "obvious criteria comparing models accuracy predictions. example, consider use liberal predict att_end.two possible values liberal — TRUE FALSE — two predictions model make: 10 liberal == FALSE 8 liberal == TRUE. (points plot jittered.) individuals, perfect predictions. others, poor predictions. red circles plot illustrate areas predictions equal true values. can see, model isn’t great predicting attitude end. (Note two individuals liberal == TRUE, model thinks att_end == 8, att_end == 15. model got , wrong.)Consider second model, using att_start forecast att_end.att_end takes 13 unique values, model makes 13 unique predictions. predictions perfect! others wrong. red line shows points predictions match truth. Note individual predicted att_end around 9 actual value 15. big miss!Rather looking individual cases, need look errors predictions. Fortunately, prediction error thing residual, easy enough calculate.Let’s look square root average squared error.many different measures error might calculate. squared difference common historical reasons: mathematically tractable pre-computer age. calculated squared difference observation, can sum take average take square root average. produce relative ranking, last popular (less) corresponds estimated \\(\\sigma\\) linear model. Note measures ones produced Bayesian models created .Sadly, wise simply select model fits data best can misleading. , cheating! using data select parameters , using data , turning around “checking” see well model fits data. better fit! used pick parameters! danger overfitting.",
    "code": "\ntrains |> \n  mutate(pred_liberal = fitted(fit_liberal)) |> \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Add a red circle where our predictions are most accurate (where the x and y\n  # values are the same, which is where our predictions = the true attitudes).\n  # pch = 1 makes the inside of the point translucent to show the number of\n  # correct predictions.\n  \n  geom_point(aes(x = 8, y = 8), \n             size = 20, pch = 1, \n             color = \"red\") +\n  geom_point(aes(x = 10, y = 10), \n             size = 20, pch = 1, \n             color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Liberals are less conservative\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\ntrains |> \n  mutate(pred_liberal = fitted(fit_att_start)) |> \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Insert red line where our predictions = the truth using geom_abline with an\n  # intercept, slope, and color.\n  \n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Survey responses are somewhat consistent\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\ntrains |> \n  select(att_end, att_start, liberal) |> \n  mutate(pred_lib = fitted(fit_liberal)) |> \n  mutate(resid_lib = fitted(fit_liberal) - att_end) |> \n  mutate(pred_as = fitted(fit_att_start)) |> \n  mutate(resid_as = fitted(fit_att_start) - att_end)## # A tibble: 115 × 7\n##    att_end att_start liberal pred_lib resid_lib pred_as resid_as\n##      <dbl>     <dbl> <lgl>      <dbl>     <dbl>   <dbl>    <dbl>\n##  1      11        11 FALSE      10.0    -0.974    10.6    -0.399\n##  2      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n##  3       5         3 TRUE        8.02    3.02      4.08   -0.916\n##  4      11        11 FALSE      10.0    -0.974    10.6    -0.399\n##  5       5         8 TRUE        8.02    3.02      8.16    3.16 \n##  6      13        13 FALSE      10.0    -2.97     12.2    -0.769\n##  7      13        13 FALSE      10.0    -2.97     12.2    -0.769\n##  8      11        10 FALSE      10.0    -0.974     9.79   -1.21 \n##  9      12        12 FALSE      10.0    -1.97     11.4    -0.584\n## 10      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n## # … with 105 more rows\ntrains |> \n  select(att_end, att_start, liberal) |> \n  mutate(lib_err = (fitted(fit_liberal) - att_end)^2) |> \n  mutate(as_err = (fitted(fit_att_start) - att_end)^2) |> \n  summarize(lib_sigma = sqrt(mean(lib_err)),\n            as_sigma = sqrt(mean(as_err))) ## # A tibble: 1 × 2\n##   lib_sigma as_sigma\n##       <dbl>    <dbl>\n## 1      2.68     1.35"
  },
  {
    "path": "four-parameters.html",
    "id": "beware-overfitting",
    "chapter": "9 Four Parameters",
    "heading": "9.3.2 Beware overfitting",
    "text": "One biggest dangers data science overfitting, using model many parameters fits data well , therefore, works poorly data yet see. Consider simple example 10 data points.happens fit model one predictor?reasonable model. fit data particularly well, certainly believe higher values x associated higher values y. linear fit unreasonable.can also use lessons try quadratic fit adding \\(x^2\\) predictor.better model? Maybe?stop adding \\(x^2\\) regression? add \\(x^3\\), \\(x^4\\) way \\(x^9\\)? , fit much better.criteria cared well model predicts using data parameters estimated, model parameters always better. truly matters. matters well model works data used create model.",
    "code": "\nnine_pred <- lm(y ~ poly(x, 9),\n                       data = ovrftng)\n\nnewdata <- tibble(x = seq(1, 10, by = 0.01),\n                  y = predict(nine_pred, \n                              newdata = tibble(x = x)))\n\novrftng |> \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = newdata, \n              aes(x, y)) +\n    labs(title = \"`y` as a 9-Degree Polynomial Function of `x`\") +\n    scale_x_continuous(breaks = seq(2, 10, 2)) +\n    scale_y_continuous(breaks = seq(2, 10, 2)) "
  },
  {
    "path": "four-parameters.html",
    "id": "better-models-make-better-predictions-on-new-data",
    "chapter": "9 Four Parameters",
    "heading": "9.3.3 Better models make better predictions on new data",
    "text": "sensible way test model use model make predictions compare predictions new data. fitting model using stan_glm, use posterior_predict obtain simulations representing predictive distribution new cases. instance, predict someone’s attitude changes toward immigration among Boston commuters based political affiliation, want go test theories new Boston commuters.thinking generalization new data, important consider relevant new data context modeling problem. models used predict future , cases, can wait eventually observe future check good model making predictions. Often models used obtain insight phenomenon without immediate plan predictions. case Boston commuters example. cases, also interested whether learned insights part data generalizes parts data. example, know political attitudes informed future immigration stances Boston commuters, may want know conclusions generalize train commuters different locations.Even detected clear problems predictions, necessarily mean anything wrong model fit original dataset. However, need understand generalizing commuters.Often, like evaluate compare models without waiting new data. One can simply evaluate predictions observed data. since data already used fit model parameters, predictions optimistic.cross validation, part data used fit model rest data—hold-set—used proxy future data. natural prediction task future data, can think cross validation way assess generalization one part data another part.form cross validation, model re-fit leaving one part data prediction held-part evaluated. next section, look type cross validation called leave-one-(LOO) cross validation.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "comparing-models-in-practice",
    "chapter": "9 Four Parameters",
    "heading": "9.4 Comparing models in practice",
    "text": "compare models without waiting new data, evaluate predictions observed data. However, due fact data used fit model parameters, predictions often optimistic assessing generalization.cross validation, part data used fit model, rest data used proxy future data. can think cross validation way assess generalization one part data another part. ?can hold individual observations, called leave-one-(LOO) cross validation; groups observations, called leave-one-group-cross validation; use past data predict future observations, called leave-future-cross validation. perform cross validation, model re-fit leaving one part data prediction held-part evaluated.purposes, performing cross validation using leave-one-(LOO) cross validation.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "cross-validation-using-loo",
    "chapter": "9 Four Parameters",
    "heading": "9.4.1 Cross validation using loo()",
    "text": "compare models using leave-one-(LOO) cross validation, one piece data excluded model. model re-fit makes prediction missing piece data. difference predicted value real value calculated. process repeated every row data dataset.essence: One piece data excluded model, model re-fit, model attempts predict value missing piece, compare true value predicted value, assess accuracy model’s prediction. process occurs piece data, allowing us assess model’s accuracy making predictions.perform leave-one-(LOO) cross validation, using function loo() R package. determine model superior purposes.First, refamiliarize first model, fit_liberal.Now, perform loo() model look results.mean?elpd_loo estimated log score along standard error representing uncertainty due using 115 data points.p_loo estimated “effective number parameters” model.looic LOO information criterion, −2 elpd_loo, compute comparability deviance.purposes, mostly need focus elpd_loo. Let’s explain, depth, information means.Basically, run loo(), telling R take piece data dataset, re-estimate parameters, predict value missing piece data. value elpd_loo() based close estimate truth. Therefore, elpd_loo() values inform us effectiveness model predicting data seen .higher value elpd_loo, better model performs. means , comparing models, want select model higher value elpd_loo.Let’s turn attention second model. begin, let’s observe qualities fit_att_start .Great! Now, let’s perform loo() model.elpd_loo value model -201.7. higher elpd_loo att_liberal, implying model superior. However, can’t see estimates together. simpler way calculate model better?Actually, yes! Using function loo_compare(), can compare models directly.",
    "code": "\nfit_liberal## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ liberal\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept) 10.0    0.3  \n## liberalTRUE -2.0    0.5  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 2.7    0.2   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nloo_liberal <- loo(fit_liberal)\n\nloo_liberal## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -279.4  7.5\n## p_loo         2.9  0.5\n## looic       558.9 15.0\n## ------\n## Monte Carlo SE of elpd_loo is 0.0.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details.\nfit_att_start## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ att_start\n##  observations: 115\n##  predictors:   2\n## ------\n##             Median MAD_SD\n## (Intercept) 1.6    0.4   \n## att_start   0.8    0.0   \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.4    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nloo_att_start <- loo(fit_att_start) \n\nloo_att_start## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -201.7 12.5\n## p_loo         4.2  1.8\n## looic       403.4 25.1\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## All Pareto k estimates are good (k < 0.5).\n## See help('pareto-k-diagnostic') for details."
  },
  {
    "path": "four-parameters.html",
    "id": "comparing-models-using-loo_compare",
    "chapter": "9 Four Parameters",
    "heading": "9.4.2 Comparing models using loo_compare()",
    "text": "compare two models directly, can use function loo_compare two loo objects created . calculate difference elpd_loo() models us, making job easier:value elpd_diff equal difference elpd_loo two models. These_diff shows difference standard error.interpret results directly, important note first row superior model. values elpd_diff att_start 0, columns show offset estimates compared better model. reiterate: better model compared , values 0. following rows show offset elpd se values less effective model, fit_liberal, effective model, fit_att_start.better model clear: fit_att_start. Therefore, attitude start trains study significant predicting final attitude compared variable liberal, analog political affiliation.seen, loo_compare shortcut comparing two models. deciding two models, loo_compare() great way simplify decision.value loo_compare() small? general practice, differences smaller four hard distinguish noise. words: elpd_diff less 4, advantage one model .",
    "code": "\nloo_compare(loo_att_start, loo_liberal)##               elpd_diff se_diff\n## fit_att_start   0.0       0.0  \n## fit_liberal   -77.7      12.0"
  },
  {
    "path": "four-parameters.html",
    "id": "testing-is-nonsense",
    "chapter": "9 Four Parameters",
    "heading": "9.5 Testing is nonsense",
    "text": "always, important look practices professionals reasons may choose follow tactics. instance, continued problem hypothesis testing. hypothesis testing, assert null hypothesis \\(H_0\\) data alternative hypothesis \\(H_a\\).performing hypothesis testing, either reject hypothesis reject . qualifications rejecting met 95% confidence interval excludes null hypothesis. hypothesis included 95% confidence interval, reject . case “insignificant” results, p > 0.5, also can’t “reject” null hypothesis. However, mean accept .premise hypothesis testing answer specific question – one may even particularly relevant understanding world – data. , problems hypothesis testing?\n- Rejecting rejecting hypotheses doesn’t helps us answer real questions.\n- fact difference “significant” relevance use posterior make decisions.\n- Statistical significance equal practical importance.\n- reason test can summarize providing full posterior probability distribution.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "parallel-lines",
    "chapter": "9 Four Parameters",
    "heading": "9.6 Parallel lines",
    "text": "conclude chapter, look four parameter model. model use measure att_end function liberal att_start treatment combination variables.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "wisdom-4",
    "chapter": "9 Four Parameters",
    "heading": "9.6.1 Wisdom",
    "text": "\nFIGURE 9.1: Wisdom\nmaking model seeks explain att_end, plot variables think connected :data believable? Maybe? One imagine att_end predicted fairly well att_start. makes sense data points, show much difference attitudes. great disparities attitude shown individual starting attitude 9 ending attitude around 15? real data science project, require investigation. now, ignore issue blithely press .Another component Wisdom population. concept “population” subtle important. population set commuters data. dataset. population larger — potentially much larger — set individuals want make inferences. parameters models refer population, dataset.many different populations, \\(\\mu\\), might interested. instance:population Boston commuters specific train time included dataset.population Boston commuters specific train time included dataset.population Boston commuters.population Boston commuters.population commuters United States.population commuters United States.populations different, different \\(\\mu\\). \\(\\mu\\) interested depends problem trying solve. judgment call, matter Wisdom, whether data “close enough” population interested justify making model.major part Wisdom deciding questions can’t answer data don’t .",
    "code": "\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  geom_point() +\n  labs(title = \"Attitude End Compared with Attitude Start and Liberal\",\n       x = \"Attitude at Start of Study\",\n       y = \"Attitude at End of Study\",\n       color = \"Liberal?\")"
  },
  {
    "path": "four-parameters.html",
    "id": "justice-7",
    "chapter": "9 Four Parameters",
    "heading": "9.6.2 Justice",
    "text": "\nFIGURE 9.2: Justice\nNow considered connection data predictions seek make, need consider model.First: model causal predictive? Recall model measures att_end function liberal att_start. variables liberal att_start involve control treatment dynamic. manipulation variables. Given causation without manipulation, predictive model.making inferences groups people according political affiliation starting attitude. measuring causality, predicting outcomes.creating parallel slops model, use basic equation line:\\[y_i = \\beta_0  + \\beta_1 x_{1,} + \\beta_2 x_{2,}\\]\\(y = att\\_end\\), \\(x_1 = att\\_start\\), \\(x\\_2 = liberal\\), equations follows:liberal = FALSE:\\[y_i = \\beta_0  + \\beta_1 x_{1,}\\]\nequates , y = b + mx form:\\[y_i = intercept +  \\beta_1 att\\_start_i\\]liberal = TRUE:\\[y_i = (\\beta_0  + \\beta_2) + \\beta_1 x_{1,}\\]equates , y = b + mx form:\\[y_i = (intercept + liberal\\_true) + \\beta_1 att\\_start_i\\]",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "courage-7",
    "chapter": "9 Four Parameters",
    "heading": "9.6.3 Courage",
    "text": "\nFIGURE 9.3: Courage\nuse stan_glm usual. Using stan_glm, create model, fit_1.remind , recall (Intercept) representing att_end value cases liberal = FALSE treatment equal Control. next row, liberalTRUE gives median value represents offset prediction compared (Intercept). words, true intercept cases liberal = TRUE represented \\((Intercept) + liberalTRUE\\). value treatmentControl offset att_end Control group.find intercepts, need tidy regression using tidy() function broom.mixed package. tidy data extract values create parallel slopes model. parallel slopes model allows us visualize multi-variate Bayesian modeling (.e. modeling one explanatory variable). complicated way saying visualize fitted model created way allows us see intercepts slopes two different groups, liberalTRUE liberalFALSE:Now, can define following terms—- liberal_false_intercept liberal_false_att_slope; liberal_true_intercept liberal_true_att_slope:’ve done extracted values intercepts slopes, named separated two groups. allows us create geom_abline object takes unique slope intercept value, can separate liberalTRUE liberalFALSE observations.parallel slopes model. done, essentially, created unique line liberalTRUE liberalFALSE observe differences groups related attitude start attitude end.can see, commuters liberal tend start slightly higher values att_start. Commuters liberal tend lower starting values att_start.Now, want look another model? judge whether can superior model? Let’s create another object, using stan_glm, looks att_end function treatment att_start.interpret briefly:(Intercept) representing att_end value cases treatment equal Control.(Intercept) representing att_end value cases treatment equal Control.treamtmentControl gives median value represents offset prediction compared (Intercept). words, true intercept cases treatment = Control represented \\((Intercept) + treatmentControl\\).treamtmentControl gives median value represents offset prediction compared (Intercept). words, true intercept cases treatment = Control represented \\((Intercept) + treatmentControl\\).att_start represents slope groups representing unit change.att_start represents slope groups representing unit change.important point models causal. including variable treatment, measured causal effect condition. different prior parallel slopes model, modeling prediction, causation.see models compare performance, perform leave-one-(LOO) cross validation .Perform loo() second model:Recall relevant data data elpd_loo. estimates elpd_loo vary quite bit. Recall superior model value elpd_loo() closer 0. standard error (SE) models also differs . compare directly, use loo_compare.Recall , loo_compare(), resulting data shows superior model first, values 0 elpd_diff se_diff, since compares models best option. values elpd_diff se_diff fit_1 show difference models. can see, fit_2, model looks treatment + att_start, better.certain can better? Note difference two models quite two standard errors. , reasonable possibility difference due chance.",
    "code": "\nfit_1 <- stan_glm(formula = att_end ~ liberal + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nfit_1## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ liberal + att_start\n##  observations: 115\n##  predictors:   3\n## ------\n##             Median MAD_SD\n## (Intercept)  1.9    0.5  \n## liberalTRUE -0.3    0.3  \n## att_start    0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.4    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\n# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy <- fit_1 |> \n  tidy() |> \n  select(term, estimate)\n\ntidy## # A tibble: 3 × 2\n##   term        estimate\n##   <chr>          <dbl>\n## 1 (Intercept)    1.93 \n## 2 liberalTRUE   -0.300\n## 3 att_start      0.799\n# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept <- tidy$estimate[1]\nliberal_true <- tidy$estimate[2]\natt_start <- tidy$estimate[3]\n# Recall that the (Intercept) shows us the estimate for the case where liberal =\n# FALSE. We want to extract the liberal_false_intercept to indicate where the\n# intercept in our visualization should be. The slope for this case, and for the\n# liberal = TRUE case, is att_start.\n\nliberal_false_intercept <- intercept\nliberal_false_att_slope <- att_start\n\n#  When wanting the intercept for liberal = TRUE, recall that the estimate for\n#  liberalTRUE is the offset from our (Intercept). Therefore, to know the true\n#  intercept, we must add liberal_true to our intercept.\n\nliberal_true_intercept <- intercept + liberal_true\nliberal_true_att_slope <- att_start\n# From the dataset trains, use att_start for the x-axis and att_end for\n# the y-axis with color as liberal. This will split our data into two color\n# coordinates (one for liberal = TRUE and one for liberal = FALSE)\n\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the liberal false values. Set the intercept\n  # equal to our previously created liberal_false_intercept, while setting slope\n  # equal to our previously created liberal_false_att_slope. The color call is\n  # for coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = liberal_false_intercept,\n              slope = liberal_false_att_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the liberal TRUE values. Set the intercept\n  # equal to our previously created liberal_true_intercept, while setting slope\n  # equal to our previously created liberal_true_att_slope. The color call is\n  # for teal, to match the colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = liberal_true_intercept,\n              slope = liberal_true_att_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Parallel Slopes Model\",\n       x = \"Attitude at Start\", \n       y = \"Attitude at End\", \n       color = \"Liberal\") \nfit_2 <- stan_glm(formula = att_end ~ treatment + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 56)\n\nfit_2## stan_glm\n##  family:       gaussian [identity]\n##  formula:      att_end ~ treatment + att_start\n##  observations: 115\n##  predictors:   3\n## ------\n##                  Median MAD_SD\n## (Intercept)       2.4    0.4  \n## treatmentControl -1.0    0.2  \n## att_start         0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 1.3    0.1   \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\nL1 <- loo(fit_1)\n\nL1## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -202.3 13.1\n## p_loo         5.5  2.3\n## looic       404.6 26.1\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     114   99.1%   2531      \n##  (0.5, 0.7]   (ok)         1    0.9%   157       \n##    (0.7, 1]   (bad)        0    0.0%   <NA>      \n##    (1, Inf)   (very bad)   0    0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\nL2 <- loo(fit_2)\n\nL2## \n## Computed from 4000 by 115 log-likelihood matrix\n## \n##          Estimate   SE\n## elpd_loo   -195.3 12.2\n## p_loo         5.1  1.8\n## looic       390.5 24.5\n## ------\n## Monte Carlo SE of elpd_loo is 0.1.\n## \n## Pareto k diagnostic values:\n##                          Count Pct.    Min. n_eff\n## (-Inf, 0.5]   (good)     114   99.1%   1625      \n##  (0.5, 0.7]   (ok)         1    0.9%   206       \n##    (0.7, 1]   (bad)        0    0.0%   <NA>      \n##    (1, Inf)   (very bad)   0    0.0%   <NA>      \n## \n## All Pareto k estimates are ok (k < 0.7).\n## See help('pareto-k-diagnostic') for details.\nloo_compare(L1, L2)##       elpd_diff se_diff\n## fit_2  0.0       0.0   \n## fit_1 -7.0       3.8"
  },
  {
    "path": "four-parameters.html",
    "id": "temperance-7",
    "chapter": "9 Four Parameters",
    "heading": "9.6.4 Temperance",
    "text": "really care data haven’t seen yet, mostly data tomorrow. world changes, always ? doesn’t change much, maybe OK. changes lot, good model ? general, world changes . means forecasts uncertain naive use model might suggest.\ncreated (checked) model, now use model answer questions. Models made use, beauty. world confronts us. Make decisions must. decisions better ones use high quality models help make .Preceptor’s Posterior posterior calculate assumptions made Wisdom Justice correct. Sadly, never ! , can never know Preceptor’s Posterior. posterior , hope, close-ish approximation Preceptor’s Posterior.",
    "code": ""
  },
  {
    "path": "four-parameters.html",
    "id": "summary-10",
    "chapter": "9 Four Parameters",
    "heading": "9.7 Summary",
    "text": "chapter, covered number topics important effectively creating models.Key commands:\n- Create model using stan_glm().\n- creating model, can use loo() perform leave-one-cross validation. assesses effectively model makes predictions data seen yet.\n- command loo_compare() allows us compare two models, see one performs better leave-one-cross validation. superior model makes better predictions.Remember:\n- can transform variables – centering, scaling, taking logs, etc. – make sensible. Consider using transformation intercept awkward. instance, intercept age represents estimate people age zero, might consider transforming age easier interpret.\n- selecting variables include model, follow rule: keep variable large well-estimated coefficient. means 95% confidence interval excludes zero. Speaking roughly, removing variable large coefficient meaningfully changes predictions model.\n- compare across unit, meaning comparing Joe George, looking causal relationship. Within unit discussions, comparing Joe treatment versus Joe control, causal. means within unit interpretation possible causal models, studying one unit two conditions.\n- talk two potential outcomes, discussing person unit two conditions.",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "five-parameters",
    "chapter": "10 Five Parameters",
    "heading": "10 Five Parameters",
    "text": "Last chapter, studied four parameters: models studied multiple right-hand side variables . next step model building education learn interactions. effect treatment relative control almost never uniform. effect might bigger women men, smaller rich relative poor. technical term effects “heterogeneous,” just Ph.D.’ese “different.” enough data, effects heterogeneous. Causal effects, least social science, always vary across units. model reality, rely interactions, allowing effect size differ. applies predictive models. relationship outcome variable \\(y\\) predictor variable \\(x\\) rarely constant. relationship varies based values variables. take account interactions, need models least 5 parameters.Packages:Consider following questions:many years expect two gubernatorial candidates — one male one female, 10 years older average candidate — live election? different lifespans ? broadly, long candidates, general, live election? winning election affect longevity?Note far come Primer. difficult questions, involving issues prediction causation. Yet, follow Cardinal Virtues, can provide sophisticated answers.",
    "code": "\nlibrary(primer.data)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(skimr)\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(broom.mixed)"
  },
  {
    "path": "five-parameters.html",
    "id": "wisdom-5",
    "chapter": "10 Five Parameters",
    "heading": "10.1 Wisdom",
    "text": "\nFIGURE 10.1: Wisdom\nRecall important aspects Wisdom: Preceptor Table, EDA (exploratory data analysis), population, Population Table. always, start Preceptor Table — table data make questions answerable mere arithmetic (inferences).",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "the-preceptor-table",
    "chapter": "10 Five Parameters",
    "heading": "10.1.1 The Preceptor Table",
    "text": "create Preceptor Table, must first revisit questions:many years expect two gubernatorial candidates — one male one female, 10 years older average candidate — live election? different lifespans ? broadly, long candidates, general, live election? winning election affect longevity?(imagined) dataset make questions easy solve little bit math? Well, obviously need data gubernatorial candidate elections United States. also need know dates birth, age time election, age time death, data age time death minus age time election. pieces information, answer questions simple math.Also, idealized table, know age time death assuming victory age time death assuming loss. possible real world due Fundamental Problem Causal Inference — observe unit two different conditions (victory loss).sample:rows every gubernatorial election candidate U.S. history. may want details, election year. merely sketch ideal dataset. Now know “perfect” reality, data actually work ?",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "eda-of-governors",
    "chapter": "10 Five Parameters",
    "heading": "10.1.2 EDA of governors",
    "text": "primer.data package includes governors data set features demographic information candidates governor United States. Barfort, Klemmensen, Larsen (2020) gathered data concluded winning gubernatorial election increases candidate’s lifespan.14 variables 1,092 observations. Chapter, looking variables last_name, year, state, sex, lived_after, election_age, won, close_race.election_age lived_after many years candidate lived election, respectively. consequence, politicians already deceased included data set. means handful observations elections last 20 years. candidates time period still alive , therefore, excluded. created won variable indicate whether candidate won election. define close_race true winning margin less 5%.One subtle issue: candidate included multiple times? example:now, leave multiple observations single person.First, let’s sample dataset.might expect, sex often “Male”. precise inspecting data, let’s skim() dataset.TABLE 10.1: Data summaryVariable type: characterVariable type: logicalVariable type: numericskim() groups variable together type, provides analysis variable. also given histograms numeric data.Looking histogram year, see skewed right — meaning data bunched left smaller tail right — half observations election years 1945 1962. makes sense logically, looking deceased candidates, candidates recent elections likely still alive.using data set, left-side variable lived_after. trying understand/predict many years candidate live election.Note rough line see observations. might ? looking year elected years lived post-election, missing data upper right quadrant due fact impossible elected post-2000 lived 21 years. Simply put: “edge” data represents, approximately, years candidate lived, still died, given year elected.reason data slanted downward maximum value scenario greater earlier years. , candidates ran governor earlier years live long time election still died prior data set creation, giving higher lived_after values ran office recent years. edge scatter plot perfectly straight , many election years, candidate decency die just data collection. reason observations later years fewer recent candidates died.begin visualizing lived_after data, inspect difference years lived post election male female candidates.plot shows men live much longer, average, women election. intuitive explanation might ?",
    "code": "\nglimpse(governors)## Rows: 1,092\n## Columns: 14\n## $ state        <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"A…\n## $ year         <int> 1946, 1946, 1950, 1954, 1954, 1958, 1962, 1966, 1966, 197…\n## $ first_name   <chr> \"James\", \"Lyman\", \"Gordon\", \"Tom\", \"James\", \"William\", \"G…\n## $ last_name    <chr> \"Folsom\", \"Ward\", \"Persons\", \"Abernethy\", \"Folsom\", \"Long…\n## $ party        <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Republican\", \"Demo…\n## $ sex          <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"Male\", \"…\n## $ died         <date> 1987-11-21, 1948-12-17, 1965-05-29, 1968-03-07, 1987-11-…\n## $ status       <chr> \"Challenger\", \"Challenger\", \"Challenger\", \"Challenger\", \"…\n## $ win_margin   <dbl> 77.3, -77.3, 82.2, -46.7, 46.7, -77.5, 100.0, -34.3, 34.3…\n## $ region       <chr> \"South\", \"South\", \"South\", \"South\", \"South\", \"South\", \"So…\n## $ population   <dbl> 2906000, 2906000, 3058000, 3014000, 3014000, 3163000, 332…\n## $ election_age <dbl> 38, 79, 49, 47, 46, 33, 43, 48, 40, 51, 68, 55, 45, 52, 6…\n## $ death_age    <dbl> 79, 81, 63, 60, 79, 88, 79, 99, 42, 79, 75, 79, 76, 81, 7…\n## $ lived_after  <dbl> 41.0, 2.1, 14.6, 13.3, 33.0, 54.5, 35.9, 51.0, 1.5, 27.9,…\nch10 <- governors %>% \n  mutate(won = if_else(win_margin > 0, TRUE, FALSE)) %>% \n  mutate(close_race = if_else(abs(win_margin) < 5, TRUE, FALSE)) %>% \n  select(last_name, year, state, sex, lived_after, election_age, won, close_race)\nch10 %>% \n  filter(last_name == \"Cuomo\")## # A tibble: 4 × 8\n##   last_name  year state    sex   lived_after election_age won   close_race\n##   <chr>     <int> <chr>    <chr>       <dbl>        <dbl> <lgl> <lgl>     \n## 1 Cuomo      1982 New York Male         32.2         50.4 TRUE  TRUE      \n## 2 Cuomo      1986 New York Male         28.2         54.4 TRUE  FALSE     \n## 3 Cuomo      1990 New York Male         24.2         58.4 TRUE  FALSE     \n## 4 Cuomo      1994 New York Male         20.2         62.4 FALSE TRUE\nch10 %>% \n  slice_sample(n = 5)## # A tibble: 5 × 8\n##   last_name  year state        sex    lived_after election_age won   close_race\n##   <chr>     <int> <chr>        <chr>        <dbl>        <dbl> <lgl> <lgl>     \n## 1 Ristine    1964 Indiana      Male          44.6         44.8 FALSE FALSE     \n## 2 Sundlun    1986 Rhode Island Male          24.7         66.8 FALSE FALSE     \n## 3 Richards   1990 Texas        Female        15.9         57.2 TRUE  TRUE      \n## 4 Turner     1946 Oklahoma     Male          26.6         52.0 TRUE  FALSE     \n## 5 Williams   1948 Michigan     Male          39.2         37.7 TRUE  FALSE\nskim(ch10)\nch10 %>%\n  ggplot(aes(x = year, y = lived_after)) +\n  geom_point() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Candidates who died more recently can't have lived for long post-election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Year Elected\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() \nch10 %>%\n  ggplot(aes(x = sex, y = lived_after)) +\n  geom_boxplot() +\n  labs(title = \"US Gubernatorial Candidate Years Lived Post-Election\",\n       subtitle = \"Male candidates live much longer after the election\",\n       caption = \"Data: Barfort, Klemmensen and Larsen (2019)\",\n       x = \"Gender\",\n       y = \"Years Lived After Election\") +\n  scale_y_continuous(labels = scales::label_number()) +\n  theme_classic() "
  },
  {
    "path": "five-parameters.html",
    "id": "population-2",
    "chapter": "10 Five Parameters",
    "heading": "10.1.3 Population",
    "text": "concept “population” subtle important. population set candidates data. dataset. population larger — potentially much larger — set individuals want make inferences. parameters models refer population, dataset.Consider simple example. Define \\(\\mu\\) average number years lived candidates governor Election Day. Can calculate \\(\\mu\\) data? ! many candidates governor still alive, included data even though part “population” want study. \\(\\mu\\) can calculated. can estimated.Another problem like estimate effect winning lifespan present day. data excludes recent candidates (since still alive), predictions mirror future well may hope.Even though original question “gubernatorial candidates” general, specifically refer United States, assume data US governors representative enough population interested (global politicians) exercise useful. believe , stop right now. major part Wisdom deciding questions can’t answer data just don’t .truth : social sciences, never perfect relationship data question trying answer. Data gubernatorial candidates past analog gubernatorial candidates today. data candidates countries. Yet, data relevant. Right? certainly better nothing.Generally speaking, using -perfect data better using data .course, always true. wanted predict lifespans gubernatorial candidates U.S., data lifespans presidential candidates France… better making predictions . data won’t help, don’t use data.",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "justice-8",
    "chapter": "10 Five Parameters",
    "heading": "10.2 Justice",
    "text": "\nFIGURE 10.2: Justice\ninspecting data deciding “close enough” questions useful, move Justice.Justice emphasizes key concepts:Population Table, structure includes row every unit population. generally break rows Population Table three categories: data units want (actual data set), data units actually (Preceptor Table), data units care (rest population, included data Preceptor Table).data representative population?meaning columns consistent, .e., can assume validity? make assumption data generating mechanism.inspect representativeness validity Population Table. Representativeness focuses rows table, validity focuses columns.",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "population-table-4",
    "chapter": "10 Five Parameters",
    "heading": "10.2.1 Population Table",
    "text": "determining data drawn population analyzing, can go produce Population Table.Population Table shows rows three sources: Preceptor Table, actual data, population (outside data).Preceptor Table rows contain information want know order answer questions. rows contain entries covariates (sex, election_age, year elected) contain outcome results (lived_after). trying answer questions train commuter population 2021, year entries rows read “2021”.actual data rows contain information know. rows contain entries covariates outcomes. case, actual data comes gubernatorial candidates deceased. columns (covariates outcomes) complete.population rows contain data. subjects fall desired population, data. , rows missing., Population Table shows expansive population making assumptions — includes data “population”, actual data, Preceptor Table.",
    "code": "\ntibble(source = c(\"Population\", \"Population\", \"...\",\n                  \"Data\", \"Data\", \"...\",\n                  \"Population\", \"Population\", \"...\",\n                  \"Preceptor Table\", \"Preceptor Table\", \"...\",\n                  \"Population\", \"Population\"),\n       year_elected = c(\"1912\", \"1924\", \"...\",\n                \"1967\", \"2004\", \"...\",\n                \"2018\", \"2019\", \"...\",\n                \"2021\", \"2021\", \"...\",\n                \"2035\", \"2040\"),\n       election_age = c(\"?\", \"?\", \"...\",\n                        \"43\", \"67\", \"...\",\n                        \"?\", \"?\", \"...\",\n                        \"46\", \"39\", \"...\",\n                        \"?\", \"?\"),\n       sex = c(\"?\", \"?\", \"...\",\n               \"Male\", \"Female\", \"...\",\n               \"?\", \"?\", \"...\",\n               \"Female\", \"Female\", \"...\",\n                 \"?\", \"?\"),\n       lived_after = c(\"?\", \"?\", \"...\", \n                 \"20\", \"19\", \"...\",\n                 \"?\", \"?\", \"...\",\n                 \"?\", \"?\", \"...\",\n                 \"?\", \"?\")) %>%\n  \n  # Then, we use the gt function to make it pretty\n  \n  gt() %>%\n  cols_label(source = md(\"Source\"),\n             year_elected = md(\"Year\"),\n             election_age = md(\"Election Age\"),\n             sex = md(\"Sex\"),\n             lived_after = md(\"Years Lived After\")) "
  },
  {
    "path": "five-parameters.html",
    "id": "validity-2",
    "chapter": "10 Five Parameters",
    "heading": "10.2.2 Validity",
    "text": "Validity, hand, involves columns. put simply, column lifespan Preceptor Table equate column lifespan dataset. , look source data: Barfort, Klemmensen, Larsen (2020).collection birth death dates winning candidates well documented. birth death dates losing candidates, however, easily gathered. fact, Barfort, Klemmensen, Larsen (2020) perform independent research information:“losing candidates, use information gathered several online sources, including Wikipedia, Political Graveyard…, Find Grave… Campaigns.”nearly reliable data collection candidates won election. , complication:“cases, able identify year birth death, exact date event. candidates, impute date July 1 given year.”candidates, , estimate longevity inaccurate. also hope birth death dates listed unreliable internet sources accurate. possible , especially older candidates.mission exploration ensure validity much possible — , equate columns equated . case, fix issues data collection, accept estimates may slightly skewed.",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "stability-1",
    "chapter": "10 Five Parameters",
    "heading": "10.2.3 Stability",
    "text": "Stability means relationship columns three categories rows: data, Preceptor table, larger population drawn.outcome variable height, easier assume stability greater period time. Changes global height occur extremely slowly, height stable across span 20 years reasonable assume. Can say example, looking years lived post-election?Lifespan changes time. fact, 1960 2015, life expectancy total population United States increased almost 10 years 69.7 years 1960 79.4 years 2015. Therefore, estimates future may need adjustment — , add years predicted life expectancy account global change lifespan time.confronted uncertainty, can consider making timeframe smaller. , confined data candidates post-1980, expect stability lifespan. modification may appropriate, limits data. Stability, essence, allows us ignore issue time.Alternatively, believe unlikely columns stable, two choices. First, abandon experiment. believe data useless, experiment. Second, can choose provide sort warning message conclusions: based data ten years ago, recent data available us.",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "representativeness-2",
    "chapter": "10 Five Parameters",
    "heading": "10.2.4 Representativeness",
    "text": "external validity study often directly related representativeness sample. Representativeness well sample represents larger population interested generalizing .looking Barfort, Klemmensen, Larsen (2020), source dataset, see :“collect data… candidates running gubernatorial election 1945 2012. limit attention two candidates received highest number votes.”data , , highly representative gubernatorial candidates, includes every candidate 1945 2012. However, one large caveat: two candidates votes included dataset. unfortunate, ideally look gubernatorial candidates (regardless votes). Regardless, still deem dataset representative enough larger population.Generally: chance certain type person experiment, make assumption person.",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "courage-8",
    "chapter": "10 Five Parameters",
    "heading": "10.3 Courage",
    "text": "\nFIGURE 10.3: Courage\n",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "sex",
    "chapter": "10 Five Parameters",
    "heading": "10.3.1 sex",
    "text": "Let’s regress lived_after sex see candidates’ post-election lifespans differ sex.value female. However intercept. regression, mathematical formula :\\[ lived\\_after_i = \\beta_0  + \\beta_1 male_i + \\epsilon_i\\]\\(\\beta_0\\) intercept, around 16 years. type model, intercept represents variable represented model. Therefore, intercept value represents male (females).\\(\\beta_1\\) affects outcome candidate male. candidate male, add coefficient male intercept value, gives us average lifespan male gubernatorial candidate election.posterior distribution \\(\\beta_0 + \\beta_1\\) can constructed via simple addition.actually “simple?” really! Manipulating parameters directly bothersome. Dealing variables named (Intercept) error-prone. Using posterior_epred() associated functions much easier.interpretation parameter seen . true average, across entire population, number years male candidates live election. can never know true average . , seems likely true average somewhere 27.5 29.5 years.",
    "code": "\nfit_2 <- stan_glm(data = ch10,\n                       formula = lived_after ~ sex,\n                       refresh = 0,\n                       seed = 76)\nprint(fit_2, detail = FALSE)##             Median MAD_SD\n## (Intercept) 16.1    2.9  \n## sexMale     12.3    2.9  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 13.3    0.3\nfit_2 %>% \n  as_tibble() %>% \n  mutate(male_intercept = `(Intercept)` + sexMale) %>% \n  ggplot(aes(male_intercept)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Male Candidate Years Left\",\n         y = \"Probability\",\n         x = \"Years To Live After the Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nposterior_epred(fit_2,\n                tibble(sex = \"Male\")) %>% \n  as_tibble() %>% \n  ggplot(aes(`1`)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Male Candidate\",\n         y = \"Probability\",\n         x = \"Expected Lifespan Post-Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nposterior_epred(fit_2,\n                tibble(sex = \"Female\")) %>% \n  as_tibble() %>% \n  ggplot(aes(`1`)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of Average Female Candidate\",\n         y = \"Probability\",\n         x = \"Expected Lifespan Post-Election\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"
  },
  {
    "path": "five-parameters.html",
    "id": "election_age",
    "chapter": "10 Five Parameters",
    "heading": "10.3.2 election_age",
    "text": "begin, let’s model candidate lifespan election function candidate lifespan prior election. data:math fairly simple:\\[ lived\\_after_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i \\]\\(\\epsilon_i \\sim N(0, \\sigma^2)\\).\n- \\(lived\\_after_i\\) number years lived election candidate \\(\\).\n- \\(election\\_age_i\\) number years lived election candidate \\(\\).\n- \\(\\epsilon_i\\) “error term,” difference actual years-lived candidate \\(\\) modeled years-lived. \\(\\epsilon_i\\) normally distributed mean 0 standard deviation \\(\\sigma\\).key distinction :Variables, always subscripted \\(\\), whose values (potentially) vary across individuals.Variables, always subscripted \\(\\), whose values (potentially) vary across individuals.Parameters, never subscripted \\(\\), whose values constant across individuals.Parameters, never subscripted \\(\\), whose values constant across individuals.use \\(lived\\_after_i\\) formula instead \\(y_i\\)? often remind variable’s actual substance, better. another common convention: always use \\(y_i\\) symbol dependent variable. unusual describe model :\\[ y_i =  \\beta_0 + \\beta_1 election\\_age_i + \\epsilon_i\\]mean thing.Either way, \\(\\beta_0\\) “intercept” regression, average value population \\(lived\\_after\\), among \\(election\\_age = 0\\).\\(\\beta_1\\) “coefficient” \\(election\\_age\\). comparing two individuals, first \\(election\\_age\\) one year older second, expect first \\(lived\\_after\\) value \\(\\beta_1\\) different second. words, expect older fewer years remaining, \\(\\beta_1\\) negative. , value population data drawn.three unknown parameters — \\(\\beta_0\\), \\(\\beta_1\\) \\(\\sigma\\) — just models used Chapter 8. get five parameter case, useful review earlier material.may recall middle school algebra equation line \\(y = m x + b\\). two parameters: \\(m\\) \\(b\\). intercept \\(b\\) value \\(y\\) \\(x = 0\\). slope coefficient \\(m\\) \\(x\\) increase \\(y\\) every one unit increase \\(x\\). defining regression line, use slightly different notation fundamental relationship .Rather repeat process Chapter 8, look resulting plot using model predicts years lived factor age election.discussed Chapter 8, common term model like “regression.” “regressed” lived_after, dependent variable, election_age, () independent variable.Consider someone 40 years old Election Day. score data points candidates around age. area highlighted red box plot. can see, two died soon election. lived 50 years election. Variation fills world. However, fitted line tells us , average, expect candidate age live 37 years election.descriptive model, causal model. Remember motto Chapter 4: causation without manipulation. way, person \\(\\), change years alive Election Day. day election, X years old. , two () potential outcomes. Without one potential outcome, can causal effect.Given , important monitor language. believe changes election_age “cause” changes lived_after. obvious. words phrases — like “associated ” “change ” — close causal. (guilty using just paragraphs ago!) wary use. Always think terms comparisons using predictive model. can’t change election_age individual candidate. can compare two candidates (two groups candidates).Let’s look posterior \\(\\beta_1\\), coefficient \\(election\\_age_i\\):",
    "code": "\nch10 %>% \n  ggplot(aes(x = election_age, y = lived_after)) +\n    geom_point() +\n    labs(title = \"Longevity of Gubernatorial Candidates\",\n         subtitle = \"Younger candidates live longer\", \n         caption = \"Data Source: Barfort, Klemmensen and Larsen (2019)\",\n         x = \"Age in Years on Election Day\",\n         y = \"Years Lived After Election\") +\n    scale_x_continuous(labels = scales::label_number()) +\n    scale_y_continuous(labels = scales::label_number()) +\n    theme_classic()\nfit_1 %>% \n  as_tibble() %>% \n  ggplot(aes(election_age)) + \n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior Distribution of the Coefficient of `election_age`\",\n         y = \"Probability\",\n         x = \"Coefficient of `election_age`\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"
  },
  {
    "path": "five-parameters.html",
    "id": "election_age-and-sex",
    "chapter": "10 Five Parameters",
    "heading": "10.3.3 election_age and sex",
    "text": "model, outcome variable continues lived_after, now two different explanatory variables: election_age sex. Note sex categorical explanatory variable election_age continuous explanatory variable. type model — parallel slopes — saw Chapter 9.Math:\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\epsilon_i \\]wait! variable name sex, male. male come ?answer male indicator variable, meaning 0/1 variable. male takes value one candidate “Male” zero otherwise. \\(male_i\\) variable used previous two examples.outcome variable \\(lived\\_after_i\\), number years person alive election. \\(male_i\\) one explanatory variables. predicting number years male candidate lives election, value 1. making prediction female candidates, value 0. \\(c\\_election\\_age_i\\) explanatory variable. number years candidate lived election, scaled subtracting average number years lived candidates.outcome variable \\(lived\\_after_i\\), number years person alive election. \\(male_i\\) one explanatory variables. predicting number years male candidate lives election, value 1. making prediction female candidates, value 0. \\(c\\_election\\_age_i\\) explanatory variable. number years candidate lived election, scaled subtracting average number years lived candidates.\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates (.e. male female). \\(\\beta_0\\) also intercept equation. words, \\(\\beta_0\\) expected value \\(lived\\_after_i\\), \\(male_i = 0\\) \\(c\\_election\\_age_i = 0\\).\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates (.e. male female). \\(\\beta_0\\) also intercept equation. words, \\(\\beta_0\\) expected value \\(lived\\_after_i\\), \\(male_i = 0\\) \\(c\\_election\\_age_i = 0\\).\\(\\beta_1\\) almost meaningless . time meaning value connected intercept (.e. \\(\\beta_0 + \\beta_1\\)). two added together, get average number years lived election males, day election, alive average number years candidates.\\(\\beta_1\\) almost meaningless . time meaning value connected intercept (.e. \\(\\beta_0 + \\beta_1\\)). two added together, get average number years lived election males, day election, alive average number years candidates.\\(\\beta_2\\) , entire population, average difference \\(lived\\_after_i\\) two individuals, one \\(c\\_election\\_age_i\\) value 1 greater .\\(\\beta_2\\) , entire population, average difference \\(lived\\_after_i\\) two individuals, one \\(c\\_election\\_age_i\\) value 1 greater .Let’s translate model code.Looking results, can see intercept value around 66. average female candidate, alive average number years candidates, live another 66 years election.Note sexMale around 6. coefficient, \\(\\beta_1\\). need connect value intercept value get something meaningful. Using formula \\(\\beta_0 + \\beta_1\\), find number years average male candidate — , day election, average age candidates — live around 72 years.Now take look coefficient \\(c\\_election\\_age_i\\), \\(\\beta_2\\). median posterior, -0.8, represents slope model. comparing two candates differ one year election_age, expect differ -0.8 years lived_after. makes sense value negative. years candidate lived, fewer years candidate left live. , every extra year candidate alive election, lifespan election 0.8 years lower, average.now show parallel slopes model, created using process explained prior chapter. ’ve done extracted values intercepts slopes, separated two groups. allows us create geom_abline object takes unique slope intercept value, can separate male female observations.",
    "code": "\nfit_3 <- stan_glm(data = ch10,\n                      formula = lived_after ~ sex + election_age,\n                      refresh = 0,\n                      seed = 12)\nprint(fit_3, detail = FALSE)##              Median MAD_SD\n## (Intercept)  66.0    3.2  \n## sexMale       6.1    2.4  \n## election_age -0.8    0.0  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 11.1    0.2## # A tibble: 3 × 2\n##   term         estimate\n##   <chr>           <dbl>\n## 1 (Intercept)    66.0  \n## 2 sexMale         6.15 \n## 3 election_age   -0.847"
  },
  {
    "path": "five-parameters.html",
    "id": "election_age-sex-and-election_agesex",
    "chapter": "10 Five Parameters",
    "heading": "10.3.4 election_age, sex and election_age*sex",
    "text": "Let’s create another model. time, however, numeric outcome variable lived_after function two explanatory variables used , election_age sex, interaction. look interactions, need 5 parameters, needed wait chapter introduce concept.Math:\\[ lived\\_after_i =  \\beta_0 + \\beta_1 male_i + \\beta_2 c\\_election\\_age_i + \\\\ \\beta_3 male_i *  c\\_election\\_age_i + \\epsilon_i \\]outcome variable still \\(lived\\_after_i\\). want know many years candidate live election. explanatory variables . \\(male_i\\) one male candidates zero female candidates. \\(c\\_election\\_age_i\\) number years candidate lived election, relative average value candidates. model, third predictor variable: interaction \\(male_i\\) \\(c\\_election\\_age_i\\).outcome variable still \\(lived\\_after_i\\). want know many years candidate live election. explanatory variables . \\(male_i\\) one male candidates zero female candidates. \\(c\\_election\\_age_i\\) number years candidate lived election, relative average value candidates. model, third predictor variable: interaction \\(male_i\\) \\(c\\_election\\_age_i\\).\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates. sense, meaning previous model, without interaction term. , always remember meaning parameter conditional model embedded. Even parameter called \\(\\beta_0\\) two different regressions necessitate means thing regressions. Parameter names arbitrary, least simply matter convention.\\(\\beta_0\\) average number years lived election women, day election, alive average number years candidates. sense, meaning previous model, without interaction term. , always remember meaning parameter conditional model embedded. Even parameter called \\(\\beta_0\\) two different regressions necessitate means thing regressions. Parameter names arbitrary, least simply matter convention.\\(\\beta_1\\) simple interpretation stand-alone parameter. measure different women men. However, \\(\\beta_0 + \\beta_1\\) straightforward meaning exactly analogous meaning \\(\\beta_0\\). sum average number years lived election men, day election, alive average number years candidates.\\(\\beta_1\\) simple interpretation stand-alone parameter. measure different women men. However, \\(\\beta_0 + \\beta_1\\) straightforward meaning exactly analogous meaning \\(\\beta_0\\). sum average number years lived election men, day election, alive average number years candidates.\\(\\beta_2\\) coefficient \\(c\\_election\\_age_i\\). just slope women. average difference \\(lived\\_after_i\\) two women, one \\(c\\_election\\_age_i\\) value 1 greater . last example, \\(\\beta_2\\) slope whole population. Now different slopes different genders.\\(\\beta_2\\) coefficient \\(c\\_election\\_age_i\\). just slope women. average difference \\(lived\\_after_i\\) two women, one \\(c\\_election\\_age_i\\) value 1 greater . last example, \\(\\beta_2\\) slope whole population. Now different slopes different genders.\\(\\beta_3\\) alone difficult interpret. However, added \\(\\beta_2\\), result slope men.\\(\\beta_3\\) alone difficult interpret. However, added \\(\\beta_2\\), result slope men.Code:intercept increased. \\(\\beta_0\\) around 20. intercept females. still means average number years lived election women 20 . sexMale coefficient, \\(\\beta_1\\), refers value must added intercept order get average males. calculated, result 72. Keep mind, however, values apply \\(c\\_election\\_age_i = 0\\), , , candidate \\(\\) around 52 years old.coefficient \\(c\\_election\\_age_i\\), \\(\\beta_2\\), -0.1. mean? slope females. , comparing two female candidates differ one year age, expect older candidate live 0.1 years less. Now direct attention coefficient sexMale:election_age, \\(\\beta_3\\), -0.8. value must added coefficient \\(c\\_election\\_age_i\\) (recall \\(\\beta_2 + \\beta_3\\)) order find slope males. two added together, value, slope, -0.9. comparing two male candidates differ age one year, expect older candidate live 0.9 years less.Key point: interpretation intercepts apply candidates \\(c\\_election\\_age_i = 0\\). Candidates 52 years-old different expected number years live. interpretation slope applies everyone. words, relationship \\(lived\\_after_i\\) \\(c\\_election\\_age_i\\) , regardless gender old .posterior:, recommend working directly parameters. analysis much easier posterior_epred(), see Temperance Section.Male candidates live longer average female candidates. Note, also, average years live election females 20 model. previous model, 66 years. difference? interpretation “average” different! previous model, average women. model, average 52 years-old women. different things, hardly surprised different posteriors.Slope posteriors:posterior distribution shows average slope values men women. can see men steeper slope, slope women practically 0! trying forecast number years women live election, may ignore number years already lived. definitely true men. difference?",
    "code": "\nfit_4 <- stan_glm(data = ch10,\n                      formula = lived_after ~ sex*election_age,\n                      refresh = 0,\n                      seed = 13)\nprint(fit_4, detail = FALSE)##                      Median MAD_SD\n## (Intercept)          20.4   20.7  \n## sexMale              52.0   20.8  \n## election_age         -0.1    0.4  \n## sexMale:election_age -0.8    0.4  \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 11.1    0.2\nfit_4 %>% \n  as_tibble() %>% \n  mutate(male_years = `(Intercept)` + sexMale) %>% \n  rename(female_years = `(Intercept)`) %>% \n  select(female_years, male_years) %>% \n  pivot_longer(cols = female_years:male_years, \n               names_to = \"parameters\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Years Lived After the Election\",\n         subtitle = \"Men live longer\",\n         x = \"Average Years Lived Post Election\",\n         y = \"Probability\", \n         fill = \"Parameters\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\nfit_4 %>% \n  as_tibble() %>% \n  mutate(slope_men = election_age + `sexMale:election_age`) %>% \n  rename(slope_women = election_age) %>% \n  select(slope_women, slope_men) %>% \n  pivot_longer(cols = slope_women:slope_men, \n               names_to = \"parameters\",\n               values_to = \"slope\") %>% \n  ggplot(aes(slope, fill = parameters)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Slope of Years-Lived on Years-to-Live\",\n         subtitle = \"Men have a steeper slope\",\n         x = \"Slope\",\n         y = \"Probability\") + \n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic() "
  },
  {
    "path": "five-parameters.html",
    "id": "interaction-model",
    "chapter": "10 Five Parameters",
    "heading": "10.3.5 Interaction model",
    "text": "Recall parallel slopes model created Chapter 9. Another visualization can create, one also uses slopes intercepts model, interaction model. model, slopes two groups different, creating non-parallel visualization.process creating interaction model similar creating parallel slopes model. Let us begin way — tidying data inspecting .tidying data, extract values assign sensible names later use. Note identical process Chapter 9, addition fourth term (interaction term):Now extracted values, create intercept slope values two different groups, females males. Recall following details finding slopes intercepts interaction model:intercept intercept females. represents average number years lived election females.sexMale coefficient refers value must added intercept order get average years lived post-election males.coefficient \\(c\\_election\\_age_i\\) slope females.coefficient sexMale:election_age value must added coefficient \\(c\\_election\\_age_i\\) order find slope males.creating objects different intercepts slopes, now create interaction model using geom_abline() male female line.final interaction model! interesting takeaways. First, may note far fewer data points female candidates — concern previously mentioned. makes sense, , slope less dramatic compared male candidates. also see female candidates run older, compared male candidates. might explain intercept years lived post-election lower female candidates.male line seems sensible, might expect far datapoints. male candidates, see clear (logical) pattern: older candidates time election, less years post-election live. makes sense, limited human lifespan.",
    "code": "\n# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy <- fit_4 %>% \n  tidy() %>% \n  select(term, estimate)\n\ntidy## # A tibble: 4 × 2\n##   term                 estimate\n##   <chr>                   <dbl>\n## 1 (Intercept)           20.4   \n## 2 sexMale               52.0   \n## 3 election_age          -0.0753\n## 4 sexMale:election_age  -0.781\n# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept <- tidy$estimate[1]\nsex_male <- tidy$estimate[2]\nelection_age <- tidy$estimate[3]\ninteraction_term <- tidy$estimate[4]\n# Recall that the intercept and the estimate for election_age act as the\n# estimates for female candidates only. Accordingly, we have assigned those\n# values (from the previous code chunk) to more sensible names: female_intercept\n# and female_slope.\n\nfemale_intercept <- intercept\nfemale_slope <- election_age\n\n# To find the male intercept, we must add the intercept for the estimate for\n# sex_male. To find the male slope, we must add election_age to our\n# interaction term estimate.\n\nmale_intercept <- intercept + sex_male\nmale_slope <- election_age + interaction_term\n# From the ch10 data, create a ggplot object with election_age as the x-axis\n# and lived_after as the y-axis. We will use color = sex.\n\nggplot(ch10, aes(x = election_age, y = lived_after, color = sex)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the female intercept and slope. Set the\n  # intercept qual to our previously created female_intercept, while setting\n  # slope equal to our previously created female_slope. The color call is for\n  # coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = female_intercept,\n              slope = female_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the male values. Set the intercept equal to\n  # our previously created male_intercept, while setting slope equal to our\n  # previously created male_slope. The color call is for teal, to match the\n  # colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = male_intercept,\n              slope = male_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Interaction Model\",\n       subtitle = \"Comparing post election lifespan across sex\",\n       x = \"Average Age at Time of Election\", \n       y = \"Years Lived Post-Election\", \n       color = \"Sex\") +\n  theme_classic()"
  },
  {
    "path": "five-parameters.html",
    "id": "temperance-8",
    "chapter": "10 Five Parameters",
    "heading": "10.4 Temperance",
    "text": "\nFIGURE 10.4: Temperance\nRecall questions began chapter:long two political candidates — one male one female, 10 years older average candidate — live election? different lifespans ?questions , purposely, less precise ones tackled Chapters 7 8, written conversational style. normal people talk.However, data scientists, job bring precision questions. two commonsense interpretations. First, curious expected values questions. averaged data thousand candidates like , answer ? Second, curious two specific individuals. long live? Averages involve questions parameters. fates individuals require predictions. general claims, violated often firm rules. Yet, highlight key point: expected values less variable individual predictions.calculate expected values, use posterior_epred(). forecast individuals, use posterior_predict().",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "expected-values",
    "chapter": "10 Five Parameters",
    "heading": "10.4.1 Expected values",
    "text": "Consider “average” interpretation first. answer begins posterior distributions parameters fit_4.Looking posterior probability distributions , can see male candidates expected live longer. much longer? previous chapters, can manipulate distributions , less, way manipulate simple numbers. want know difference two posterior distributions, can simply subtract.average value difference years--live probably positive, likely value around 45 years. still 1% chance true value less zero, .e., expect female candidates live longer.Instead using posterior_epred(), answered questions using posterior probability distributions parameters model, along simple math. Don’t ! First, much likely make mistake. Second, approach generalize well complex models scores parameters interactions.",
    "code": "\nnewobs = tibble(sex = c(\"Male\", \"Female\"), \n                 election_age = 10)\n\npe <- posterior_epred(object = fit_4, \n                      newdata = newobs) %>% \n  as_tibble() %>% \n  rename(\"Male\" = `1`,\n         \"Female\" = `2`)\n\n\npe %>% \n pivot_longer(cols = Male:Female, \n               names_to = \"Gender\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = Gender)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Years Lived Post-Election\",\n         subtitle = \"Male candidates live longer\",\n         x = \"Years\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = \n                         scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\npe <- posterior_epred(object = fit_4, \n                      newdata = newobs) %>% \n  as_tibble() %>% \n  mutate(diff = `1` - `2`)\n\n\npe %>% \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Additional Male Years Lived\",\n         subtitle = \"Male candidates live about 4 years longer\",\n         x = \"Expected Additional Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "path": "five-parameters.html",
    "id": "individual-predictions",
    "chapter": "10 Five Parameters",
    "heading": "10.4.2 Individual predictions",
    "text": ", instead, interpret question asking prediction small number individuals, need use posterior_predict().Use posterior_predict() create draws posterior probability distribution prediction cases. posterior_predict() takes two arguments: model simulations run, tibble indicating covariate values individual(s) want predict. case, using fit_4 model tibble one just created . words, inputs posterior_predict() posterior_epred() identical.resulting tibble 2 columns, first male candidate second female candidate. columns draws posterior predictive distributions. cases, forecasts depend values covariates. , provide different forecast candidates younger older.need weird mutate_all(.numeric) incantation? reason posterior_epred() returns simple matrix, easy transform tibble. posterior_predict(), hand, returns special sort matrix much harder work . , need little hackery make next steps easier.Let’s look posterior predictive distribution candidate.big overlap predictions individuals , time, much less overlap averages. Random stuff happens individual time. Random stuff cancels take average many individuals. Consider difference posterior predictive distributions two individuals.words, predict male candidate live longer female candidate. much? Well, number unknown parameter. looking posterior , best estimate 44.6 years. However, quite possible , given male/female candidates, female live longer.fact, 4 10 chance female candidate lives longer.Note different move question averages question individuals. cases, likely value . , average behavior expected value given individual. uncertainty much greater individual prediction. chance true average male candidates less female candidates low. Yet, individual pair candidates, even slightly surprising female candidate outlive male candidate. Individuals vary. Averages never tell whole story.",
    "code": "\npp <- posterior_predict(object = fit_4, \n                        newdata = newobs) %>%\n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  rename(\"Male\" = `1`,\n         \"Female\" = `2`)\n\npp  ## # A tibble: 4,000 × 2\n##     Male Female\n##    <dbl>  <dbl>\n##  1  80.0 41.1  \n##  2  58.3 15.4  \n##  3  65.2 17.0  \n##  4  74.0 38.4  \n##  5  46.0 67.8  \n##  6  56.6 20.4  \n##  7  54.4  0.763\n##  8  74.7  2.93 \n##  9  46.5 12.5  \n## 10  69.1 -1.22 \n## # … with 3,990 more rows\npp %>% \n  pivot_longer(cols = Male:Female, \n               names_to = \"Gender\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = Gender)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for a Candidate's Years Lived Post-Election\",\n         subtitle = \"Individual lifespans have a great deal of variation\",\n         x = \"Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\npp %>% \n  mutate(diff = Male - Female) %>% \n  ggplot(aes(diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for a Male Candidate's Extra Years Lived\", \n         subtitle = \"Any random male candidate may die before a random female candidate\",\n         x = \"Years\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\npp %>% \n  mutate(diff = Male - Female) %>% \n  summarize(f_live_longer = sum(diff < 0),\n            total = n(),\n            f_live_longer / total)## # A tibble: 1 × 3\n##   f_live_longer total `f_live_longer/total`\n##           <int> <int>                 <dbl>\n## 1           107  4000                0.0268"
  },
  {
    "path": "five-parameters.html",
    "id": "expectation-versus-individual-variation",
    "chapter": "10 Five Parameters",
    "heading": "10.4.3 Expectation versus individual variation",
    "text": "Let’s compare results posterior_epred() posterior_predict() scenario directly. code shown , think useful look everything together.Expected values vary much less predictions. chart makes easy see. somewhat sure true underlying average numbers years male candidates live post-election female candidates. , two individual candidates, good chance female candidate live longer. can ignore \\(\\epsilon\\) predicting outcome individuals. estimating expected values long-run averages, \\(\\epsilon\\)’s cancel .",
    "code": "\nnewobs <- tibble(sex = c(\"Male\", \"Female\"),\n                  election_age = 10)\n\npe <- posterior_epred(fit_4, \n                      newdata = newobs) %>%\n  as_tibble() %>% \n  mutate(diff = `1` - `2`)\n\npp <- posterior_predict(fit_4, \n                        newdata = newobs) %>% \n  as_tibble() %>% \n  mutate_all(as.numeric) %>% \n  mutate(diff = `1` - `2`)\n\ntibble(Expectation = pe$diff,\n       Prediction = pp$diff) %>% \n  pivot_longer(cols = Expectation:Prediction, \n               names_to = \"Type\",\n               values_to = \"years\") %>% \n  ggplot(aes(years, fill = Type)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected and Individual Male Advantage\",\n         subtitle = \"Expected male advantage is much more precisely estimated\",\n         x = \"Additional Years Lived Post Election\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()"
  },
  {
    "path": "five-parameters.html",
    "id": "testing-1",
    "chapter": "10 Five Parameters",
    "heading": "10.4.4 Testing",
    "text": "“Tests,” “testing,” “hypothesis tests,” “tests significance,” “null hypothesis significance testing” refer concept. refer collection approaches NHST, common abbreviation derived initials last phrase. Wikipedia provides overview.hypothesis testing, null hypothesis — hypothesis represents particular probability model. also alternative hypothesis, typically alternative null hypothesis. Let’s look example unrelated statistics first.Imagine criminal trial held United States. criminal justice system assumes “defendant innocent proven guilty.” , initial assumption defendant innocent.Null hypothesis (\\(H_0\\)): Defendent guilty (innocent)\nAlternative hypothesis (\\(H_a\\)): Defendant guiltyIn statistics, always assume null hypothesis true. , null hypothesis always initial assumption.collect evidence — finger prints, blood spots, hair samples — hopes finding “sufficient evidence” make assumption innocence refutable.statistics, data evidence.jury makes decision based available evidence:jury finds sufficient evidence — beyond reasonable doubt — make assumption innocence refutable, jury rejects null hypothesis deems defendant guilty. behave defendant guilty. insufficient evidence, jury reject null hypothesis. behave defendant innocent.statistics, always make one two decisions. either reject null hypothesis fail reject null hypothesis. Rather collect physical evidence, test hypothesis model. example, say hypothesis certain parameter equals zero. hypotheses :\\(H_0\\): parameter equals 0.\n\\(H_a\\): parameter equal 0.hypothesis parameter equals zero (fixed value) can directly tested fitting model includes parameter question examining corresponding 95% interval. 95% interval excludes zero (specified fixed value), hypothesis said rejected. 95% interval inclues zero, reject hypothesis. also accept hypothesis.sounds nonsensical, ’s . view: Amateurs test. Professionals summarize.Yes/question throws away much information (almost) ever useful. reason test can summarize providing full posterior probability distribution.arguments apply case “insignificant” results can’t “reject” null hypothesis. simple terms: cares!? full posterior probability distribution prediction — also known posterior predictive distribution — graphed . fact result “significant” relevance use posterior make decisions.reasoning applies every parameter estimate, every prediction make. Never test — unless boss demands test. Use judgment, make models, summarize knowledge world, use summary make decisions.",
    "code": ""
  },
  {
    "path": "five-parameters.html",
    "id": "summary-11",
    "chapter": "10 Five Parameters",
    "heading": "10.5 Summary",
    "text": "major part Wisdom deciding questions can’t answer data don’t .Avoid answering questions working parameters directly. Use posterior_epred() instead.Good data science involves intelligent tour space possible models.Always think terms comparisons using predictive model.Spend less time thinking parameters mean time using posterior_epred() posterior_predict() examine implications models.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "n-parameters",
    "chapter": "11 N Parameters",
    "heading": "11 N Parameters",
    "text": "chapter still DRAFT.may time move assuming research honestly conducted reported assuming untrustworthy evidence contrary. – Richard Smith, former editor British Medical Journal.created models one parameter Chapter 6, two parameters Chapter 7, three parameters Chapter 8, four parameters Chapter 9 five parameters Chapter 10, now ready make jump \\(N\\) parameters.chapter, consider models many parameters complexities arise therefrom. models grow complexity, need pay extra attention basic considerations like validity, population, representativeness. easy jump right start interpreting! harder, necessary, ensure models really answering questions.Imagine running Governor Texas want better job getting voters vote. can encourage voters go polls election day?",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "wisdom-6",
    "chapter": "11 N Parameters",
    "heading": "11.1 Wisdom",
    "text": "\nFIGURE 11.1: Wisdom\nresearch ways increase voting, come across large-scale experiment showing effect sending voting reminder “shames” citizens vote. considering sending “shaming” voting reminder .looking shaming dataset primer.data package. dataset “Social Pressure Voter Turnout: Evidence Large-Scale Field Experiment” Gerber, Green, Larimer (2008). Check paper . can, , familiarize data typing ?shaming.Recall initial question: can encourage voters go polls election day? now need translate precise question, one can answer data.question:causal effect, likelihood voting, different postcards voters different levels political engagement?",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "ideal-preceptor-table",
    "chapter": "11 N Parameters",
    "heading": "11.1.1 Ideal Preceptor Table",
    "text": "Recall ideal Preceptor Table. rows columns data need , , calculation number interest trivial? want know average height adult India, ideal Preceptor Table include row adult India column height.One key aspect ideal Preceptor Table whether need one potential outcome order calculate estimand. Mainly: need causal model, one estimates attitude treatment control? Preceptor Table require two columns outcome. case, trying see causal effect mailed voting reminders voting.modeling (just) prediction (also) modeling causation? Predictive models care nothing causation. Causal models often also concerned prediction, means measuring quality model. , looking causation., ideal table look like? Assuming running governor United States, ideally data every citizen voting age. means approximately 200 million rows.missing data ideal Preceptor Table, also know outcomes treatment (receiving reminder) control (receiving reminder). sample row table:ideal table, rows American citizens voting age. good start! However, may want even information ideal Preceptor Table. Perhaps column sex informative. column age? Political affiliation? perfect world, know pieces information. perfect world, measure exact causal effect voting reminders different subsets US population.may also want narrow ideal Preceptor Table. running governor Florida, may want study citizens Florida. running Democrat, may want study citizens registered Democrats.However, main point exercise see want know compared actually know.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "eda-of-shaming",
    "chapter": "11 N Parameters",
    "heading": "11.1.2 EDA of shaming",
    "text": "loading packages need, let’s perform EDA, starting running glimpse() shaming tibble primer.data package.glimpse() gives us look raw data contained within shaming data set. top output, can see number rows columns, observations variables respectively. see 344,084 observations, row corresponding unique respondent. summary provides idea variables working .Variables particular interest us sex, hh_size, primary_06. variable hh_size tells us size respondent’s household, sex tells us sex respondent, primary_06 tells us whether respondent voted 2006 Primary election.\nthings note exploring data set. may – may – noticed response general_04 variable “Yes”. published article, authors note “registered voters voted November 2004 selected sample” (Gerber, Green, Larimer, 2008). , authors found history sent mailings. Thus, non-registered voters excluded data.also important identify dependent variable meaning. shaming experiment, dependent variable primary_06, variable coded either 0 1 whether respondent voted 2006 primary election. dependent variable authors trying measure effect treatments voting behavior 2006 general election.yet discussed important variable : treatment. treatment variable factor variable 5 levels, including control. Since curious sending mailings affects voter turnout, treatment variable tell us impact type mailing can make. Let’s start taking broad look different treatments.Four types treatments used experiment, voters receiving one four types mailing. mailing treatments carried message, “CIVIC DUTY - VOTE!”.first treatment, Civic Duty, also read, “Remember rights responsibilities citizen. Remember vote.” message acted baseline treatments, since carried message similar one displayed mailings.second treatment, Hawthorne, households received mailing told voters studied voting behavior examined public records. adds small amount social pressure households receiving mailing.third treatment, Self, mailing includes recent voting record member household, placing word “Voted” next name fact vote 2004 election blank space next name . mailing, households also told, “intend mail updated chart” voting record household members 2006 primary. emphasizing public nature voting records, type mailing exerts social pressure voting Hawthorne treatment.fourth treatment, Neighbors, provides household members’ voting records, well voting records live nearby. mailing also told recipients, “intend mail updated chart” voted 2006 election entire neighborhood.",
    "code": "\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(ggthemes)\nlibrary(ggdist)\nlibrary(gt)\nlibrary(janitor)\nlibrary(broom.mixed)\nlibrary(gtsummary)\nglimpse(shaming)## Rows: 344,084\n## Columns: 15\n## $ cluster       <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n## $ primary_06    <int> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,…\n## $ treatment     <fct> Civic Duty, Civic Duty, Hawthorne, Hawthorne, Hawthorne,…\n## $ sex           <chr> \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\", \"F…\n## $ age           <int> 65, 59, 55, 56, 24, 25, 47, 50, 38, 39, 65, 61, 57, 37, …\n## $ primary_00    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n## $ general_00    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n## $ primary_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n## $ general_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n## $ primary_04    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n## $ general_04    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n## $ hh_size       <int> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1,…\n## $ hh_primary_04 <dbl> 0.095, 0.095, 0.048, 0.048, 0.048, 0.048, 0.048, 0.048, …\n## $ hh_general_04 <dbl> 0.86, 0.86, 0.86, 0.86, 0.86, 0.90, 0.90, 0.90, 0.90, 0.…\n## $ neighbors     <int> 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, …\nshaming |>\n  count(treatment)## # A tibble: 5 × 2\n##   treatment        n\n##   <fct>        <int>\n## 1 No Postcard 191243\n## 2 Civic Duty   38218\n## 3 Hawthorne    38204\n## 4 Self         38218\n## 5 Neighbors    38201"
  },
  {
    "path": "n-parameters.html",
    "id": "population-3",
    "chapter": "11 N Parameters",
    "heading": "11.1.3 Population",
    "text": "One important components Wisdom concept “population”. Recall questions asked earlier:discussed , population set people, voters, data. dataset. set voters like data. rows ideal Preceptor Table. population larger — potentially much larger — set individuals include data data want. Generally, population much larger either data data want.case, viewing data perspective someone running Governor year wants increase voter turnout. want increase turnout now, people voting 2006! also may want increase turnout citizens registered vote, group excluded dataset. reasonable generate conclusions group? likely, . However, limited data work determine far willing generalize groups.judgment call, matter Wisdom, whether may assume data data want (.e., ideal Preceptor Table) drawn population.Even though original question “voters” general, specifically refer specific states might interested, assume data random voters , uh, representative enough population interested . believe , stop right now. major part Wisdom deciding questions can’t answer data just don’t .",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "justice-9",
    "chapter": "11 N Parameters",
    "heading": "11.2 Justice",
    "text": "\nFIGURE 11.2: Justice\nJustice emphasizes key concepts:actual Preceptor Table, structure includes row every unit population. generally break rows actual Preceptor Table three categories: data units want , data units actually , data units care .data representative population?meaning columns consistent, .e., can assume validity?make assumption data generating mechanism.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "preceptor-table-3",
    "chapter": "11 N Parameters",
    "heading": "11.2.1 Preceptor Table",
    "text": "Recall actual Preceptor Table, bunch missing data! can use simple arithmetic calculate causal effect voting reminders voting behavior. Instead, required estimate . estimand, variable real world trying measure. estimand value calculated, rather unknown variable want estimate.Let’s build basic visualization actual Preceptor Table scenario:Citizen 1Voted??Citizen 23Did vote??Citizen 40?Voted?Citizen 53?vote?Citizen 80Voted??, two possible outcomes: vote vote. really want know Average Treatment Effect (ATE) treatment, voting reminder. want estimate much voting reminder impacts odds someone voting.Note simplified version actual Preceptor Table. dataset, number columns know subjects: age, sex, past voting history. expanded actual Preceptor Table, columns included.Now, can fill question marks? Fundamental Problem Causal Inference, can never know missing values. can never know missing values, must make assumptions. “Assumption” just means need “model,” models parameters.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "the-population-table-1",
    "chapter": "11 N Parameters",
    "heading": "11.2.2 The Population Table",
    "text": "Population Table shows data actually desired population. shows rows three sources: ideal Preceptor Table, data, population outside data (rows exist data).ideal Preceptor rows, information covariates sex, year, state. However, rows included data, outcome results. Since scenario pertains upcoming election Texas, state column read Texas year column read 2021.rows data everything: covariates outcomes. covariates Michigan state 2006 year, since pieces information included data. course, still values Treatment minus Control, since observe one subject two conditions.rows population data. subjects fall desired population, data. , rows missing.Population?1990????Population?1995????.....................DataMale2006MichiganDid vote??DataFemale2006Michigan?Voted?.....................Population?2010????Population?2012????.....................Preceptor TableFemale2021Texas???Preceptor TableFemale2021Texas???.....................Population?2026????Population?2030????",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "validity-3",
    "chapter": "11 N Parameters",
    "heading": "11.2.3 Validity",
    "text": "understand validity regards Population Table, must first recognize inherent flaw experiment design: two units receive exactly treatment.might thinking: well, surely two postcards ? , aren’t! postcards sent data sent information relevant 2006 — different candidate, different language, different syntax. postcard sent 2021, even used exact language, encouraging new candidate new reform differing policies.Thus, despite fact two units treatment — , receiving postcard — different versions treatment. Indeed, infinite number possible treatments. important define estimand clearly.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "stability-2",
    "chapter": "11 N Parameters",
    "heading": "11.2.4 Stability",
    "text": "Stability means relationship columns three categories rows: data, Preceptor table, larger population drawn.something like height, much easier assume stability greater period time. Changes global height occur extremely slowly, height stable across span 20 years reasonable assume. Can say example, looking voting behavior?data collected 2006 voting behavior likely 2021? Frankly, don’t know! aren’t sure impact someone’s response postcard encouraging vote. possible, instance, postcard informing neighbors voting status effect voting behavior pandemic, closely interacting neighbors.confronted uncertainty, can consider making timeframe smaller. However, still need assume stability 2006 (time data collection) today. Stability allows us ignore issue time.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "representativeness-3",
    "chapter": "11 N Parameters",
    "heading": "11.2.5 Representativeness",
    "text": "good time consider really means accept data representative population. mind, let’s break real, current question:running governor Texas year 2021. year United States, consider sending voting reminder postcard citizens voting age. reminder encourage voting, much?Now, let’s break data shaming dataset:data gathered Michigan prior August 2006 primary election. population experiment 180,002 households state Michigan. data included voted 2004 general election. Therefore, include non-voters. reminders mailed households random., similar groups? Let’s start differences.\n* data 2006. question asking answers 2021. small gap time. lot changes decade half!\n* data excludes non-voters last election. question, seeks increase voting turnout citizens, want non-voters included. , can make claims citizens? Probably .\n* data includes voters Michigan. want make inferences Texas, perhaps United States whole. within reason ?Generally: chance certain type person experiment, make assumption person.purpose section make us think critically assumptions making whether assumptions can reasonably made. Though continue using dataset remainder chapter, clear must make predictions caution.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "functional-form-1",
    "chapter": "11 N Parameters",
    "heading": "11.2.6 Functional form",
    "text": "",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "courage-9",
    "chapter": "11 N Parameters",
    "heading": "11.3 Courage",
    "text": "\nFIGURE 11.3: Courage\n",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "set-up",
    "chapter": "11 N Parameters",
    "heading": "11.3.1 Set-up",
    "text": "Now, create object named object_1 includes 3-level factor classifying voters level civic engagement.Convert primary general election variables already 1/0 binary binary format.Convert primary general election variables already 1/0 binary binary format.Create new column named civ_engage sums person’s voting behavior , including, 2006 primary.Create new column named civ_engage sums person’s voting behavior , including, 2006 primary.Create column named voter_class classifies voters 3 bins: “Always Vote” voted least 5 times, “Sometimes Vote” voted 3 4 times, “Rarely Vote” voted 2 fewer times. variable classified factor.Create column named voter_class classifies voters 3 bins: “Always Vote” voted least 5 times, “Sometimes Vote” voted 3 4 times, “Rarely Vote” voted 2 fewer times. variable classified factor.Create column called z_age z-score age.Create column called z_age z-score age.Let’s inspect object:Great! Now, create first model: relationship primary_06, represents whether citizen voted , sex treatment.",
    "code": "\nobject_1 <- shaming |> \n  \n  # Converting the Y/N columns to binaries with the function we made \n  # note that primary_06 is already binary and also that we don't \n  # need it to predict construct previous voter behavior status variable.\n  \n  mutate(p_00 = (primary_00 == \"Yes\"),\n         p_02 = (primary_02 == \"Yes\"),\n         p_04 = (primary_04 == \"Yes\"),\n         g_00 = (general_00 == \"Yes\"),\n         g_02 = (general_02 == \"Yes\"),\n         g_04 = (general_04 == \"Yes\")) |> \n  \n  # A sum of the voting action records across the election cycle columns gives\n  # us an idea (though not weighted for when across the elections) of the voters\n  # general level of civic involvement.\n  \n  mutate(civ_engage = p_00 + p_02 + p_04 + \n                      g_00 + g_02 + g_04) |> \n  \n  # If you look closely at the data, you will note that g_04 is always Yes, so\n  # the lowest possible value of civ_engage is 1. The reason for this is that\n  # the sample was created by starting with a list of everyone who voted in the\n  # 2004 general election. Note how that fact makes the interpretation of the\n  # relevant population somewhat subtle.\n  \n  mutate(voter_class = case_when(civ_engage %in% c(5, 6) ~ \"Always Vote\",\n                                 civ_engage %in% c(3, 4) ~ \"Sometimes Vote\",\n                                 civ_engage %in% c(1, 2) ~ \"Rarely Vote\"),\n         voter_class = factor(voter_class, levels = c(\"Rarely Vote\", \n                                                      \"Sometimes Vote\", \n                                                      \"Always Vote\"))) |> \n  \n  # Centering and scaling the age variable. Note that it would be smart to have\n  # some stopifnot() error checks at this point. For example, if civ_engage < 1\n  # or > 6, then something has gone very wrong.\n  \n  mutate(z_age = as.numeric(scale(age))) |> \n  select(primary_06, treatment, sex, civ_engage, voter_class, z_age)\nobject_1 |> \n  slice(1:3)## # A tibble: 3 × 6\n##   primary_06 treatment  sex    civ_engage voter_class    z_age\n##        <int> <fct>      <chr>       <int> <fct>          <dbl>\n## 1          0 Civic Duty Male            4 Sometimes Vote 1.05 \n## 2          0 Civic Duty Female          4 Sometimes Vote 0.638\n## 3          1 Hawthorne  Male            4 Sometimes Vote 0.361"
  },
  {
    "path": "n-parameters.html",
    "id": "primary_06-treatment-sex",
    "chapter": "11 N Parameters",
    "heading": "11.3.2 primary_06 ~ treatment + sex",
    "text": "section, look relationship primary voting treatment + sex.math:Without variable names:\\[ y_{} = \\beta_{0} + \\beta_{1}x_{, 1} + \\beta_{2}x_{,2} ... + \\beta_{n}x_{,n} + \\epsilon_{} \\]\nvariable names:\\[ y_{} = \\beta_{0} + \\beta_{1}civic\\_duty_i + \\beta_{2}hawthorne_i + \\beta_{3}self_i + \\beta_{4}neighbors_i + \\beta_{5}male_i + \\epsilon_{} \\]two ways formalize model used fit_1: without variable names. former related concept Justice acknowledge model constructed via linear sum n parameters times value n variables, along error term. words, linear model. model learned semester logistic model, kinds models, defined mathematics assumptions error term.\nsecond type formal notation, associated virtue Courage, includes actual variable names using. trickiest part transformation character/factor variables indicator variables, meaning variables 0/1 values. treatment 5 levels, need 4 indicator variables. fifth level — , default, first variable alphabetically (character variables) first level (factor variables) — incorporated intercept.Let’s translate model code.now create table nicely formats results fit_1 using tbl_regression() function gtsummary package. also display associated 95% confidence interval coefficient.Interpretation:\n* intercept model expected value probability someone voting 2006 primary given part control group female. case, estimate women control group vote ~29.1% time.\n* coefficient sexMale indicates difference likelihood voting male female. words, comparing men women, 0.01 implies men ~1.2% likely vote women. Note , linear model interactions sex variables, difference applies male, regardless treatment received. sex can manipulated (assumption), use causal interpretation coefficient.\n* coefficients treatments, hand, causal interpretation. single individual, either sex, sent Self postcard increases probability voting 4.8%. appears Neighbors treatment effective ~8.1% Civic Duty least effective ~1.8%.",
    "code": "\nfit_1 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ treatment + sex,\n                  refresh = 0,\n                  seed = 987)\nprint(fit_1, digits = 3)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      primary_06 ~ treatment + sex\n##  observations: 344084\n##  predictors:   6\n## ------\n##                     Median MAD_SD\n## (Intercept)         0.291  0.001 \n## treatmentCivic Duty 0.018  0.003 \n## treatmentHawthorne  0.026  0.003 \n## treatmentSelf       0.048  0.002 \n## treatmentNeighbors  0.081  0.003 \n## sexMale             0.012  0.002 \n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.464  0.001 \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\ntbl_regression(fit_1, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |>\n  \n  # Using Beta as the name of the parameter column is weird.\n  \n  as_gt() |>\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Age Predict Likelihood of Voting\") |>\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |> \n  cols_label(estimate = md(\"**Parameter**\"))## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead\n\n## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead\n\n## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead"
  },
  {
    "path": "n-parameters.html",
    "id": "primary_06-z_age-sex-treatment-voter_class-voter_classtreatment",
    "chapter": "11 N Parameters",
    "heading": "11.3.3 primary_06 ~ z_age + sex + treatment + voter_class + voter_class*treatment",
    "text": "time look interactions! Create another model named fit_2 estimates primary_06 function z_age, sex, treatment, voter_class, interaction treatment voter classification.math:\n\\[y_{} = \\beta_{0} + \\beta_{1}z\\_age + \\beta_{2}male_i + \\beta_{3}civic\\_duty_i + \\\\ \\beta_{4}hawthorne_i + \\beta_{5}self_i + \\beta_{6}neighbors_i + \\\\ \\beta_{7}Sometimes\\ vote_i + \\beta_{8}Always\\ vote_i + \\\\ \\beta_{9}civic\\_duty_i Sometimes\\ vote_i + \\beta_{10}hawthorne_i Sometimes\\ vote_i + \\\\ \\beta_{11}self_i Sometimes\\ vote_i + \\beta_{11}neighbors_i Sometimes\\ vote_i + \\\\ \\beta_{12}civic\\_duty_i Always\\ vote_i + \\beta_{13}hawthorne_i Always\\ vote_i + \\\\ \\beta_{14}self_i Always\\ vote_i + \\beta_{15}neighbors_i Always\\ vote_i + \\epsilon_{}\\]\nTranslate code:first model, create regression table observe findings:Now summarized visual data, let’s interpret findings:\n* intercept fit_2 expected probability voting upcoming election woman average age (~ 50 years old data), assigned Postcard group, Rarely Voter. \nestimate 15.3%.\n* coefficient z_age, 0, implies change ~3.5% likelihood voting increment one standard deviation (~ 14.45 years). example: comparing someone 50 years old someone 65, latter 3.5% likely vote.\n* Exposure Neighbors treatment shows ~4.4% increase voting likelihood someone Rarely Vote category. random assignment treatment, can interpret coefficient estimate\naverage treatment effect.\n* someone different voter classification, calculation complex need account interaction term. example, individuals Sometimes Vote, treatment effect Neighbors 0.1%. Always Vote Neighbors, 0.1%.",
    "code": "\nfit_2 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ z_age + sex + treatment + voter_class + \n                            treatment*voter_class,\n                  family = gaussian,\n                  refresh = 0,\n                  seed = 789)\nprint(fit_2, digits = 3)## stan_glm\n##  family:       gaussian [identity]\n##  formula:      primary_06 ~ z_age + sex + treatment + voter_class + treatment * \n##     voter_class\n##  observations: 344084\n##  predictors:   17\n## ------\n##                                               Median MAD_SD\n## (Intercept)                                    0.153  0.003\n## z_age                                          0.035  0.001\n## sexMale                                        0.008  0.002\n## treatmentCivic Duty                            0.010  0.007\n## treatmentHawthorne                             0.007  0.007\n## treatmentSelf                                  0.023  0.007\n## treatmentNeighbors                             0.044  0.007\n## voter_classSometimes Vote                      0.114  0.003\n## voter_classAlways Vote                         0.294  0.004\n## treatmentCivic Duty:voter_classSometimes Vote  0.014  0.008\n## treatmentHawthorne:voter_classSometimes Vote   0.019  0.007\n## treatmentSelf:voter_classSometimes Vote        0.030  0.007\n## treatmentNeighbors:voter_classSometimes Vote   0.042  0.008\n## treatmentCivic Duty:voter_classAlways Vote    -0.001  0.009\n## treatmentHawthorne:voter_classAlways Vote      0.025  0.009\n## treatmentSelf:voter_classAlways Vote           0.026  0.008\n## treatmentNeighbors:voter_classAlways Vote      0.047  0.009\n## \n## Auxiliary parameter(s):\n##       Median MAD_SD\n## sigma 0.451  0.001 \n## \n## ------\n## * For help interpreting the printed output see ?print.stanreg\n## * For info on the priors used see ?prior_summary.stanreg\ntbl_regression(fit_2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |>\n  as_gt() |>\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Other Variables Predict Likelihood of Voting\") |>\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |> \n  cols_label(estimate = md(\"**Parameter**\"))## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead\n\n## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead\n\n## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead"
  },
  {
    "path": "n-parameters.html",
    "id": "temperance-9",
    "chapter": "11 N Parameters",
    "heading": "11.4 Temperance",
    "text": "\nFIGURE 11.4: Temperance\nFinally, let’s remember virtue Temperance. gist temperance : humble inferences, inferences always, certainly, unfortunately going match real world. apply shaming scenario?Recall initial question: causal effect, likelihood voting, different postcards voters different levels political engagement?answer question, want look different average treatment effects treatment type voting behavior. real world, treatment effect person almost always different treatment effect person B.section, create plot displays posterior probability distributions average treatment effects men average age across combinations 4 treatments 3 voter classifications. means making total 12 inferences.Important note: look lots ages Male Female subjects. However, change estimates treatment effects. model linear, terms associated z_age sex disappear subtraction. one great advantages linear models.begin, need create newobs object.Now newobs work , need create object named plot_data collects treatment effect calculations.Recall , calculating treatment effect, need subtract estimate category control group category. example, wanted find treatment effect Always Vote Neighbors group, need: Always Vote Neighbors - Always Vote Postcard.Therefore, use mutate() twelve times, treatments voting frequencies. , pivot_longer order treatment effects sensibly categorized plotting. sounds confusing, read code comments carefully.Finally, plot data! Read code comments explanations aesthetic choices, well helpful discussion fct_reorder().interesting! shows us valuable bits information:interested average treatment effect postcards. 4 different postcards, can compared happened voter receive postcard.four treatment effects, however, heterogeneous. vary depending individual’s voting history, organize three categories: Rarely Vote, Sometimes Vote Always Vote. , 12 different\naverage treatment effects, one possible combination postcard voting history.combinations, graphic shows posterior distribution.mean us, consider postcards send?\n* Consider highest yellow distribution, posterior distribution average treatment effect receiving Neighbors postcard (compared getting postcard) Always Voters. posterior centered around 9% 95% confidence interval , roughly, 8% 10%.\n* Overall, Civic Duty Hawthorne postcards small average treatment effects, across three categories voter. causal effect Rarely Voters much smaller, regardless treatment. also much less precisely estimated many fewer Rarely Voters data.\n*best way increase turnover, assuming limits many postcards can send, focus Sometimes/Always voters use Neighbors postcard.Conclusion: limited number postcards, send Neighbors postcard citizens already demonstrate tendency vote.confident findings? needed convince boss right strategy, need explain confident assumptions. , must understand three levels knowledge world posteriors.",
    "code": "\n# Because our model is linear, the terms associated with z_age and sex disappear\n# when we perform subtraction. The treatment effects calculated thereafter will\n# not only apply to males of the z-scored age of ~ 50 years. The treatment\n# effects apply to all participants, despite calling these inputs.\n\n\nsex <- \"Male\"\nz_age <- 0\ntreatment <- c(\"No Postcard\",\n               \"Civic Duty\",\n               \"Hawthorne\",\n               \"Self\",\n               \"Neighbors\")\nvoter_class <- c(\"Always Vote\",\n                 \"Sometimes Vote\",\n                 \"Rarely Vote\")\n\n# This question requires quite the complicated tibble! Speaking both\n# hypothetically and from experience, keeping track of loads of nondescript\n# column names after running posterior_epred() while doing ATE calculations\n# leaves you prone to simple, but critical, errors. expand_grid() was created\n# for cases just like this - we want all combinations of treatments and voter\n# classifications in the same way that our model displays the interaction term\n# parameters.\n\nnewobs <- expand_grid(sex, z_age, treatment, voter_class) |> \n  \n  # This is a handy setup for the following piece of code that allows us to\n  # mutate the ATE columns with self-contained variable names. This is what\n  # helps to ensure that the desired calculations are indeed being done. If you\n  # aren't familiar, check out the help page for paste() at `?paste`.\n  \n  mutate(names = paste(treatment, voter_class, sep = \"_\"))\n\npe <- posterior_epred(fit_2,\n                        newdata = newobs) |> \n  as_tibble() |> \n  \n  # Here we can stick the names that we created in newobs onto the otherwise\n  # unfortunately named posterior_epred() output. \n  \n  set_names(newobs$names)\nplot_data <- pe |> \n  \n  # Using our cleaned naming system, ATE calculations are simple enough. Note\n  # how much easier the code reads because we have taken the trouble to line up\n  # the columns.\n  \n  mutate(`Always Civic-Duty`    = `Civic Duty_Always Vote`     - `No Postcard_Always Vote`,\n         `Always Hawthorne`     = `Hawthorne_Always Vote`      - `No Postcard_Always Vote`,\n         `Always Self`          = `Self_Always Vote`           - `No Postcard_Always Vote`,\n         `Always Neighbors`     = `Neighbors_Always Vote`      - `No Postcard_Always Vote`,\n         `Sometimes Civic-Duty` = `Civic Duty_Sometimes Vote`  - `No Postcard_Sometimes Vote`,\n         `Sometimes Hawthorne`  = `Hawthorne_Sometimes Vote`   - `No Postcard_Sometimes Vote`,\n         `Sometimes Self`       = `Self_Sometimes Vote`        - `No Postcard_Sometimes Vote`,\n         `Sometimes Neighbors`  = `Neighbors_Sometimes Vote`   - `No Postcard_Sometimes Vote`,\n         `Rarely Civic-Duty`    = `Civic Duty_Rarely Vote`     - `No Postcard_Rarely Vote`,\n         `Rarely Hawthorne`     = `Hawthorne_Rarely Vote`      - `No Postcard_Rarely Vote`,\n         `Rarely Self`          = `Self_Rarely Vote`           - `No Postcard_Rarely Vote`,\n         `Rarely Neighbors`     = `Neighbors_Rarely Vote`      - `No Postcard_Rarely Vote`) |> \n  \n  # This is a critical step, we need to be able to reference voter\n  # classification separately from the treatment assignment, so pivoting in the\n  # following manner reconstructs the relevant columns for each of these\n  # individually. \n  \n  pivot_longer(names_to = c(\"Voter Class\", \"Group\"),\n               names_sep = \" \",\n               values_to = \"values\",\n               cols = `Always Civic-Duty`:`Rarely Neighbors`) |> \n  \n    # Reordering the factors of voter classification forces them to be displayed\n    # in a sensible order in the plot later.\n  \n    mutate(`Voter Class` = fct_relevel(factor(`Voter Class`),\n                                     c(\"Rarely\",\n                                       \"Sometimes\",\n                                       \"Always\")))\nplot_data  |> \n  \n  # Reordering the y axis values allows a smoother visual interpretation - \n  # you can see the treatments in sequential ATE.\n  \n  ggplot(aes(x = values, y = fct_reorder(Group, values))) +\n  \n  # position = \"dodge\" is the only sure way to see all 3 treatment distributions\n  # identity, single, or any others drop \"Sometimes\" - topic for further study\n  \n    stat_slab(aes(fill = `Voter Class`),\n              position = 'dodge') +\n    scale_fill_calc() +\n  \n    # more frequent breaks on the x-axis provides a better reader interpretation\n    # of the the shift across age groups, as opposed to intervals of 10%\n    \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1),\n                       breaks = seq(-0.05, 0.11, 0.01)) +\n    labs(title = \"Treatment Effects on The Probability of Voting\",\n         subtitle = \"Postcards work less well on those who rarely vote\",\n         y = \"Postcard Type\",\n         x = \"Average Treatment Effect\",\n         caption = \"Source: Gerber, Green, and Larimer (2008)\") +\n    theme_clean() +\n    theme(legend.position = \"bottom\")"
  },
  {
    "path": "n-parameters.html",
    "id": "the-three-levels-of-knowledge",
    "chapter": "11 N Parameters",
    "heading": "11.4.1 The Three Levels of Knowledge",
    "text": "exist three primary levels knowledge possible knowledge scenario: Truth (ideal Preceptor Table), DGM Posterior, Posterior.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "the-truth-1",
    "chapter": "11 N Parameters",
    "heading": "11.4.1.1 The Truth",
    "text": "know Truth (capital “T”), know ideal Preceptor Table. knowledge, can directly answer question precisely. can calculate individual’s treatment effect, summary measure might interested , like average treatment effect.level knowledge possible omniscient power, one can see every outcome every individual every treatment. Truth show, given individual, actions control, actions treatment, little factor impacted decisions.Truth represents highest level knowledge one can — , questions merely require algebra. need estimate treatment effect, different treatment effects different groups people. need predict — know.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "dgm-posterior-1",
    "chapter": "11 N Parameters",
    "heading": "11.4.1.2 DGM posterior",
    "text": "DGM posterior next level knowledge, lacks omniscient quality Truth. posterior posterior calculate perfect knowledge data generating mechanism, meaning correct model structure exact parameter values. often falsely conflated “posterior”, subject error model structure parameter value estimations.DGM posterior, certain individual’s causal effect, Fundamental Problem Causal Inference. words, can never measure one person’s causal effect unable see person’s resulting behavior treatment control; data one two conditions.DGM posterior posterior — estimate parameters based data predict future latest relevant information possible. difference , calculate posteriors unknown value DGM posterior, expect posteriors perfect.go boss estimates posterior, expect 95% confidence interval perfectly calibrated. , expect true value lie within 95% confidence interval 95% time. world, surprised see values outside confidence interval 5% time.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "our-posterior-1",
    "chapter": "11 N Parameters",
    "heading": "11.4.1.3 Our posterior",
    "text": "Unfortunately, posterior possesses even less certainty! real world, don’t perfect knowledge DGM: model structure exact parameter values. mean?go boss, tell best guess. informed estimate based relevant data possible. data, created 95% confidence interval treatment effect various postcards. estimate treatment effect Neighbors postcard 8% 10%.mean certain treatment effect Neighbors values? course ! tell boss, shocking find actual treatment effect less estimate.lot assumptions make process building model, processes Wisdom, subject error. Perhaps data match future well hoped. Ultimately, try account uncertainty estimates. Even safeguard, aren’t surprised bit .instance, shocked treatment effect Neighbors postcard 7%? 12%? course ! slightly , know posterior subject error. surprised treatment effect found 20%? Yes. large enough difference suggest real problem model, real world change forgot factor predictions., amounts large enough difference cause concern? words, wrong one-boss suspicious? “bad luck” sign stupidity? delve question next section chapter.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "bad-luck-or-bad-work",
    "chapter": "11 N Parameters",
    "heading": "11.4.1.4 Bad luck or bad work?",
    "text": "one problem, hard know “right,” posterior similar DGM posterior. , 5% time answer outside 95% confidence interval. truth ends , far away median posterior, boss rightly suspicious. many MAD SDs standard errors away truth obviously fool?many ways judge forecast. , ’re looking two main things: calibration forecast — , whether events said happen 30 percent time actually happened 30 percent time — forecast compared unskilled estimate relies solely historical averages. can answer questions using calibration plots skill scores, respectively. concepts bit advanced course, foundations important understand.Calibration plots compare predicted actually happened. Single predictions can difficult judge , often want group many predictions together bins plot averages bin’s forecasted increase voting actual increase voting. forecasts well-calibrated, bins calibration plot close 45 degree line; forecast poorly calibrated, bins away. second tool, skill scores, lets us evaluate forecasts even , combining accuracy appetite risk single number.Brier skill scores tell us much valuable forecasts unskilled estimate, one informed historical averages — e.g., guess postcard increase voting 5%.technical ways can judge work’s accuracy. boss likely judge using methods.instance, answer many questions (creating many posteriors different problems) , time, boss get sense actual skill, median truth proportion confidence intervals correctly calibrated.know, experience, posteriors often narrow. assume know DGM , fact, know . knowledge? First, prepare boss fact. humility Temperance. Second, estimate dozens different models combine posteriors. result might well median correct posterior, confidence intervals much wider. concepts advanced Primer, important consider making predictions.",
    "code": ""
  },
  {
    "path": "n-parameters.html",
    "id": "summary-12",
    "chapter": "11 N Parameters",
    "heading": "11.5 Summary",
    "text": "Use tidy() function broom.mixed package make models \\(N\\) parameters easier interpret.Use tidy() function broom.mixed package make models \\(N\\) parameters easier interpret.function familiar , stan_glm(), used create models \\(N\\) parameters.function familiar , stan_glm(), used create models \\(N\\) parameters.important remember data equal truth.important remember data equal truth.population like make inferences population data. matter wisdom whether data maps closely enough population studying.population like make inferences population data. matter wisdom whether data maps closely enough population studying.dealing models many parameters, double check know find true slope intercepts — often, requires adding numerous values coefficient studying.dealing models many parameters, double check know find true slope intercepts — often, requires adding numerous values coefficient studying.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "tools",
    "chapter": "Tools",
    "heading": "Tools",
    "text": "chapter broken following sections. Read whichever ones relevant.Absolute relative file pathsWorking terminalGit, GitHub, RStudioPDFStyle guideHow use RpubsHow get helpHow make table",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "absolute-and-relative-file-paths",
    "chapter": "Tools",
    "heading": "11.6 Absolute and relative file paths",
    "text": "read data R, first need tell R data lives. times, data file. file live computer (local) somewhere internet (remote).place file lives computer called “path”. can think path directions file. path includes location file name file . two kinds paths: relative absolute. relative path describes location file relative currently computer. absolute path file respect base (root) folder computer’s filesystem. Absolute paths always start forward slash, “/”.Consider file called report.csv. Read file using relative path:Read report.csv using absolute path:ensure code can run different computer, use relative paths. added bonus ’s also less typing! absolute path file (names folders computer’s root / file) isn’t usually across different computers. example, suppose Fatima Jayden working project together report.csv data. Fatima’s file stored /home/Fatima/files/report.csv,Jayden’s stored /home/Jayden/files/report.csv.Even though Fatima Jayden stored files place computers, absolute paths different due different usernames. Jayden code loads report.csv data using absolute path, code won’t work Fatima’s computer. relative path inside files folder (files/report.csv) computers; code uses relative paths work !One important part using paths recognizing spaces file name. example, trying use path/home/preceptor/desktop/projects/important data/report.csvdoes work includes space file name, “important data”. Instead, path/home/preceptor/desktop/projects/important\\ data/report.csvis valid allows access report.csv data. \\ tells computer treat next character character instead something special. example, space character normally used show break code, computer treats break point whenever sees space. However, \\ tells computer space isn’t break point instead part file path, solving issue.See video another explanation:Source: Udacity course “Linux Command Line Basics”",
    "code": "x <- read_csv(\"data/report.csv\")x <- read_csv(\"/home/preceptor/desktop/projects/data/report.csv\")"
  },
  {
    "path": "tools.html",
    "id": "working-with-the-terminal",
    "chapter": "Tools",
    "heading": "Working with the terminal",
    "text": "Terminal powerful window allows interact computer’s filesystem directly, uses file paths find files want interact .Let’s open Terminal tab left window start learning use Terminal.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "pwd-working-directory",
    "chapter": "Tools",
    "heading": "pwd: Working directory",
    "text": "first question may working Terminal might : can’t see folders, know ? Well ’s great place start learning Terminal. see current folder , type pwd (print working directory):currently directory (folder) called Yao, directory named /Users. forward slash front “Users” tells directory lowest possible level.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "ls-seeing-items-in-the-directory",
    "chapter": "Tools",
    "heading": "ls: Seeing items in the directory",
    "text": "see items current folder, use command ls (list). Type ls terminal hit return/enter. see something like :Notice lists exactly items bottom right window RStudio. Terminal just another way interact computer’s filesystem. Anything can normally mouse/trackpad, like opening folder, can also Terminal.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "cd-changing-directories",
    "chapter": "Tools",
    "heading": "cd: Changing directories",
    "text": "move one directory another, use cd (change directory). ’ll using cd change Desktop folder.change Desktop directory, type cd Desktop/. helpful hint, type first letters folder file name, can hit tab computer auto complete name. Try ! Type cd Desk hit tab auto complete name!type ls , can see item Desktop listed.go back previous folder (aka directory ), can type cd .. two periods represent one level . can see hierarchy view Mac:",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "mkdir-and-rmdir-make-and-remove-a-directory",
    "chapter": "Tools",
    "heading": "mkdir and rmdir: Make and remove a directory",
    "text": "Now ’re Desktop folder, let’s get set-stay organized Gov 1005. Staying organized critical working many data projects. , using mkdir Gov-1005 (make directory) can create folder exclusively Gov 1005 like :Now, type ls, can see new folder created. Note used hyphen Gov 1005. Terminal can’t recognize spaces unless put \\ , like : mkdir Gov\\ 1005. Never use spaces weird characters file directory names.remove folder, use rmdir (remove directory). won’t using right now don’t need remove anything.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "touch-creating-files",
    "chapter": "Tools",
    "heading": "touch: Creating files",
    "text": "order experiment next commands Terminal, ’ll need test file. Type touch text.txt create test file., course, can see test.txt file created using ls.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "mv-moving-files",
    "chapter": "Tools",
    "heading": "mv: Moving files",
    "text": "Oh ! created test.txt file, Gov-1005 folder, right now ’s desktop. happened created Gov-1005 folder using mkdir, forgot move using cd Gov-1005/. worries, can move file folder using mv:using mv first thing type mv file want move. next thing location want move . case want move test.txt Gov-1005/, type mv test.txt Gov-1005/. , can use cd enter Gov-1005 folder use ls see test.txt file successfully moved Gov-1005 directory.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "cp-copying-files",
    "chapter": "Tools",
    "heading": "cp: Copying files",
    "text": "Copying files similar moving files Terminal. Using previous example, wanted copy test.txt Gov-1005 folder delete original test.txt file, just replace mv cp (copy paste):",
    "code": "cp test.txt Gov-1005/"
  },
  {
    "path": "tools.html",
    "id": "rm-removing-files",
    "chapter": "Tools",
    "heading": "rm: Removing files",
    "text": "Ok, last Terminal command book teaching . , ’re done test.txt file. Let’s remove rm (remove):Make sure Gov-1005 folder type rm test.txt! Using ls, can see test file now gone.Congrats! now able basic tasks Terminal! want learn Terminal commands, check Sean Kross’s Unix Workbench.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "git-github-and-rstudio",
    "chapter": "Tools",
    "heading": "Git, GitHub, and RStudio",
    "text": "next section focuses connecting GitHub RStudio using Git. care GitHub? Think Google Drive R code projects. computer blows , GitHub save R work just Google Drive saves paper.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "installing-git",
    "chapter": "Tools",
    "heading": "Installing Git",
    "text": "first step using GitHub installing Git computer. first, may already Git installed computer. check, go Terminal type git --version. already Git, command return Git version installed. get error, can download install git .",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "github-accounts",
    "chapter": "Tools",
    "heading": "GitHub accounts",
    "text": "installing Git, ’ll need GitHub account. like Google account. However, one difference GitHub account visible public. want pick name carefully. professional since sending potential employers link GitHub account near future. Check former Gov 1005 students’ GitHub profiles inspiration:Evelyn CaiEvelyn CaiJessica EdwardsJessica EdwardsBeau MecheBeau MecheOnce GitHub account, ready connect Git RStudio account. Type following two commands Terminal pane. Replace Name name @email.com email used sign GitHub.",
    "code": "git config --global user.name \"Your Name\"\ngit config --global user.email \"your@email.com\""
  },
  {
    "path": "tools.html",
    "id": "github-repositories",
    "chapter": "Tools",
    "heading": "GitHub repositories",
    "text": "now ready create GitHub repository (repo). GitHub repo similar Google Drive folder. make first repo, make sure signed go GitHub homepage click green new button left.want choose good name repo add brief description. use productivity. can choose make repo public private, recommend make repo public important world see. keeps public GitHub profile clean professional. repo probably private. Let’s also add README file repo. document can add information.now first repo GitHub. next step download computer — process often known “cloning” — start editing syncing using Git. , ’ll need copy link repo use RStudio. , green button friend. Click copy link shown. can use clipboard button right automatically copy .",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "connecting-github-to-rstudio",
    "chapter": "Tools",
    "heading": "Connecting GitHub to RStudio",
    "text": "now ready connect productivity repo RStudio. link productivity repo copied, can go back RStudio begin new project. Go File, New Project:Next, ’ll need go steps create project: Version Control Git paste link GitHub click Create Project.Congrats! ’ve linked productivity repo RStudio. Note Github ask location place projects. recommend creating folder desktop called “projects” placing RStudio projects . Don’t just scatter across computer mess. dozens . organized!",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "updating-.gitignore",
    "chapter": "Tools",
    "heading": "Updating .gitignore",
    "text": "first thing always working new repo updating .gitignore file. can open file bottom right window Files tab. file includes files don’t want uploaded GitHub. can come handy working big datasets files private information. case, want add productivityl.Rproj file .gitignore list.file private project file usually don’t want uploaded GitHub. , .gitignore, ’ll want add *.Rproj * tells computer want prevent files ending .Rproj uploaded. also just add productivity.Rproj.Save .gitignore file see productivity.Rproj file disappear Git tab top right window. don’t see changes, click refresh button upper left.symbols Git tab part “conversation” Git. “?” Git’s way saying: “new file . want ?” Adding line .gitignore way replying “Ignore file.”",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "commit-and-push",
    "chapter": "Tools",
    "heading": "Commit and Push",
    "text": "Now ’ve updated .gitignore file, want upload new version GitHub. , first select .gitignore file click Commit button Git window:open new window write commit message. message short note ’re adding/changing repo. case, ’ve updated .gitignore let’s write just :Press commit. way telling Git “Yes files want upload. ’m committed.” Next, press Push. pushes uploads files GitHub. (can probably guess pull , won’t using yet)Now, go GitHub repo refresh page, can see .gitignore file uploaded commit message:Congrats! just uploaded first file GitHub.One tricky aspect caching Github ID password. likely, type things first push. bad. , , Github needs know , otherwise people mess repo. hundreds commits/pushes. don’t want type ID/password time! Follow instructions. Key steps:Turn two-factor authentication GitHub account Settings -> Security.Turn two-factor authentication GitHub account Settings -> Security.Create token:Create token:, logging , bring back Github. Accept defaults press Generate token button bottom. (may need change Note generated tokens .) Copy token created. look something like:8be3e800891425f8462c4491d9a4dbb5b1c1f35cThen, issue R command:Provide token. start new RStudio instance, Github ask login/password . might just ask one time. Seek help work.Happy Git GitHub useR best source Git Github problems arise.",
    "code": "\nusethis::create_github_token()\ngitcreds::gitcreds_set()"
  },
  {
    "path": "tools.html",
    "id": "pdf",
    "chapter": "Tools",
    "heading": "PDF",
    "text": "Generating PDF files RStudio easy hard. easy R markdown designed produce files variety output formats, including PDF. hard , RStudio make PDF files, computer set must set LaTeX installation. four options:Making PDF files may just “work,” especially using Mac. Give try!Making PDF files may just “work,” especially using Mac. Give try!doesn’t just work, strongly recommend using tinytex R package. First, install R package.doesn’t just work, strongly recommend using tinytex R package. First, install R package.Second, use R package install underlying LaTeX distribution.Depending operating system, may work. error message providing instructions. Follow instructions.Restart R everything just work.can just generate html file, open Chrome, select Print . . . drop-menu. get pop-window. Click arrow right Destination choose Save PDF drop-menu. ’ll see preview. Choose Save PDF option. convenient workflow , disaster strikes problem set due 10 minutes, reasonable option.can just generate html file, open Chrome, select Print . . . drop-menu. get pop-window. Click arrow right Destination choose Save PDF drop-menu. ’ll see preview. Choose Save PDF option. convenient workflow , disaster strikes problem set due 10 minutes, reasonable option.can install full LaTeX installation . Good luck! Don’t come us help.can install full LaTeX installation . Good luck! Don’t come us help.",
    "code": "\ninstall.packages('tinytex')\ntinytex::install_tinytex()"
  },
  {
    "path": "tools.html",
    "id": "style-guide",
    "chapter": "Tools",
    "heading": "Style guide",
    "text": "Much material comes Tidyverse Style Guide. take points work submitted violates guidelines. extremis, may go advice, add code comment work explaining decision .",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "comments",
    "chapter": "Tools",
    "heading": "Comments",
    "text": "Include comments code. Easy--understand chunks code comments. code comment. code merit many, many lines comments, lines code . given file, many total lines comments lines code.Make comments meaningful. simple description code . best comments descriptions approaches tried considered. (code already tells us .) Good comments often “Dear Diary” quality: “. tried . finally chose thing reasons X, Y Z. work , look approach.” , structure often paragraph comments followed several lines code.line comment begin comment symbol (“hash”) followed single space: #. Code comments must separated code one empty line sides. Format code comments neatly. Ctrl-Shift-/ easiest way . Name R code chunks, without using weird characters spaces. download_data good R code chunk name. Plot #1 .Spelling matters. Comments constructed sentences, appropriate capitalization punctuation.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "graphics",
    "chapter": "Tools",
    "heading": "Graphics",
    "text": "Use captions, titles, axis labels make clear tables graphics mean.Anytime make graphic without title (explaining graphic ), subtitle (highlighting key conclusion draw), caption (information source data) axis labels (information variables), justify decision code comment. (try ) always include items situations makes less sense. Ultimately, decisions , need understand reasoning.Use best judgment. example, sometimes axis labels unnecessary. Read Data Visualization: practical introduction Kieran Healy guidance making high quality graphics.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "formating",
    "chapter": "Tools",
    "heading": "11.6.1 Formating",
    "text": "",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "long-lines",
    "chapter": "Tools",
    "heading": "Long Lines",
    "text": "Limit code 80 characters per line. fits comfortably printed page reasonably sized font. calling functions, can omit argument names common arguments (.e. arguments used almost every invocation function). Short unnamed arguments can also go line function name, even whole function call spans multiple lines.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "whitespace",
    "chapter": "Tools",
    "heading": "Whitespace",
    "text": "|> always space , usually followed new line. first step pipe, line indented two spaces. structure makes easier add new steps (rearrange existing steps) harder overlook step.ggplot2 code handled similar fashion. commands initial invocation ggplot() indented.",
    "code": "# Good\n\niris |>\n  group_by(Species) |>\n  summarize_if(is.numeric, mean) |>\n  ungroup() |>\n  gather(measure, value, -Species) |>\n  arrange(value)\n\n# Bad\n\niris |> group_by(Species) |> summarize_all(mean) |>\nungroup |> gather(measure, value, -Species) |>\narrange(value)\n# Good\n\ndiamonds |> \n  ggplot(aes(x = depth)) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\n# Bad\n\ndiamonds |> \nggplot(aes(x = depth)) +\ngeom_histogram(bins = 100) + labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")"
  },
  {
    "path": "tools.html",
    "id": "commas",
    "chapter": "Tools",
    "heading": "Commas",
    "text": "Always put space comma, never , just like regular English.",
    "code": "\n# Good\n\nx[, 1]\n\n# Bad\n\nx[,1]\nx[ ,1]\nx[ , 1]"
  },
  {
    "path": "tools.html",
    "id": "parentheses",
    "chapter": "Tools",
    "heading": "Parentheses",
    "text": "put spaces inside outside parentheses regular function calls.",
    "code": "\n# Good\n\nmean(x, na.rm = TRUE)\n\n# Bad\n\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )"
  },
  {
    "path": "tools.html",
    "id": "infix-operators",
    "chapter": "Tools",
    "heading": "Infix operators",
    "text": "infix operators (=, ==, +, -, <-, ~, et cetera) surrounded one space.operators — like ::, :::, $, @, [, [[, ^, : — never surrounded spaces.may add extra spaces improves alignment = <-.add extra spaces places space usually allowed.",
    "code": "\n# Good\n\nheight <- (feet * 12) + inches\nmean(x, na.rm = TRUE)\ny ~ a + b\n\n\n# Bad\n\nheight<-feet*12+inches\nmean(x, na.rm=TRUE)\ny~a + b\n# Good\n\nsqrt(x^2 + y^2)\ndf$z\nx <- 1:10\n\n# Bad\n\nsqrt(x ^ 2 + y ^ 2)\ndf $ z\nx <- 1 : 10\nlist(total = a + b + c,\n     mean = (a + b + c) / n)"
  },
  {
    "path": "tools.html",
    "id": "messageswarningserrors",
    "chapter": "Tools",
    "heading": "Messages/Warnings/Errors",
    "text": "R messages/warnings/errors never appear submitted document. right way deal issues find cause fix underlying problem. Students sometimes use “hacks” make messages/warnings/errors disappear. common hacks involve using code chunk options like message = FALSE, warning = FALSE, results = \"hide\", include = FALSE others. Don’t , general. message/warning/error worth understanding fixing. Don’t close eyes (metaphorically) pretend problem doesn’t exist. situations, however, , matter try, can’t fix problem. cases, can use one hacks, must make code comment directly , explaining situation. exception “setup” chunk (included default every new Rmd) comes include = FALSE. chunk, explanation necessary, convention.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "how-to-use-rpubs",
    "chapter": "Tools",
    "heading": "How to use Rpubs",
    "text": "Rpubs provides free hosting service R work. use :Begin creating new repository GitHub. clone computer. calling repository “rpubs_example.” , put *Rproj .gitignore file. prevent private project file uploaded GitHub.Start new R Markdown file. Go File –> New File –> R Markdown. simplicity, leave name “Untitled” hit “OK.”Save file, , “Untitled” project directory.Knit. see following.Notice blue icon upper right-hand corner reads “Publish.” Click .asked whether want publish RPubs RStudio Connect. Choose RPubs. get reminder documents publish RPubs publicly visible. Click “Publish.”take RPubs website. need create account. Follow steps prompted.Add document details. Name document. Add meaningful slug – otherwise end ugly, long address didn’t choose can’t remember. can leave Description blank simplicity exercise.Hit “Continue”, et voilá! published first document Rpubs!one important step. “rsconnect” contains files specific computer want push GitHub. Therefore, .Rproj files , want add rsconnect folder .gitignore file. Click .gitignore, add hit “Save.” see disappear GitHub top right window. don’t see changes, hit Refresh button top right corner. Since ’ve updated .gitignore file, now good time commit push changes GitHub repository.",
    "code": ""
  },
  {
    "path": "tools.html",
    "id": "how-to-make-a-table",
    "chapter": "Tools",
    "heading": "How to make a table",
    "text": "gt R package creating elegant tables. First, ’ll create gt summary table observations data. Second, ’ll run regression display outcome using gtsummary, companion package gt specializes presenting results statistical models.want learn gt check fantastic guide. Go official gt package website. See extensive guide gtsummary.Load necessary libraries.set message=FALSE code chunk avoid showing ugly notes libraries loaded.Let’s pull data use table:Create simplest table gt(), key command: Now let’s make professional. gt offers variety functions add features like these1:can add title subtitle using tab_header(): default, titles text can formatted. want formatting, must wrap character string call md(), md stands (M)ark(d). example, bolded title. can use tab_spanner() add spanner columns. c() argument takes variables spanner column cover., current table include spanner column. wish see examples spanner columns, go Chapter 4. can change column names using cols_label(): Use tab_source_note() cite source data create caption. function exclusively providing source — though ’s handy way — can used display text ’d like: Using md() , can italicize name Enos study caption: Now table structure looks good, want format numbers . Let’s add dollar signs income column using fmt_currency(). function also adds commas (want commas without dollar signs use fmt_number()). c() within fmt_currency() denotes variable formatted currency: Note line return title “Intergroup” “Contact” effect break title displayed md().",
    "code": "\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(gt)\nx <- trains |>\n  select(gender, income, att_end) |>\n  slice(1:5)\nx## # A tibble: 5 × 3\n##   gender income att_end\n##   <chr>   <dbl>   <dbl>\n## 1 Female 135000      11\n## 2 Female 105000      10\n## 3 Male   135000       5\n## 4 Male   300000      11\n## 5 Male   135000       5\nx |> \n  gt()\nx |> \n  gt() |>\n   tab_header(title = \"Enos Data Observations\", \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |> \n   tab_spanner(label = \"Name of Spanner Column Here\", c(gender, income))\nx |> \n  gt()|>\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |>\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\")\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |>\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") |> \n  tab_source_note(\"Source: Ryan Enos\")\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |>\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") |> \n  tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup Contact on Exclusionary Attitudes*\"))\nx |> \n  gt() |>\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\")|>\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\") |> \n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                       Contact on Exclusionary Attitudes*\")) |>\n    fmt_currency(columns = c(income), \n                 decimals = 0) "
  },
  {
    "path": "tools.html",
    "id": "regression-tables",
    "chapter": "Tools",
    "heading": "Regression tables",
    "text": "can making gt table stan_glm() regression object. Key gtsummary package tbl_regression() function. ",
    "code": "\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\nfit2 <- stan_glm(att_end ~ party, data = trains, refresh = 0)\n\ntbl_regression(fit2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 2)) |>\n  as_gt() |>\n    tab_header(title = \"Regression of Attitudes about Immigration\", \n               subtitle = \"The Effect of Party on End Attitude\") |>\n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                        Contact on Exclusionary Attitudes*\"))## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead\n\n## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead\n\n## Warning: The `fmt_missing()` function is deprecated and will soon be removed\n## * Use the `sub_missing()` function instead"
  },
  {
    "path": "functions.html",
    "id": "functions",
    "chapter": "Functions",
    "heading": "Functions",
    "text": "",
    "code": ""
  },
  {
    "path": "functions.html",
    "id": "introduction-1",
    "chapter": "Functions",
    "heading": "11.7 Introduction",
    "text": "function piece code packaged way makes easy reuse. Functions make easy filter(), arrange(), select(), create tibble(), seen Chapters 1 2. Functions also allow transform variables perform mathematical calculations. use functions like rnorm() runif() generate random draws distribution.Every time reference function Primer, include parentheses. call function including parentheses necessary arguments within parentheses. correct call rnorm():run function name without parentheses, R return code makes function.Functions can sorts things. sample() takes vector values returns number values randomly selected vector. can specify number random values argument size. call equivalent rolling die.Functions can also take functions arguments. example, replicate() takes expression repeats n times. replicated rolling die ten times?especially useful type function family map_* functions, purrr package, automatically loaded library(tidyverse). functions apply function every row tibble.Let’s create tibble one variable x takes three values: 3, 7, 2.easy use mutate create new variable, sq_root, square root value x.map_* functions provide another approach. map_* function takes two required arguments. First object want iterate. generally column tibble working. Second function want run row tibble.map_dbl() (pronounced “map-double”) took function sqrt() applied element x. two tricky parts use map_* functions. First, need put tilde symbol — “~” — name function want call. Without ~, get error:Second, need include period — “.” — spot variable goes. Using name variable — x case — generate error.Tilde dot (~ .) easy forget.know expected output function, can specify kind vector:map(): listmap_lgl(): logicalmap_int(): integermap_dbl(): double (numeric)map_chr(): charactermap_df(): data frameSince example returns numeric output, use map_dbl() instead map().key difference using mutate() map_* functions map_* functions designed work well lists, inputs outputs. mutate() designed atomic vectors, meaning vectors element single value.",
    "code": "\nrnorm(n = 1)## [1] -0.36\nrnorm## function (n, mean = 0, sd = 1) \n## .Call(C_rnorm, n, mean, sd)\n## <bytecode: 0x10ab99438>\n## <environment: namespace:stats>\nsample(x = 1:6, size = 1)## [1] 1\nreplicate(10, sample(1:6, 1))##  [1] 1 1 5 3 4 2 5 6 2 4\nlibrary(tidyverse)\ntibble(x = c(4, 16, 9))## # A tibble: 3 × 1\n##       x\n##   <dbl>\n## 1     4\n## 2    16\n## 3     9\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = sqrt((x)))## # A tibble: 3 × 2\n##       x sq_root\n##   <dbl>   <dbl>\n## 1     4       2\n## 2    16       4\n## 3     9       3\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = map_dbl(x, ~ sqrt(.)))## # A tibble: 3 × 2\n##       x sq_root\n##   <dbl>   <dbl>\n## 1     4       2\n## 2    16       4\n## 3     9       3\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = map_dbl(x, sqrt(.)))## Error in `mutate()`:\n## ! Problem while computing `sq_root = map_dbl(x, sqrt(.))`.\n## Caused by error in `as_mapper()`:\n## ! object '.' not found\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = map_dbl(x, ~ sqrt(x)))## Error in `mutate()`:\n## ! Problem while computing `sq_root = map_dbl(x, ~sqrt(x))`.\n## Caused by error in `stop_bad_type()`:\n## ! Result 1 must be a single double, not a double vector of length 3"
  },
  {
    "path": "functions.html",
    "id": "list-columns-and-map-functions-1",
    "chapter": "Functions",
    "heading": "11.8 List-columns and map functions",
    "text": "Recall list different atomic vector. atomic vectors, element vector one value. Lists, however, can contain vectors, even complex objects, elements.x list two elements. element numeric vector length 3. second element character vector length 2. use [[]] extract specific elements. Example:first [[]] extracts first element form list x. second `[[]]`` extracts 3rd element vector first element.number built-R functions output lists. example, ggplot objects making store plot information lists. function returns multiple values can used create list output wrapping returned object list().Notice 1x1 tibble one observation, list one element. Voila! just created list-column.function returns multiple values vector, like range() , must use list() wrapper want create list-column.list column column data list rather atomic vector. Like lists, can pipe str() examine column.can use map_* functions create list-column , much importantly, work list-column afterwards. Example:flexibility possible via use list-columns map_* functions. workflow extremely common. start empty tibble, using ID specify number rows. skeleton, step pipe adds new column, working column already exists.Let’s practice nhanes dataset primer.data package. add column dataset included quantiles height variable gender?Select relevant variables, group gender. grouping curious height distributed gender. drop rows missing data.two approaches. first, happy output tibble just two rows:Note need use map_* functions case. simple dplyr approach works fine. “trick” use list() wrap output quantile(). Use str() examine exact values.Men taller women throughout distribution, smallest individual (child) data, see 0% quantile, happens male.second case involves scenario want “lose” rows tibble. want q_height column rows, even values included repetitive. common scenario want use q_height perform calculation individual. , need map_* function.first four lines pipe cases. difference use list(quantile(height)) first map(height, ~ quantile(.) second.now, practiced using map_* functions built-R functions. Sometimes, however, R function want. happens, need create function.",
    "code": "\nx <- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx## [[1]]\n## [1]  4 16  9\n## \n## [[2]]\n## [1] \"A\" \"Z\"\nx[[1]][3]## [1] 9\nx <- rnorm(10)\n\n# range() returns the min and max of the argument \n\nrange(x)## [1] -0.64  1.34\ntibble(col_1 = list(range(x))) ## # A tibble: 1 × 1\n##   col_1    \n##   <list>   \n## 1 <dbl [2]>\ntibble(col_1 = list(range(x))) |>\n  str()## tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n##  $ col_1:List of 1\n##   ..$ : num [1:2] -0.636 1.341\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) |> \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) |> \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) |> \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))## # A tibble: 3 × 4\n##      ID draws        max min_max  \n##   <int> <list>     <dbl> <list>   \n## 1     1 <dbl [10]>  1.39 <dbl [2]>\n## 2     2 <dbl [10]>  1.19 <dbl [2]>\n## 3     3 <dbl [10]>  1.87 <dbl [2]>\nlibrary(primer.data)\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender)## # A tibble: 9,647 × 2\n## # Groups:   gender [2]\n##    gender height\n##    <chr>   <dbl>\n##  1 Male     165.\n##  2 Male     165.\n##  3 Male     165.\n##  4 Male     105.\n##  5 Female   168.\n##  6 Male     133.\n##  7 Male     131.\n##  8 Female   167.\n##  9 Female   167.\n## 10 Female   167.\n## # … with 9,637 more rows\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender) |> \n  summarise(q_height = list(quantile(height)),\n            .groups = \"drop\")## # A tibble: 2 × 2\n##   gender q_height \n##   <chr>  <list>   \n## 1 Female <dbl [5]>\n## 2 Male   <dbl [5]>\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender) |> \n  summarise(q_height = list(quantile(height)),\n            .groups = \"drop\") |> \n  str()## tibble [2 × 2] (S3: tbl_df/tbl/data.frame)\n##  $ gender  : chr [1:2] \"Female\" \"Male\"\n##  $ q_height:List of 2\n##   ..$ : Named num [1:5] 83.8 154.3 160.6 165.9 184.5\n##   .. ..- attr(*, \"names\")= chr [1:5] \"0%\" \"25%\" \"50%\" \"75%\" ...\n##   ..$ : Named num [1:5] 83.6 166.2 173.8 179.4 200.4\n##   .. ..- attr(*, \"names\")= chr [1:5] \"0%\" \"25%\" \"50%\" \"75%\" ...\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender) |> \n  summarize(q_height = map(height, ~ quantile(.)),\n            .groups = \"drop\")## # A tibble: 9,647 × 2\n##    gender q_height \n##    <chr>  <list>   \n##  1 Female <dbl [5]>\n##  2 Female <dbl [5]>\n##  3 Female <dbl [5]>\n##  4 Female <dbl [5]>\n##  5 Female <dbl [5]>\n##  6 Female <dbl [5]>\n##  7 Female <dbl [5]>\n##  8 Female <dbl [5]>\n##  9 Female <dbl [5]>\n## 10 Female <dbl [5]>\n## # … with 9,637 more rows"
  },
  {
    "path": "functions.html",
    "id": "custom-functions",
    "chapter": "Functions",
    "heading": "11.9 Custom Functions",
    "text": "many built-functions R. function composed name, list arguments body. create functions function() function.",
    "code": ""
  },
  {
    "path": "functions.html",
    "id": "creating-your-own-functions",
    "chapter": "Functions",
    "heading": "11.9.1 Creating your own functions",
    "text": "Assume want create function adds 1 1 together. first step write R code .code become “body” function, part curly braces. also need function definition, composed name function, call function() function, pair curly braces.Combining function definition body function completes process.just created function! function return 1 + 1 whenever called.Consider function adds number 6 value x, value want allow user provide.incorporated first formal argument. Formal arguments functions additional parameters allow user customize use function. Instead adding 1 + 1 , function takes number x user defines adds 6. Consider function two formal arguments.",
    "code": "\n1 + 1## [1] 2\nadd_one_and_one <- function(){}\nadd_one_and_one <- function(){\n  1 + 1\n}\n\nadd_one_and_one()## [1] 2\nadd_six_to_something <- function(x){\n  x + 6\n}\n\nadd_six_to_something(x = 1)## [1] 7\nadd_x_to_y <- function(x, y) {\n  x + y\n}\n\nadd_x_to_y(1, 2)## [1] 3\nadd_x_to_y(4, 3)## [1] 7"
  },
  {
    "path": "functions.html",
    "id": "anonymous-functions-with-map_-functions",
    "chapter": "Functions",
    "heading": "11.9.2 Anonymous functions with map_* functions",
    "text": "can create functions perform operations “fly,” without bothering give name. nameless functions called anonymous functions.can use anonymous functions conjunction map_* family functions. probably common use anonymous functions, least Primer.can call anonymous function using ~ operator using . represent current element. Consider three approaches:three produce answer, expect. Just using mutate() best, long accomplishes goal. complex situations, especially involving simulation, often won’t. Example:Calling rnorm(), function random component, effect probably want context simple mutate(). Instead, R runs rnorm(1) , copies value generated remaining two rows tibble. get different value row, need explicitly tell R using map_* function:Note parentheses anonymous function necessary. long everything ~ works R code, anonymous function work, time replacing . value relevant row .x variable — old case.",
    "code": "\ntibble(old = c(4, 16, 9)) |> \n  mutate(new_1 = old + 6) |> \n  mutate(new_2 = map_dbl(old, ~ add_six_to_something(.))) |> \n  mutate(new_3 = map_dbl(old, ~ (. + 6)))## # A tibble: 3 × 4\n##     old new_1 new_2 new_3\n##   <dbl> <dbl> <dbl> <dbl>\n## 1     4    10    10    10\n## 2    16    22    22    22\n## 3     9    15    15    15\ntibble(ID = 1:3) |> \n  mutate(x = rnorm(1))## # A tibble: 3 × 2\n##      ID      x\n##   <int>  <dbl>\n## 1     1 -0.282\n## 2     2 -0.282\n## 3     3 -0.282\ntibble(ID = 1:3) |> \n  mutate(x = rnorm(1)) |> \n  mutate(y = map_dbl(ID, ~ rnorm(1)))## # A tibble: 3 × 3\n##      ID      x      y\n##   <int>  <dbl>  <dbl>\n## 1     1 -0.282 -1.31 \n## 2     2 -0.282  0.795\n## 3     3 -0.282  0.270\ntibble(old = c(4, 16, 9)) |> \n  mutate(new = map_dbl(old, ~ . + 1))## # A tibble: 3 × 2\n##     old   new\n##   <dbl> <dbl>\n## 1     4     5\n## 2    16    17\n## 3     9    10"
  },
  {
    "path": "functions.html",
    "id": "skateboard-perfectly-formed-rear-view-mirror",
    "chapter": "Functions",
    "heading": "11.9.3 Skateboard >> perfectly formed rear-view mirror",
    "text": "image — widely attributed Spotify development team — conveys important point.\nFIGURE 11.5: ultimate guide Minimum Viable Product (+great examples)\nBuild skateboard build car fancy car part. limited--functioning thing useful. also keeps spirits high.related Telescope Rule:faster make four-inch mirror six-inch mirror make six-inch mirror.",
    "code": ""
  },
  {
    "path": "functions.html",
    "id": "no_na_sampler",
    "chapter": "Functions",
    "heading": "11.10 no_NA_sampler()",
    "text": "Assume want sample 10 observations height nhanes tibble primer.data package. easy built function sample().One problem approach sample missing values height. can avoid manipulating vector inside call sample().works, , first, ugly code. , second, hard extend constraints. example, assume want sample individuals missing values variables, just height. , really make custom function. Call function no_NA_sampler().first step function creation write code normal pipe want function . case, code look like:start nhanes, use drop_na() remove rows missing values variable, sample 10 rows random pull height. turn function, just need copy/paste pipe within body function definition:Voila! function just executes code within body. first step building function write function. write code want function execute.first version, however, “hard codes” lot options might want change. want sample 5 values height 500? case, hard code new number place “10”. better option add argument can pass whatever value want.want sample different variable height different tibble nhanes? , trick turn hard coded values arguments. argument tbl placeholder data set, n number samples want extracted data set, var variable samples studying.R know interpret something like age passed argument. double curly braces around var tell R, essence, var variable tibble created sampling input tibble tbl. can use order arguments, without naming , no_NA_sampler(), just R function:Now function want, add comments error checking.comments code seem weird? Perhaps. good comments! First, many lines comments lines code. good rule thumb. Second, comments simple report code . redundant! code tells us code . comments, instead, discussion issues related code, things don’t understand, topics revisit. like diary. Good programmers keep good diaries.",
    "code": "\nsample(nhanes$height, size = 10)##  [1] 159 119 170 171 181 136  99 169 153 175\nsample(nhanes$height[! is.na(nhanes$height)], size = 10)##  [1] 163 141 158 160 169 183 117 158  86 176\nnhanes |> \n  drop_na() |>\n  slice_sample(n = 10) |> \n  pull(height)##  [1] 157 166 171 160 156 152 166 150 175 160\nno_NA_sampler <- function(){\n  nhanes |> \n    drop_na() |>\n    slice_sample(n = 10) |> \n    pull(height)\n}\n\nno_NA_sampler()##  [1] 162 165 159 156 159 160 157 171 164 156\nno_NA_sampler <- function(n){\n  nhanes |> \n    drop_na() |>\n    slice_sample(n = n) |> \n    pull(height)\n}\n\nno_NA_sampler(n = 2)## [1] 164 170\nno_NA_sampler(n = 25)##  [1] 156 172 166 168 166 164 164 161 159 156 163 164 168 170 155 177 173 166 169\n## [20] 173 174 157 159 157 159\nno_NA_sampler <- function(tbl, var, n){\n  tbl |> \n    drop_na() |>\n    slice_sample(n = n) |> \n    pull({{var}})\n}\n\nno_NA_sampler(tbl = nhanes, var = height, n = 2)## [1] 164 163\nno_NA_sampler(trains, age, 5)## [1] 45 42 41 31 44\nno_NA_sampler <- function(tbl, var, n){\n  \n  # Function for grabbing `n` samples from a variable `var` which lives in a\n  # tibble `tbl`. \n  \n  # I could not figure out how to check to see if `var` actually lives in the\n  # tibble in my error checking. Also, I don't like that I need to use\n  # is_double() as the check on `n` even though I want `n` to be an integer.\n  \n  stopifnot(is_tibble(tbl))\n  stopifnot(is_double(n))\n\n  tbl |> \n    drop_na() |>\n    \n    # What happens if n is \"too large\"? That is, I need to think harder about a)\n    # whether or not I am sampling with or without replacement and b) which I\n    # should be doing.\n    \n    slice_sample(n = n) |> \n    pull({{var}})\n}"
  },
  {
    "path": "functions.html",
    "id": "prediction-game",
    "chapter": "Functions",
    "heading": "11.11 Prediction Game",
    "text": "Let’s play prediction game. Consider kenya tibble primer.data.game pick random value rv13, number people live vicinity polling station. guess number. guess number. winner Prediction Game person whose guess closest random value selected. Example:Run code R Console try . works! also sloppy disorganized. first step writing good code write bad code.don’t want play Prediction Game just . want play thousands times. Copy/pasting code thousand times stupid. Instead, need function. Just place working code within function definition, Voila!function definition , changes. Yet, creating function, can now easily run many times.problem version want prediction_game() return message winner. Right now, returns nothing. just prints winner. Let’s change , also allow guesses passed argument, along tibble variable. can leave n hard coded 1 since, definition, Prediction Game attempt guess one number, least now. , need use return() function , executed, causes function finish return whatever value within paratheses.general, want store results tibble, makes later analysis plotting easier.wins game play 1,000 times?hardly surprising 500 wins often 600 since mean rv13 539.23. mean seems like pretty good guess! best guess.test whether mean median better guess, use created prediction_game function guesses 442 (median) 539 (mean) plot results.mean bad prediction. best prediction (surprisingly?) median, 442.",
    "code": "\nkenya## # A tibble: 1,672 × 9\n##    block poll_station treatment poverty distance pop_density mean_age reg_byrv13\n##    <chr> <chr>        <fct>       <dbl>    <dbl>       <dbl>    <dbl>      <dbl>\n##  1 KWAL… 007/001      control     0.247     22.0    0.00296      39.6    0.00358\n##  2 KWAL… 007/004      local + …   0.329     25.1    0.000888     43.8    0.0742 \n##  3 KWAL… 007/009      local       0.263     27.8    0.00184      34.7    0.00691\n##  4 KWAL… 007/011      local + …   0.429     27.2    0.000270     44.6    0.26   \n##  5 KWAL… 007/017      local + …   0.341     19.3    0.000544     39.0    0.0228 \n##  6 KWAL… 007/018      local       0.204     24.0    0.00798      37.1    0.00243\n##  7 KWAL… 007/019      SMS         0.272     25.1    0.00167      39.2    0.00487\n##  8 KWAL… 007/020      control     0.316     23.8    0.000538     36.3    0      \n##  9 KWAL… 007/022      canvass     0.396     20.5    0.000216     42.9    0.00575\n## 10 KWAL… 007/023      local + …   0.398     14.5    0.000148     39.8    0.0360 \n## # … with 1,662 more rows, and 1 more variable: rv13 <dbl>\nyour_guess <- 500\nmy_guess <- 600\n\nsampled_value <- no_NA_sampler(kenya, rv13, n = 1) \n\nyour_error <- abs(your_guess - sampled_value)\nmy_error <- abs(my_guess - sampled_value)\n\nif(your_error < my_error) cat(\"You win!\")## You win!\nif(your_error > my_error) cat(\"I win!\")\nprediction_game <- function(){\n  your_guess <- 500\n  my_guess <- 600\n  \n  sampled_value <- no_NA_sampler(kenya, rv13, n = 1) \n  \n  your_error <- abs(your_guess - sampled_value)\n  my_error <- abs(my_guess - sampled_value)\n  \n  if(your_error < my_error) cat(\"You win!\")\n  if(your_error > my_error) cat(\"I win!\")\n}\nreplicate(3, prediction_game())## You win!You win!You win!## [[1]]\n## NULL\n## \n## [[2]]\n## NULL\n## \n## [[3]]\n## NULL\nprediction_game <- function(guesses, tbl, var){\n  \n  # Check to make sure that guesses is a vector of doubles of length 2.\n  \n  stopifnot(all(is_double(guesses)))\n  stopifnot(length(guesses) == 2)\n  \n  # This tells the function that the \"guess\" inputted first in the \n  # guesses is \"your\" guess, whereas the second input is \"my\" guess.\n  \n  your_guess <- guesses[1]\n  my_guess <- guesses[2]\n  \n  # Use the function no_NA_sampler to draw a sample from a data set\n  # of our choosing, with a {{var}} and n.\n  \n  sampled_value <- no_NA_sampler(tbl, {{var}}, n = 1) \n  \n  # Subtract the sampled value obtained from no_NA_sampler from \n  # both of our guesses. \n  \n  your_error <- abs(your_guess - sampled_value)\n  my_error <- abs(my_guess - sampled_value)\n  \n  # If the difference between your guess and the sampled value is \n  # less than the difference between my guess and the sampled value\n  # (meaning that your guess was closer to the truth), the function\n  # returns the message \"Guess, your_guess, wins!\".\n  \n  if(your_error < my_error){ \n    return(paste(\"Guess\", your_guess, \"wins!\"))\n  }\n  \n  # If your error exceeds my error (meaning that your guess was\n  # further than the truth than mine), the function prints the \n  # message \"Guess, my_guess, wins!\" \n  \n  if(your_error > my_error){ \n    return(paste(\"Guess\", my_guess, \"wins!\"))\n  }\n  \n  # If we guess the same number, and our error rates are therefore\n  # identical, we return the message \"A tie!\". \n  \n  if(your_error == my_error){ \n    return(\"A tie!\")\n  }\n\n}\nreplicate(5, prediction_game(guesses = c(500, 600), kenya, rv13))## [1] \"Guess 500 wins!\" \"Guess 500 wins!\" \"Guess 600 wins!\" \"Guess 500 wins!\"\n## [5] \"Guess 500 wins!\"\ntibble(ID = 1:3) |> \n  mutate(result = map_chr(ID, ~ \n                            prediction_game(guesses = c(500, 600),\n                                            kenya, \n                                            rv13)))## # A tibble: 3 × 2\n##      ID result         \n##   <int> <chr>          \n## 1     1 Guess 500 wins!\n## 2     2 Guess 500 wins!\n## 3     3 Guess 500 wins!\ntibble(ID = 1:1000) |> \n  mutate(result = map_chr(ID, ~ \n                            prediction_game(guesses = c(500, 600),\n                                            kenya, \n                                            rv13))) |> \n  ggplot(aes(result)) +\n    geom_bar()\ntibble(ID = 1:1000) |> \n  mutate(result = map_chr(ID, \n                          ~ prediction_game(c(442, 539),\n                                            kenya,\n                                            rv13))) |> \n  ggplot(aes(result)) +\n    geom_bar()"
  },
  {
    "path": "functions.html",
    "id": "playing-within-a-tibble",
    "chapter": "Functions",
    "heading": "11.11.0.1 Playing within a tibble",
    "text": "cases, convenient play portions Prediction Game within tibble. Imagine trying guess biggest value 10 random samples.can now manipulate result column see prediction better. Using structure , subtract guesses variable guessing; case, biggest value 10 random samples.Run test 1,000 times.Empirically, see 900 much better guess 800. Instead calling function run 1,000 times, just performed step within row tibble 1,000 rows. approaches work. best choice depends context problem.",
    "code": "\ntibble(ID = 1:3, guess_1 = 800, guess_2 = 900) |> \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10)))## # A tibble: 3 × 4\n##      ID guess_1 guess_2 result    \n##   <int>   <dbl>   <dbl> <list>    \n## 1     1     800     900 <dbl [10]>\n## 2     2     800     900 <dbl [10]>\n## 3     3     800     900 <dbl [10]>\ntibble(ID = 1:3, guess_1 = 800, guess_2 = 900) |> \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) |> \n  mutate(biggest = map_dbl(result, ~ max(.))) |> \n  mutate(error_1 = abs(guess_1 - biggest)) |> \n  mutate(error_2 = abs(guess_2 - biggest)) |> \n  mutate(winner = case_when(error_1 < error_2 ~ \"Guess one wins!\",\n                            error_1 > error_2 ~ \"Guess two wins!\",\n                            TRUE ~ \"A tie!\"))## # A tibble: 3 × 8\n##      ID guess_1 guess_2 result     biggest error_1 error_2 winner         \n##   <int>   <dbl>   <dbl> <list>       <dbl>   <dbl>   <dbl> <chr>          \n## 1     1     800     900 <dbl [10]>    1382     582     482 Guess two wins!\n## 2     2     800     900 <dbl [10]>    2246    1446    1346 Guess two wins!\n## 3     3     800     900 <dbl [10]>    1422     622     522 Guess two wins!\ntibble(ID = 1:1000, guess_1 = 800, guess_2 = 900) |> \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) |> \n  mutate(biggest = map_dbl(result, ~ max(.))) |> \n  mutate(error_1 = abs(guess_1 - biggest)) |> \n  mutate(error_2 = abs(guess_2 - biggest)) |> \n  mutate(winner = case_when(error_1 < error_2 ~ \"Guess one wins!\",\n                            error_1 > error_2 ~ \"Guess two wins!\",\n                            TRUE ~ \"A tie!\")) |> \n  ggplot(aes(winner)) +\n    geom_bar()"
  },
  {
    "path": "functions.html",
    "id": "summary-13",
    "chapter": "Functions",
    "heading": "11.12 Summary",
    "text": "first step writing good code write bad code.Tilde dot (~ .) easy forget.first step building function write function. write code want function execute.",
    "code": ""
  },
  {
    "path": "functions.html",
    "id": "lists-and-list-columns",
    "chapter": "Functions",
    "heading": "11.12.1 Lists and list-columns",
    "text": "list different atomic vector. Atomic vectors familiar us: element vector one value, thus atomic vector column data set, observation gets single value. Lists, however, can contain vectors, complex objects, elements.various ways create lists. common use list() function “wrap” object. map() always returns list.can take list column , applying anonymous function map(), create another list column. similar taking tibble piping function, like mutate(), returns new tibble work .can also use map_* functions take list column input return atomic vector – column single value per observation – output.function returns multiple values vector, must use list() wrapper want create list-column.",
    "code": ""
  },
  {
    "path": "functions.html",
    "id": "writing-functions",
    "chapter": "Functions",
    "heading": "11.12.2 Writing functions",
    "text": "Optimize usefulness adding formal arguments needed. function gives option n may helpful function allows us enter options data set, variable, n value.Give arguments sensible names.default, function returns result last line body. Use return() override default.starting function, remember smaller steps easier trying build everything one motion. general: start writing body, test body basic function, add formal arguments.Use double curly braces around vars, since R know interpret variable names passed argument. double curly braces tell R var variable tibble.",
    "code": ""
  },
  {
    "path": "functions.html",
    "id": "distributions-1",
    "chapter": "Functions",
    "heading": "11.12.3 Distributions",
    "text": "word “distribution” can mean two things. First, object — mathematical formula, imaginary urn — can draw values. Second, list values.two important aspects distribution center variability.median often stable measure center mean. mad (scaled median absolute deviation) often stable measure variation standard deviation.Outliers cause lack stability. distribution without outliers, mean/median mad/sd close value matter much ones use.",
    "code": ""
  },
  {
    "path": "animation.html",
    "id": "animation",
    "chapter": "Animation",
    "heading": "Animation",
    "text": "gganimate package creating animated ggplots. provides range new functionality can added plot object order customize change time.Key features gganimate:transitions: want data changeviews: want viewpoint changeshadows: want animation memoryMany thanks Alboukadel Kassambara allowing us use tutorial section.",
    "code": ""
  },
  {
    "path": "animation.html",
    "id": "set-up-1",
    "chapter": "Animation",
    "heading": "Set Up",
    "text": "Load required packages set default ggplot2 theme theme_bw():",
    "code": "\nlibrary(tidyverse)\nlibrary(gapminder)\nlibrary(gganimate)\ntheme_set(theme_bw())\nhead(gapminder)## # A tibble: 6 × 6\n##   country     continent  year lifeExp      pop gdpPercap\n##   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n## 1 Afghanistan Asia       1952    28.8  8425333      779.\n## 2 Afghanistan Asia       1957    30.3  9240934      821.\n## 3 Afghanistan Asia       1962    32.0 10267083      853.\n## 4 Afghanistan Asia       1967    34.0 11537966      836.\n## 5 Afghanistan Asia       1972    36.1 13079460      740.\n## 6 Afghanistan Asia       1977    38.4 14880372      786."
  },
  {
    "path": "animation.html",
    "id": "transition-through-distinct-states-in-time",
    "chapter": "Animation",
    "heading": "Transition through distinct states in time",
    "text": "Begin static plot:",
    "code": "\np <- ggplot(gapminder,\n            aes(x = gdpPercap, y=lifeExp, size = pop, color = country)) +\n      geom_point(show.legend = FALSE, alpha = 0.7) +\n      scale_color_viridis_d() +\n      scale_size(range = c(2, 12)) +\n      scale_x_log10() +\n      labs(x = \"GDP per capita\", y = \"Life expectancy\")\np"
  },
  {
    "path": "animation.html",
    "id": "basics",
    "chapter": "Animation",
    "heading": "Basics",
    "text": "Key R function: transition_time(). transition length states set correspond actual time difference .Label variables: frame_time. Gives time current frame corresponds .",
    "code": "\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\")"
  },
  {
    "path": "animation.html",
    "id": "create-facets-by-continent",
    "chapter": "Animation",
    "heading": "11.12.4 Create facets by continent",
    "text": "",
    "code": "\np + facet_wrap(~continent) +\n  transition_time(year) +\n  labs(title = \"Year: {frame_time}\")"
  },
  {
    "path": "animation.html",
    "id": "let-the-view-follow-the-data-in-each-frame",
    "chapter": "Animation",
    "heading": "11.12.5 Let the view follow the data in each frame",
    "text": "",
    "code": "\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  view_follow(fixed_y = TRUE)"
  },
  {
    "path": "animation.html",
    "id": "show-preceding-frames-with-gradual-falloff",
    "chapter": "Animation",
    "heading": "11.12.6 Show preceding frames with gradual falloff",
    "text": "shadow meant draw small wake data showing latest frames current. can choose gradually diminish size /opacity shadow. length wake given absolute frames make animation susceptible changes framerate. Instead given proportion total length animation.",
    "code": "\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  shadow_wake(wake_length = 0.1, alpha = FALSE)"
  },
  {
    "path": "animation.html",
    "id": "show-the-original-data-as-background-marks",
    "chapter": "Animation",
    "heading": "11.12.7 Show the original data as background marks",
    "text": "shadow lets show raw data behind current frame. past /future raw data can shown styled want.",
    "code": "\np + transition_time(year) +\n  labs(title = \"Year: {frame_time}\") +\n  shadow_mark(alpha = 0.3, size = 0.5)"
  },
  {
    "path": "animation.html",
    "id": "reveal-data-along-a-given-dimension",
    "chapter": "Animation",
    "heading": "Reveal data along a given dimension",
    "text": "transition allows let data gradually appear, based given time dimension. Start static plot:",
    "code": "\np <- ggplot(airquality,\n            aes(Day, Temp, group = Month, color = factor(Month))) +\n      geom_line() +\n      scale_color_viridis_d() +\n      labs(x = \"Day of Month\", y = \"Temperature\") +\n      theme(legend.position = \"top\")\np"
  },
  {
    "path": "animation.html",
    "id": "let-data-gradually-appear",
    "chapter": "Animation",
    "heading": "11.12.8 Let data gradually appear",
    "text": "Reveal day (x-axis)Show points:Points can kept giving unique group:",
    "code": "\np + transition_reveal(Day)\np + \n  geom_point() +\n  transition_reveal(Day)\np + \n  geom_point(aes(group = seq_along(Day))) +\n  transition_reveal(Day)"
  },
  {
    "path": "animation.html",
    "id": "transition-between-several-distinct-stages-of-the-data",
    "chapter": "Animation",
    "heading": "Transition between several distinct stages of the data",
    "text": "Data preparation:Create bar plot mean temperature:transition_states():enter_grow() + enter_fade()",
    "code": "\nmean.temp <- airquality |>\n  group_by(Month) |>\n  summarise(Temp = mean(Temp), .groups = \"drop_last\")\nmean.temp## # A tibble: 5 × 2\n##   Month  Temp\n##   <int> <dbl>\n## 1     5  65.5\n## 2     6  79.1\n## 3     7  83.9\n## 4     8  84.0\n## 5     9  76.9\np <- ggplot(mean.temp, \n            aes(Month, Temp, fill = Temp)) +\n      geom_col() +\n      scale_fill_distiller(palette = \"Reds\", direction = 1) +\n      theme_minimal() +\n      theme(panel.grid = element_blank(),\n            panel.grid.major.y = element_line(color = \"white\"),\n            panel.ontop = TRUE)\np\np + transition_states(Month, wrap = FALSE) +\n  shadow_mark()\np + transition_states(Month, wrap = FALSE) +\n  shadow_mark() +\n  enter_grow() +\n  enter_fade()"
  },
  {
    "path": "animation.html",
    "id": "save-your-animation",
    "chapter": "Animation",
    "heading": "Save your animation",
    "text": "code create animations can take long time run. created animation, ’ll want save somewhere can display without run code.key function use anim_save(), similar saving static plots using ggsave(). save animation gif. first argument filename want give animation second animation object, animation object called p wanted save file called “p.gif”, save like :don’t supply second argument, anim_save() default saving recent animation rendered. anim_save(\"animation.gif\") save recent animation “animation.gif”.don’t want save gif current directory, can specify directory using path argument. Let’s say subdirectory working directory called “gifs”. can thus save “animation.gif” “gifs” anim_save(\"animation.gif\", path = \"gifs\").created gif, can post online. can post Facebook selecting “Photo/Video” Facebook status Twitter clicking photo icon.",
    "code": "\nanim_save(\"p.gif\", p)"
  },
  {
    "path": "set-up-for-working-on-the-primer.html",
    "id": "set-up-for-working-on-the-primer",
    "chapter": "Set Up for Working on The Primer",
    "heading": "Set Up for Working on The Primer",
    "text": "document provides guide setting R/RStudio work Primer, book , PPDBS/primer, associated tutorial data packages: PPDBS/primer.tutorials PPDBS/primer.data. three steps:first part ensures knowledge computer settings successful. luck, need .first part ensures knowledge computer settings successful. luck, need .second part involves making connection true repos PPBDS Github organization Github account computer. may end dozens times since, whenever something gets messed , easiest solution often just nuke orbit start . , weeks, won’t .second part involves making connection true repos PPBDS Github organization Github account computer. may end dozens times since, whenever something gets messed , easiest solution often just nuke orbit start . , weeks, won’t .third part involves daily workflow merges pull requests. steps many times day.third part involves daily workflow merges pull requests. steps many times day.test ensures successfully completed set submit PR TODO.txt file primer package adds name top file. PR change file.",
    "code": ""
  },
  {
    "path": "set-up-for-working-on-the-primer.html",
    "id": "computer-set-up",
    "chapter": "Set Up for Working on The Primer",
    "heading": "11.13 Computer Set Up",
    "text": "Install latest released versions R RStudio. Install usethis package.Read Getting Started Tools sections Primer make sure Git/Github working. Read (watch videos ) Getting Used R, RStudio, R Markdown Chester Ismay Patrick C. Kennedy. Check RStudio Essentials Videos. relevant us “Writing code RStudio”, “Projects RStudio” “Github RStudio”. best reference R/RStudio/Git/Github issues always Happy Git GitHub useR.Make sure Git/Github connections good. gone key chapters Happy Git R — — may already OK. (, even ), need run usethis::git_sitrep().left end output.first part — Git config — seems messed , execute:second part seems messed , try:read Github credentials. , restart R run git_sitrep() make sure things look like mine, less.Install renv package. can read renv package .critical understand details renv works. big picture creates set libraries used just project whose versions kept sync .point, tools need contribute. never done pull request, however, need learn . Start reading help page. Read whole thing! Don’t just skim . important concepts professional-level workflow. usethis package mostly providing wrappers around underlying git commands. want understand happening lower level, read , optional., luck, steps .Prove () set working submittimg pull request simply adds name top one TODO.txt files. (See .)",
    "code": "> library(usethis)   \n> git_sitrep()    \nGit config (global)   \n● Name: 'David Kane'   \n● Email: 'dave.kane@gmail.com'   \n● Vaccinated: FALSE   \nℹ See `?git_vaccinate` to learn more   \nℹ Defaulting to https Git protocol   \n● Default Git protocol: 'https'   \nGitHub   \n● Default GitHub host: 'https://github.com'   \n● Personal access token for 'https://github.com': '<discovered>'   \n● GitHub user: 'davidkane9'   \n● Token scopes: 'delete_repo, gist, notifications, repo, user, workflow'   \n● Email(s): 'dave.kane@gmail.com (primary)', 'dkane@fas.harvard.edu'   \n...   \nuse_git_config(user.name = \"David Kane\", user.email = \"dave.kane@gmail.com\")\nusethis::create_github_token()"
  },
  {
    "path": "set-up-for-working-on-the-primer.html",
    "id": "project-set-up",
    "chapter": "Set Up for Working on The Primer",
    "heading": "11.14 Project Set Up",
    "text": "need steps least one time. likely, however, dozens times. things working, great! start working, can try diagnose problem. , can’t, nuke orbit scenario, means start deleting current version package two places: computer, Github account. delete primer computer, put R Studio project directory Trash. Make sure also close R Studio session delete . reason completely remove , consider using command $sudo rm -r dirname replace “dirname” path primer computer! sudo rm can extremely dangerous used together, make sure double check command /additional research. successfully remove computer, go Github account go Settings delete repo.Key steps:Fork/download target repo:repo working book. working PPBDS/primer.data PPBDS/primer.tutorials, need create_from_github() using repos. must change destdir location computer. Indeed, professionals generally several different RStudio sessions open, working different R project/package, connected Github repo.education, worth reading help page create_from_github(). fork protocal arguments may really necessary , obviously, place project location computer projects live. command first forks copy PPBDS/primer Github account clone/downloads fork computer.may seem like overkill, , Pro Git explains, (essentially) large projects organized. luck, issue command . , always connected, fork true repos, live github/com/PPBDS. Also, note , something ever gets totally messed computer, can just delete project folder computer repo Github account start . (made changes don’t want lose, just save files changes one side move back recreated project.)Note command automatically put new RStudio session primer (primer.tutorials primer.data) RStudio project resides computerThe next step get renv setup running package versions everyone else. Run :install packages need directory project. (effect main library R packages.) Restart R session. , means now two separate installations , example, ggplot2. One default place R sessions default pointed . (different project without renv directory, can run .libPaths() see .) second place ggplot2 installed renv directory lives project.Note , part, won’t anything renv initial use. use error = TRUE code chunk, also need renv.ignore = TRUE code chunk, get annoying warning renv can’t parse code chunk.However, three renv commands might issue:renv::status() just reports anything messed . won’t hurt anything.renv::restore() looks renv.lock file installs packages specifies. need make change renv.lock, e.g., upgrade version ggplot2 add new package.renv::snapshot() issued know . changes renv.lock file, something , usually, . common case use need add new package project.Create branch work :Make sure branch name sensible. , command need issue , least current workflow. always “” branch, never default (master) branch. can check upper right corner git panel R Studio.professional settings, often work several different branches . , comfortable, feel free create one branch, use , delete . Never work default branch, however. , use multiple branches, careful .",
    "code": "\nlibrary(usethis)  \ncreate_from_github(\"PPBDS/primer\",   \n                    fork = TRUE,   \n                    destdir = \"/Users/davidkane/Desktop/projects/\",   \n                    protocol = \"https\")  \nlibrary(renv)\nrenv::restore()\npr_init(branch = \"chapter-9\")"
  },
  {
    "path": "set-up-for-working-on-the-primer.html",
    "id": "daily-work",
    "chapter": "Set Up for Working on The Primer",
    "heading": "11.15 Daily Work",
    "text": "Pull regularly:Issue command time. make sure repo computer updated latest changes made book. word “upstream” associated repos PPBDS. word “origin” associated fork Github account. , general, don’t need worry . Just pull every time sit . (Just clicking pull button enough. pulls repo, changes made. pull PPBDS/primer, et al.) issue command multiple times day.Make changes file editing. Knit make sure changes work. Commit message. Push repo Github account. .point, ready push PPBDS organization. However, can’t directly. Instead, must submit pull request (PR). part larger project, commands slightly different done , usually just clicking pull (blue) push (green) arrows Git pane RStudio.Issue pull requests every days, depending much work done /whether people waiting something done.command bundles bunch git commands (hand) one handy step. command everything needed create “pull request” — request accept changes proposing repo PPBDS/primer — opens web page show . done! must PRESS green button web page, sometimes twice. , PR actually created. pr_push() just everything . “pr” pr_push() stands pull request.leave aside now issues associated back--forth discussions might around pull request. probably just accept . changes go repos PPBDS distributed everyone else run pr_merge_main().leave aside now issues associated back--forth discussions might around pull request. probably just accept . changes go repos PPBDS distributed everyone else run pr_merge_main().can now continue . need wait deal pull request. need fork/clone/download . don’t need create new branch, although many people , branch name describes working now. just keep editing files, knitting, committing pushing forked repo. feel completed another chunk work, just run pr_push() .can now continue . need wait deal pull request. need fork/clone/download . don’t need create new branch, although many people , branch name describes working now. just keep editing files, knitting, committing pushing forked repo. feel completed another chunk work, just run pr_push() .Read usethis setup help page least , perhaps week two working within framework. lots good stuff!Read usethis setup help page least , perhaps week two working within framework. lots good stuff!",
    "code": "\npr_merge_main()\npr_push()"
  },
  {
    "path": "set-up-for-working-on-the-primer.html",
    "id": "common-problems",
    "chapter": "Set Up for Working on The Primer",
    "heading": "11.16 Common Problems",
    "text": "immediate aftermath creation process, blue/green arrows (Git panel) pulling/pushing may grayed . sign connection computer forked repo “settled .” (sure cause even right terminology.) think just issuing first pr_merge_main() fixes . , always goes away. , however, can’t pull/push repo. doesn’t really matter, however, since key commands need pr_merge_main() pr_push(), always work immediately.immediate aftermath creation process, blue/green arrows (Git panel) pulling/pushing may grayed . sign connection computer forked repo “settled .” (sure cause even right terminology.) think just issuing first pr_merge_main() fixes . , always goes away. , however, can’t pull/push repo. doesn’t really matter, however, since key commands need pr_merge_main() pr_push(), always work immediately.running pr_merge_main(), often see bunch files Git tab top right corner Rstudio marked M (Modified), including files know edit. files updated “truth” — PPBDS/primer — since last pr_merge_main(). Since pulled directly PPBDS/primer repo, forked repo sees changes people made thinks made . easily fixed, however — just commit changes forked repo. (Strangely, seems always happen. don’t see effect, don’t worry.)running pr_merge_main(), often see bunch files Git tab top right corner Rstudio marked M (Modified), including files know edit. files updated “truth” — PPBDS/primer — since last pr_merge_main(). Since pulled directly PPBDS/primer repo, forked repo sees changes people made thinks made . easily fixed, however — just commit changes forked repo. (Strangely, seems always happen. don’t see effect, don’t worry.)Always run pr_merge_main() committing file. Otherwise, may create lots merge conflicts. happens, save copy file(s) personally editing side. , nuke orbit, following instructions . Repeat Project Set process. move file(s) hand new repo, commit/push normal.Always run pr_merge_main() committing file. Otherwise, may create lots merge conflicts. happens, save copy file(s) personally editing side. , nuke orbit, following instructions . Repeat Project Set process. move file(s) hand new repo, commit/push normal.submit pull request merge work PPBDS repo, won’t always smiles sunshine — every , ’ll run merge conflicts. arise, two parties work file separately submit conflicting changes. makes hard GitHub “merge” version version. happens, find multiple adjacent “>”, “<”, “=” signs document — show conflicts occur. background merge conflicts, read .submit pull request merge work PPBDS repo, won’t always smiles sunshine — every , ’ll run merge conflicts. arise, two parties work file separately submit conflicting changes. makes hard GitHub “merge” version version. happens, find multiple adjacent “>”, “<”, “=” signs document — show conflicts occur. background merge conflicts, read .see -mentioned conflicts document, submit pull request. mess things . Instead, first, go document, make sure weird conflict indicators (<, >, =) removed. Second, decide goes space. might stuff wrote. might stuff. might combination two decide . Whatever happens, making affirmative choice appear file location. merge conflicts fixed, run pr_push() .pr_push() can tricky. First, note , accepted (p)ull (r)equest submitted, PR still open. can see Github. fact, can see closed/completed pull requests well. , one PR still open, submit another pr_push(), just added current PR. OK! don’t need separate.even open PR, pr_push() can tricky. key thing remember must press green button Github new PR created. Normally, easy. Running pr_push() automatically (perhaps run pr_view()) puts browser brings correct Github page. Press button – presto! – created PR. , sometimes, web page different. actually sends back old pull request. happens, need click “Pull Request” tab . take new page, green button labeled “Compare & Pull Request”. Press button.end needing install new package — rare — just install type renv::status() confirm renv aware change. , type renv::snapshot(). update renv.lock file include new package. just commit/push new version renv.lock, shares information everyone else project. Never commit/push modified renv.lock unless know changed.end needing install new package — rare — just install type renv::status() confirm renv aware change. , type renv::snapshot(). update renv.lock file include new package. just commit/push new version renv.lock, shares information everyone else project. Never commit/push modified renv.lock unless know changed.careful committing garbage files like “.DS_Store”, file created sometimes. commit changes understand. vast majority cases PRs involve one two files.careful committing garbage files like “.DS_Store”, file created sometimes. commit changes understand. vast majority cases PRs involve one two files.",
    "code": ""
  },
  {
    "path": "set-up-for-working-on-the-primer.html",
    "id": "style-guide-1",
    "chapter": "Set Up for Working on The Primer",
    "heading": "11.17 Style Guide",
    "text": "Never use just single # using chapter title. first subpart uses ##. 5 8 subparts chapter. Within subpart, may sub-subparts, indicated ###. 3 10 . may use #### like.Never use just single # using chapter title. first subpart uses ##. 5 8 subparts chapter. Within subpart, may sub-subparts, indicated ###. 3 10 . may use #### like.Section headings (Chapter titles) sentence case (first word capitalized, unless something always capitalized) rather title case (words except small words like “” “” capitalized). Chapter titles title case. Headings end period.Section headings (Chapter titles) sentence case (first word capitalized, unless something always capitalized) rather title case (words except small words like “” “” capitalized). Chapter titles title case. Headings end period.Never hard code stuff like “tibble 336,776 rows 19 columns.” happens update data? Instead, calculate numbers fly, “r scales::comma(x)” whenever x number thousands greater. Example: “tibble ‘r scales::comma(nrow(x))’ rows ‘r ncol(x)’ columns.”Never hard code stuff like “tibble 336,776 rows 19 columns.” happens update data? Instead, calculate numbers fly, “r scales::comma(x)” whenever x number thousands greater. Example: “tibble ‘r scales::comma(nrow(x))’ rows ‘r ncol(x)’ columns.”“” writing book.“” writing book.Package names bold: ggplot2 package graphics. general, reserve bolding package names. Use italics emphasis contexts.Package names bold: ggplot2 package graphics. general, reserve bolding package names. Use italics emphasis contexts.R code, anything might type console, always within backticks. Example: mtcars built-dataset.R code, anything might type console, always within backticks. Example: mtcars built-dataset.Function names always include parentheses: write pivot_wider(), pivot_wider.Function names always include parentheses: write pivot_wider(), pivot_wider.Add lots memes videos cartoons.Add lots memes videos cartoons.use code chunk names messes building book limits bookdown.use code chunk names messes building book limits bookdown.Make ample use comments, placed handy CMD-Shift-/ shortcut. notes everyone else working chapter, future .Make ample use comments, placed handy CMD-Shift-/ shortcut. notes everyone else working chapter, future .tables created gt package.tables created gt package.images gifs loaded knitr::include_graphics().images gifs loaded knitr::include_graphics().code chunk options allowed include = FALSE, echo = FALSE, fig.cap = “cap” message = FALSE loading packages like ggplot2 since prevents messages printing .code chunk options allowed include = FALSE, echo = FALSE, fig.cap = “cap” message = FALSE loading packages like ggplot2 since prevents messages printing .Interim data sets called x something sensible situation, like ch7 data set working Chapter 7. use names like data df, R commands.Interim data sets called x something sensible situation, like ch7 data set working Chapter 7. use names like data df, R commands.Students sometimes tentative. Don’t ! Edit aggressively. don’t like , delete . (disagree decision, can always get text back Github.) Move things around. Make chapter , keeping style chapters. Note 90% prose written . Cut anything don’t like.Students sometimes tentative. Don’t ! Edit aggressively. don’t like , delete . (disagree decision, can always get text back Github.) Move things around. Make chapter , keeping style chapters. Note 90% prose written . Cut anything don’t like.make mp4, can convert .gif using https://convertio.co/mp4-gif.make mp4, can convert .gif using https://convertio.co/mp4-gif.Everything Bayesian. confidence interval regression means 95% chance true value lies within interval. Use Rubin Causal Model potential outcomes define precisely “true value” talking . .Everything Bayesian. confidence interval regression means 95% chance true value lies within interval. Use Rubin Causal Model potential outcomes define precisely “true value” talking . .",
    "code": ""
  },
  {
    "path": "set-up-for-working-on-the-primer.html",
    "id": "stray-thoughts",
    "chapter": "Set Up for Working on The Primer",
    "heading": "11.17.1 Stray thoughts",
    "text": "Every chapter 5+ begins problem, decision must make. often toy, highly stylized problems. decisions realistic. , structure, problems parallel real problems people face, actual decisions must make.problem specified end “preamble,” untitled part chapter title first subpart. Example Chapter 8:person arrives Boston commuter station. thing know political party. old ? Two people arrive: Democrat Republican. odds Democrat 10% older Republican?different person arrives station. know nothing . attitude toward immigration exposed Spanish-speakers platform? ? certain ?actual problem someone might face? ! like problems. first requires creation predictive model. second necessitates causal model. rest chapter teaches reader create models. end chapter harkens back questions beginning.Might nice put meat story ? Perhaps. ideal world, “decision” faced complex just playing prediction game. Begin decision. real world problem trying solve? costs benefits different approaches? unknown thing trying estimate? Sampling, might : many people call? estimating one parameter — like vote share ballots come — might : much bet election outcome?data might directly connected problem. example, might running Senate campaign trying decide spend money . Spanish-speakers---train-platform data set directly related problem, isn’t unrelated. Indeed, first theme “validity” directly related issue: data relevant problem want solve?",
    "code": ""
  },
  {
    "path": "references.html",
    "id": "references",
    "chapter": "References",
    "heading": "References",
    "text": "",
    "code": ""
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preceptor’s Primer for Bayesian Data Science",
    "section": "",
    "text": "This isn’t the book you’re looking for.\n\n\n\n\nFirst, the book is for students in my classes. Everything about the book is designed to make the experience of those students better. I hope that some of the material here may be useful to people outside of this class.\nSecond, the book changes all the time. It is as up-to-date as possible.\nThird, I am highly opinionated about what matters and what does not. You might not share my views."
  },
  {
    "objectID": "index.html#dedication",
    "href": "index.html#dedication",
    "title": "Preceptor’s Primer for Bayesian Data Science",
    "section": "Dedication",
    "text": "Dedication\n\n\n\nAnd what is romantic, Kay —\nAnd what is love?\nNeed we ask anyone to tell us these things?"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Preceptor’s Primer for Bayesian Data Science",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis work builds on the contributions of many people in the R and Open Source communities. In particular, I would like to acknowledge extensive material taken from @isrs2014, @rds2016, @dapa2019, @mdrt2019, @bryan2019, @isrs2014, @bsms2012, @rds2016, @tmwr2020, @timbers, and @glmlm2019.\nAlboukadel Kassambara, Andrew Tran, Thomas Mock and others kindly allowed for the re-use and/or modification of their work.\n\n\n\n\n\n\n\n\nThanks to contributions from Harvard students, colleagues and random people I met on the internet: Albert Rivero, Nicholas Dow, Celine Vendler, Sophia Zheng, Maria Burzillo, Robert McKenzie, Deborah Gonzalez, Beau Meche, Evelyn Cai, Miro Bergam, Jessica Edwards, Emma Freeman, Cassidy Bargell, Yao Yu, Vivian Zhang, Ishan Bhatt, Mak Famulari, Tahmid Ahmed, Eliot Min, Hannah Valencia, Asmer Safi, Erin Guetzloe, Shea Jenkins, Thomas Weiss, Diego Martinez, Andy Wang, Tyler Simko, Jake Berg, Connor Rust, Liam Rust, Alla Baranovsky, Carine Hajjar, Diego Arias, Stephanie Yao and Tyler Simko.\n\n\n\n\n\n\n\n\nAlso, Becca Gill, Ajay Malik, Heather Li, Nosa Lawani, Stephanie Saab, Nuo Wen Lei, Anmay Gupta and Dario Anaya.\n\n\n\n\n\n\n\n\nAlso, Kevin Xu, Anmay Gupta, Sophia Zhu, Arghayan Jeiyasarangkan, Yuhan Wu, Ryan Southward, George Pentchev, Ahmet Atilla Colak, Mahima Malhotra, and Shreeram Patkar.\nI would like to gratefully acknowledge funding from The Derek Bok Center for Teaching and Learning at Harvard University, via its Digital Teaching Fellows and Learning Lab Undergraduate Fellows programs.\n\n\n\n\n\nDavid Kane(former) Preceptor in Statistical Methods and MathematicsDepartment of GovernmentHarvard University"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Preceptor’s Primer for Bayesian Data Science",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "05-probability.html",
    "href": "05-probability.html",
    "title": "5  Probability",
    "section": "",
    "text": "The usual touchstone of whether what someone asserts is mere persuasion or at least a subjective conviction, i.e., firm belief, is betting. Often someone pronounces his propositions with such confident and inflexible defiance that he seems to have entirely laid aside all concern for error. A bet disconcerts him. Sometimes he reveals that he is persuaded enough for one ducat but not for ten. For he would happily bet one, but at ten he suddenly becomes aware of what he had not previously noticed, namely that it is quite possible that he has erred. -— Immanuel Kant, Critique of Pure Reason\nThe central tension, and opportunity, in data science is the interplay between the data and the science, between our empirical observations and the models which we use to understand them. Probability is the language we use to explore that interplay; it connects models to data, and data to models.\nWhat does it mean that Trump had a 30% chance of winning election in the fall of 2016? That there is a 90% probability of rain today? That the dice at the casino are fair?\nProbability quantifies uncertainty. Think of probability as a proportion. The probability of an event occurring is a number from 0 to 1, where 0 means that the event is impossible and 1 means that the event is 100% certain.\nBegin with the simplest events: coin flips and dice rolls. The set of all outcomes is the sample space. With fair coins and dice, we know that:\nIf the probability of an outcome is unknown, we will often refer to it as an unknown parameter, something which we might use data to estimate. We usually use Greek letters to refer to parameters. Whenever we are talking about a specific probability (represented by a single value), we will use \\(\\rho\\) (the Greek letter “rho” but spoken aloud as “p” by us) with a subscript which specifies the exact outcome of which it is the probability. For instance, \\(\\rho_h = 0.5\\) denotes the probability of getting heads on a coin toss when the coin is fair. \\(\\rho_t\\) — spoken as “PT” or “P sub T” or “P tails” — denotes the probability of getting tails on a coin toss. This notation can become annoying if the outcome whose probability we seek is less concise. For example, we might write the probability of rolling a 1, 2 or 3 using a fair six-sided die as:\n\\[\n\\rho_{die\\ roll\\ is\\ 1,\\ 2\\ or\\ 3} = 0.5\n\\]\nWe will rarely write out the full definition of the event with the \\(\\rho\\) symbol. Instead, we will define an event “a” as when a rolled die equals 1, 2 or 3 and, then, write\n\\[\\rho_a = 0.5\\]\nA random variable is a function which produces a value from a sample set. A random variable can be either discrete — where the sample set has a limited number of members, like H or T for the result of a coin flip, or 2, 3, …, 12 for the sum of two die — or continuous (any value within a range). Probability is a claim about the value of a random variable, i.e., that you have a 50% probability of getting a 1, 2 or 3 when you roll a fair die.\nWe usually use capital letters for random variables. So, \\(C\\) might be our symbol for the random variable which is a coin toss and \\(D\\) might be our symbol for the random variable which is the sum of two dice. When discussing random variables in general, or when we grow tired of coming up with new symbols, we will use \\(Y\\).\nSmall letters refer to a single outcome or result from a random variable. \\(c\\) is the outcome from one coin toss. \\(d\\) is the result from one throw of the dice. The value of the outcome must come from the sample space. So, \\(c\\) can only take on two possible values: heads or tails. When discussing random variables in general, we use \\(y\\) to refer to one outcome of the random variable \\(Y\\). If there are multiple outcomes — if we have, for example, flipped the coin multiple times — then we use subscripts to indicate the separate outcomes: \\(y_1\\), \\(y_2\\), and so on. The symbol for an arbitrary outcome is \\(y_i\\), where \\(i\\) ranges from 1 through \\(N\\), the total number of events or experiments for which an outcome \\(y\\) was produced.\nThe only package we need in this chapter is tidyverse.\nTo understand probability more fully, we first need to understand distributions."
  },
  {
    "objectID": "05-probability.html#distributions",
    "href": "05-probability.html#distributions",
    "title": "5  Probability",
    "section": "\n5.1 Distributions",
    "text": "5.1 Distributions\nA variable in a tibble is a column, a vector of values. We sometimes refer to this vector as a “distribution.” This is somewhat sloppy in that a distribution can be many things, most commonly a mathematical formula. But, strictly speaking, a “frequency distribution” or an “empirical distribution” is a list of values, so this usage is not unreasonable.\n\n5.1.1 Scaling distributions\nConsider the vector which is the result of rolling one die 10 times.\n\nten_rolls <- c(5, 5, 1, 5, 4, 2, 6, 2, 1, 5)\n\nThere are other ways of storing the data in this vector. Instead of reporting every observation, we could record the number of times each value appears and the percentage of the total which this number accounts for.\n\n\n\n\n\n\n\n\nDistribution of Ten Rolls of a Fair Die\n    \n\nCounts and percentages reflect the same information\n    \n\n\nOutcome\n      Count\n      Percentage\n    \n\n\n1\n2\n0.2\n\n\n2\n2\n0.2\n\n\n4\n1\n0.1\n\n\n5\n4\n0.4\n\n\n6\n1\n0.1\n\n\n\n\n\n\nIn this case, with only 10 values, it is actually less efficient to store the data like this. But what happens when we have 1,000 rolls?\n\n\n\n\n\n\n\n\nDistribution of One Thousand Rolls of a Fair Die\n    \n\nCounts and percentages reflect the same information\n    \n\n\nOutcome\n      Count\n      Percentage\n    \n\n\n1\n190\n0.190\n\n\n2\n138\n0.138\n\n\n3\n160\n0.160\n\n\n4\n173\n0.173\n\n\n5\n169\n0.169\n\n\n6\n170\n0.170\n\n\n\n\n\n\nInstead of keeping around a vector of length 1,000, we can just keep 12 values — the 6 possible outcomes and their frequency — without losing any information.\nTwo distributions can be identical even if they are of very different lengths. Let’s compare our original distribution of 10 rolls of the die with another distribution which just features 100 copies of those 10 rolls.\n\nmore_rolls <- rep(ten_rolls, 100)\n\n\n\n\n\n\nThe two graphs have the exact same shape because, even though the vectors are of different lengths, the relative proportions of the outcomes are identical. In some sense, both vectors are from the same distribution. Relative proportions, not the total counts, are what matter.\n\n5.1.2 Normalizing distributions\nIf two distributions have the same shape, then they only differ by the labels on the y-axis. There are various ways of “normalizing” distributions to make them all the same scale. The most common scale is one in which the area under the distribution adds to 1, e.g., 100%. For example, we can transform the plots above to look like:\n\n\n\n\n\nWe sometimes refer to a distribution as “unnormalized” if the area under the curve does not add up to 1.\n\n5.1.3 Simulating distributions\nThere are two distinct concepts: a distribution and a set values drawn from that distribution. But, in everyday use, we use “distribution” for both. When given a distribution (meaning a vector of numbers), we often use geom_histogram() or geom_density() to graph it. But, sometimes, we don’t want to look at the whole thing. We just want some summary measures which report the key aspects of the distribution. The two most important attributes of a distribution are its center and its variation around that center.\nWe use summarize() to calculate statistics for a variable, a column, a vector of values or a distribution. Note the language sloppiness. For the purposes of this book, “variable,” “column,” “vector,” and “distribution” all mean the same thing. Popular statistical functions include: mean(), median(), min(), max(), n() and sum(). Functions which may be new to you include three measures of the “spread” of a distribution: sd() (the standard deviation), mad() (the scaled median absolute deviation) and quantile(), which is used to calculate an interval which includes a specified proportion of the values.\n\nThink of the distribution of a variable as an urn from which we can pull out, at random, values for that variable. Drawing a thousand or so values from that urn, and then looking at a histogram, can show where the values are centered and how they vary. Because people are sloppy, they will use the word distribution to refer to at least three related entities:\n\nthe (imaginary!) urn from which we are drawing values.\nall the values in the urn\nall the values which we have drawn from the urn, whether that be 10 or 1,000\n\nSloppiness in the usage of the word distribution is universal. However, keep three distinct ideas separate:\n\nThe unknown true distribution which, in reality, generates the data which we see. Outside of stylized examples in which we assume that a distribution follows a simple mathematical formula, we will never have access to the unknown true distribution. We can only estimate it. This unknown true distribution is often referred to as the data generating mechanism, or DGM. It is a function or black box or urn which produces data. We can see the data. We can’t see the urn.\nThe estimated distribution which, we think, generates the data which we see. Again, we can never know the unknown true distribution. But, by making some assumptions and using the data we have, we can estimate a distribution. Our estimate may be very close to the true distribution. Or it may be far away. The main task of data science to to create and use these estimated distributions. Almost always, these distributions are instantiated in computer code. Just as there is a true data generating mechanism associated with the (unknown) true distribution, there is an estimated data generating mechanism associated with the estimated ditribution.\nA vector of numbers drawn from the estimated distribution. Both true and estimated distributions can be complex animals, difficult to describe accurately and in detail. But a vector of numbers drawn from a distribution is easy to understand and use. So, in general, we work with vectors of numbers. When someone — either a colleague or a piece of R code — creates a distribution which we want to use to answer a question, we don’t really want the distribution itself. Rather, we want a vectors of “draws” from that distribution. Vectors are easy to work with! Complex computer code is not.\n\nAgain, people (including us!) will often be sloppy and use the same word, “distribution,” without making it clear whether they are talking about the true distribution, the estimated distribution, or a vector of draws from the estimated distribution. The same sloppiness applies to the use of the term data generating mechanism. Try not to be sloppy.\nMuch of the rest of the Primer involves learning how to work with distributions, which generally means working with the draws from those distributions. Fortunately, the usual rules of arithmetic apply. You can add/subtract/multiply/divide distributions by working with draws from those distributions, just as you can add/subtract/multiply/divide regular numbers."
  },
  {
    "objectID": "05-probability.html#probability-distributions",
    "href": "05-probability.html#probability-distributions",
    "title": "5  Probability",
    "section": "\n5.2 Probability distributions",
    "text": "5.2 Probability distributions\n\n\n\n\nBruno de Finetti, an Italian statistician who wrote a famous treatise on the theory of probability that began with the statement “PROBABILITY DOES NOT EXIST.”\n\n\n\n\nFor the purposes of this Primer, a probability distribution is a mathematical object which maps a set of outcomes to probabilities, where each distinct outcome has a chance of occurring between 0 and 1 inclusive. The probabilities must sum to 1. The set of possible outcomes, i.e., the sample space — heads or tails for the coin, 1 through 6 for a single die, 2 through 12 for the sum of a pair of dice — can be either discrete or continuous. Remember, discrete data can only take on certain values. Continuous data, like height and weight, can take any value within a range. The set of outcomes is the domain of the probability distribution. The range is the associated probabilities.\n\n\nAssume that a probability distribution is created by a probability function, a set function which maps outcomes to probabilities. The concept of a “probability function” is often split into two categories: probability mass functions (for discrete random variables) and probability density functions (for continuous random variables). As usual, we will be a bit sloppy, using the term probability distribution for both the mapping itself and for the function which creates the mapping.\nWe discuss three types of probability distributions: empirical, mathematical, and posterior.\nThe key difference between a distribution, as we have explored them in Section @ref(distributions), and a probability distribution is the requirement that the sum of the probabilities of the individual outcomes must be exactly 1. There is no such requirement for a distribution in general. But any distribution can be turned into a probability distribution by “normalizing” it. In this context, we will often refer to a distribution which is not (yet) a probability distribution as an “unnormalized” distribution.\nPay attention to notation. Recall that when we are talking about a specific probability (represented by a single value), we will use \\(\\rho\\) (the Greek letter “rho”) with a subscript which specifies the exact outcome of which it is the probability. For instance, \\(\\rho_h = 0.5\\) denotes the probability of getting heads on a coin toss when the coin is fair. \\(\\rho_t\\) — spoken as “PT” or “P sub T” or “P tails” — denotes the probability of getting tails on a coin toss. However, when we are referring to the entire probability distribution over a set of outcomes, we will use \\(P()\\). For example, the probability distribution of a coin toss is \\(P(\\text{coin})\\). That is, \\(P(\\text{coin})\\) is composed of the two specific probabilities (50% and 50%) mapped from the two values in the domain (Heads and Tails). Similarly, \\(P(\\text{sum of two dice})\\) is the probability distribution over the set of 11 outcomes (2 through 12) which are possible when you take the sum of two dice. \\(P(\\text{sum of two dice})\\) is made up of 11 numbers — \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_{12}\\) — each representing the unknown probability that the sum will equal their value. That is, \\(\\rho_2\\) is the probability of rolling a 2.\n\n5.2.1 Flipping a coin\nAll data science problems start with a question. Example: What are the chances of getting three heads in a row when flipping a fair coin? All questions are answered with the help of probability distributions.\nAn empirical distribution is based on data. You can think of this as the probability distribution created by running a simulation. In theory, if we increase the number of coins we flip in our simulation, the empirical distribution will look more and more similar to the mathematical distribution. The mathematical distribution is the Platonic form. The empirical distribution will often look like the mathematical probability distribution, but it will rarely be exactly the same.\nIn this simulation, there are 56 heads and 44 tails. The outcome will vary every time we run the simulation, but the proportion of heads to tails should not be too different if this coin is fair.\n\n# We are flipping one fair coin a hundreds times. We need to get the same result\n# each time we create this graphic because we want the results to match the\n# description in the text. Using set.seed() guarantees that the random results\n# are the same each time. We define 0 as tails and 1 as heads.\n\nset.seed(3)\n\ntibble(results = sample(c(0, 1), 100, replace = TRUE)) |> \n  ggplot(aes(x = results)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Flipping one coin a hundred times\",\n         x = \"Outcome\\nResult of Coin Flip\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = c(0, 1), \n                       labels = c(\"Heads\", \"Tails\")) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nA mathematical distribution is based on a mathematical formula. Assuming that the coin is perfectly fair, we should, on average, get heads as often as we get tails.\n\n\n\n\n\nThe distribution of a single observation is described by this formula.\n\\[ P(Y = y) = \\begin{cases} 1/2 &\\text{for }y= \\text{Heads}\\\\ 1/2 &\\text{for }y= \\text{Tails} \\end{cases}\\]\nWe sometimes do not know that the probability of heads and the probability of tails both equal 50%. In that case, we might write:\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ \\rho_T &\\text{for }y= \\text{Tails} \\end{cases}\\]\nYet, we know that, by definition, \\(\\rho_H + \\rho_T = 1\\), so we can rewrite the above asL\n\\[ P(Y = y) = \\begin{cases} \\rho_H &\\text{for }y= \\text{Heads}\\\\ 1- \\rho_H &\\text{for }y= \\text{Tails} \\end{cases}\\]\nCoin flipping (and related scenarios with only two possible outcomes) are such common problems, that the notation is often simplified further, with \\(\\rho\\) understood, by convention, to be the probability of heads. In that case, we can write the mathematical distribution is two canonical forms:\n\\[P(Y) = Bernoulli(\\rho)\\]\nand\n\\[y_i \\sim Bernoulli(\\rho)\\]\nAll five of these versions mean the same thing! The first four describe the mathematical probability distribution for a fair coin. The capital \\(Y\\) within the \\(P()\\) indicates a random variable. The fifth highlights one “draw” from that random variable, hence the lower case \\(y\\) and the subscript \\(i\\).\nMost probability distributions do not have special names, which is why we will use the generic symbol \\(P\\) to define them. But some common probability distributions do have names, like “Bernoulli” in this case.\nIf the mathematical assumptions are correct, then as your sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nA posterior distribution is based on beliefs and expectations. It displays your belief about things you can’t see right now. You may have posterior distributions for outcomes in the past, present, or future.\nIn the case of the coin toss, the posterior distribution changes depending on your beliefs. For instance, let’s say your friend brought a coin to school and asked to bet you. If the result is heads, you have to pay them $5. In that case, your posterior probability distribution might look like this:\n\n\n\n\n\nThe full terminology is mathematical (or empirical or posterior) probability distribution. But we will often shorten this to just mathematical (or empirical or posterior) distribution. The word “probability” is understood, even if it is not present.\n\nRecall the question with which we started this section: What are the chances of getting three heads in a row when flipping a fair coin? To answer this question, we need to use a probability distribution as our data generating mechanism. Fortunately, the rbinom() function allows us to generate the results for coin flips. For example:\n\nrbinom(n = 10, size = 1, prob = 0.5)\n\n [1] 1 1 0 1 1 0 0 0 0 0\n\n\ngenerates the results of 10 coin flips, where a result of heads is presented as 1 and tails as 0. With this tool, we can generate 1,000 draws from our experiment:\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5))\n\n# A tibble: 1,000 × 3\n   toss_1 toss_2 toss_3\n    <int>  <int>  <int>\n 1      0      1      1\n 2      0      1      1\n 3      0      1      0\n 4      0      0      1\n 5      1      1      0\n 6      1      0      1\n 7      1      0      0\n 8      1      0      1\n 9      0      0      1\n10      0      1      1\n# … with 990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nBecause the flips are independent, we can consider each row to be a draw from the experiment. Then, we simply count up the proportion of experiments in which resulted in three heads.\n\ntibble(toss_1 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_2 = rbinom(n = 1000, size = 1, prob = 0.5),\n       toss_3 = rbinom(n = 1000, size = 1, prob = 0.5)) |> \n  mutate(three_heads = toss_1 + toss_2 + toss_3 == 3) |> \n  summarize(chance = mean(three_heads))\n\n# A tibble: 1 × 1\n  chance\n   <dbl>\n1  0.104\n\n\nThis is close enough to the correct answer of \\(1/8\\)th. If we increase the sample size, we will get closer to the truth.\nThis is our first example of using a data generating mechanism — meaning rbinom() — to answer a question. We will see many more in the chapters to come.\n\n5.2.2 Rolling two dice\nWe get an empirical distribution by rolling two dice a hundred times, either by hand or with a computer simulation. The result is not identical to the mathematical distribution because of the inherent randomness of the real world and/or of simulation.\n\n# In the coin example, we create the vector ahead of time, and then assigned\n# that vector to a tibble. There was nothing wrong with that approach. And we\n# could do the same thing here. But the use of map_* functions is more powerful,\n# although it requires creating the 100 rows of the tibble at the start and then\n# doing things \"row-by_row.\"\n\nset.seed(1)\n\nemp_dist_dice <- tibble(ID = 1:100) |> \n  mutate(die_1 = map_dbl(ID, ~ sample(c(1:6), size = 1))) |> \n  mutate(die_2 = map_dbl(ID, ~ sample(c(1:6), size = 1))) |> \n  mutate(sum = die_1 + die_2) |> \n  ggplot(aes(x = sum)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 1, \n                   color = \"white\") +\n    labs(title = \"Empirical Probability Distribution\",\n         subtitle = \"Sum from rolling two dice, replicated one hundred times\",\n         x = \"Outcome\\nSum of Two Die\",\n         y = \"Probability\") +\n    scale_x_continuous(breaks = seq(2, 12, 1), labels = 2:12) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\nemp_dist_dice\n\n\n\n\nWe might consider labeling the y-axis in plots of empirical distributions as “Proportion” rather than “Probability” since it is an actual proportion, calculated from real (or simulated) data. We will keep it as “Probability” since we want to emphasize the parallels between mathematical, empirical and posterior probability distributions.\nOur mathematical distribution tells us that, with a fair dice, the probability of getting 1, 2, 3, 4, 5, and 6 are equal: there is a 1/6 chance of each. When we roll two dice at the same time and sum the numbers, the values closest to the middle are more common than values at the edge because there are more combinations of numbers that add up to the middle values.\n\n\n\n\n\n\\[ P(Y = y) = \\begin{cases} \\dfrac{y-1}{36} &\\text{for }y=1,2,3,4,5,6 \\\\ \\dfrac{13-y}{36} &\\text{for }y=7,8,9,10,11,12 \\\\ 0 &\\text{otherwise} \\end{cases} \\]\nThe posterior distribution for rolling two dice a hundred times depends on your beliefs. If you take the dice from your Monopoly set, you have reason to believe that the assumptions underlying the mathematical distribution are true. However, if you walk into a crooked casino and a host asks you to play craps, you might be suspicious, just as in the “flipping a coin example” the word “suspicious” means you no longer trust the “population” where the mathematical and empircal distribution drawn their data from. For example, in craps, a come-out roll of 7 and 11 is a “natural,” resulting in a win for the “shooter” and a loss for the casino. You might expect those numbers to occur less often than they would with fair dice. Meanwhile, a come-out roll of 2, 3 or 12 is a loss for the shooter. You might also expect values like 2, 3 and 12 to occur more frequently. Your posterior distribution might look like this:\n\n\n\n\n\nSomeone less suspicious of the casino would have a posterior distribution which looks more like the mathematical distribution.\n\n5.2.3 Presidential elections\nNow let’s say we are building probability distributions for political events, like a presidential election. We want to know the probability that Democratic candidate wins X electoral votes, where X comes from the range of possible outcomes: 0 to 538. (The total number of electoral votes in US elections since 1964 is 538.)\nThe empirical distribution in this case could involve looking into past elections in the United States and counting the number of electoral votes that the Democrats won in each. For the empirical distribution, we create a tibble with electoral vote results from past elections. Looking at elections since 1964, we can observe that the number of electoral votes that the Democrats received in each one is different. Given that we only have 15 entries, it is difficult to draw conclusions or make predictions based off of this empirical distribution.\n\n\n\n\n\nWe can build a mathematical distribution for X which assumes that the chances of the Democratic candidate winning any given state’s electoral votes is 0.5 and the results from each state are independent.\n\n\n\n\n\nIf our assumptions about this mathematical distribution are correct (they are not), then as the sample size increase, the empirical distribution should look more and more similar to the our mathematical distribution.\nHowever, the data from past elections is more than enough to demonstrate that the assumptions of the mathematical probability distribution above do not work for electoral votes. The model assumes that the Democrats have a 50% chance of receiving each of the 538 votes. Just looking at the mathematical probability distribution, we can observe that receiving 13 or 17 or 486 votes out of 538 would be extreme and almost impossible if the mathematical model were accurate. However, our empirical distribution shows that such extreme outcomes are quite common. Presidential elections have resulted in much bigger victories or defeats than this distribution seems to allow for.\nThe posterior distribution of electoral votes is a popular topic, and an area of strong disagreement, among data scientists. Consider this posterior from FiveThirtyEight.\n\n\n\n\n\nHere is a posterior from the FiveThirtyEight website from August 13, 2020. This was created using the same data as the above distribution, but simply displayed differently. For each electoral result, the height of the bar represents the probability that a given event will occur. However, there are no lablels y-axis telling us what the specific probability of each outcome is. And that is OK! The specific values are not that useful. If we removed the labels on our y-axes, would it matter?\n\n\n\n\n\nHere is the posterior from The Economist, also from August 13, 2020. This looks confusing at first because they chose to merge the axes for Republican and Democratic electoral votes. We can tell that The Economist was less optimistic, relative to FiveThirtyEight, about Trump’s chances in the election.\n\n\n\n\n\nThese two models, built by smart people using similar data sources, have reached fairly different conclusions. Data science is difficult! There is not one “right” answer. Real life is not a problem set.\n\n\n\n\nWatch the makers of these two models throw shade at each other on Twitter! Eliot Morris is one of the primary authors of the Economist model. Nate Silver is in charge of 538. They don’t seem to be too impressed with each other’s work! More smack talk here and here.\n\n\n\n\nThere are many political science questions you could explore with posterior distributions. They can relate to the past, present, or future.\n\nPast: How many electoral votes would Hilary Clinton have won if she had picked a different VP?\nPresent: What are the total campaign donations from Harvard faculty?\nFuture: How many electoral votes will the Democratic candidate for president win in 2024?\n\n5.2.4 Height\nQuestion: What is the height of the next adult male we will meet?\nThe three examples above are all discrete probability distributions, meaning that the outcome variable can only take on a limited set of values. A coin flip has two outcomes. The sum of a pair of dice has 11 outcomes. The total electoral votes for the Democratic candidate has 539 possible outcomes. In the limit, we can also create continuous probability distributions which have an infinite number of possible outcomes. For example, the average height for an American male could be any real number between 0 inches and 100 inches. (Of course, an average value anywhere near 0 or 100 is absurd. The point is that the average could be 68.564, 68.5643, 68.56432 68.564327, or any real number.)\nAll the characteristics for discrete probability distributions which we reviewed above apply just as much to continuous probability distributions. For example, we can create mathematical, empirical and posterior probability distributions for continuous outcomes just as we did for discrete outcomes.\nThe empirical distribution involves using data from the National Health and Nutrition Examination Survey (NHANES). What we are doing here is instead making an model by ourself using some mathematical formula, we use the actual data, we can get the data from either simulated by our own like in the “flipping a coin” and “Rolling two dice” scenario, or we used the data from someone else, like the presidential election and this scenario.\n\n\n\n\n\nMathematical distribution is complete based on mathematical formula and assumptions like in the Flipping a coin session we assume that the coin is an perfectly fair coin where where the probability landing on heads or tails is equal. In this case, we assume that the average hight of men is 175 cm, as well as the standard deviation for height is around 9 cm. When we have these two values, the average which we also called the mean, and standard deviation (sd), we can create an normal distribution using the rnorm() function. And an normal distribution is an good approximation and generalization for height in our scenario.\nMathematical Distribution:\n\n\n\n\n\nAgain, the Normal distribution which is an probability distribution that is symmetric about the mean is described by this formula.\n\\[y_i \\sim N(\\mu, \\sigma^2)\\].\nEach value \\(y_i\\) is drawn from a normal distribution with parameters \\(\\mu\\) for the mean and \\(\\sigma\\) for the standard deviation. If the mathematical assumptions are correct in this case the two parameters \\(\\mu\\) and \\(\\sigma\\), then as our sample size increases, the empirical probability distribution will look more and more like the mathematical distribution.\nThe posterior distribution for heights depends on the context. Are we considering all the adult men in America? In that case, our posterior would probably look a lot like the empirical distribution using NHANES data. If we are being asked about the distribution of heights among players in the NBA, then our posterior might look like:\n\n\n\n\n\nComments:\n\nContinuous variables are a myth. Nothing that can be represented on a computer is truly continuous. Even something which appears continuous, like height, actually can only take on a (very large) set of discrete variables.\nThe math of continuous probability distributions can be tricky. Read a book on mathematical probability for all the messy details. Little of that matters in applied work.\nThe most important difference is that, with discrete distributions, it makes sense to estimate the probability of a specific outcome. What is the probability of rolling a 9? With continuous distributions, this makes no sense because there are an infinite number of possible outcomes. With continuous variables, we only estimate intervals.\n\nDon’t worry about the distinctions between discrete and continuous outcomes, or between the discrete and continuous probability distributions which we will use to summarize our beliefs about those outcomes. The basic intuition is the same in both cases.\n\n5.2.5 Joint distributions\n\nRecall that \\(P(\\text{coin})\\) is the probability distribution for the result of a coin toss. It includes two parts, the probability of heads (\\(\\rho_h\\)) and the probability of tails (\\(\\rho_t\\)). This is a univariate distribution because there is only one outcome, which can be heads or tails. If there is more than one outcome, then we have a joint distribution.\n\nJoint distributions are also mathematical objects that cover a set of outcomes, where each distinct outcome has a chance of occurring between 0 and 1 and the sum of all chances must equal 1. The key to a joint distribution is it measures the chance that both events A and B will occur. The notation is \\(P(A, B)\\).\nLet’s say that you are rolling two six-sided dice simultaneously. Die 1 is weighted so that there is a 50% chance of rolling a 6 and a 10% chance of each of the other values. Die 2 is weighted so there is a 50% chance of rolling a 5 and a 10% chance of rolling each of the other values. Let’s roll both dice 1,000 times. In previous examples involving two dice, we cared about the sum of results and not the outcomes of the first versus the second die of each simulation. With a joint distributions, the order matters; so instead of 11 possible outcomes on the x-axis of our distribution plot (ranging from 2 to 12), we have 36. Furthermore, a 2D probability distribution is not sufficient to represent all of the variables involved, so the joint distribution for this example is displayed using a 3D plot.\n\n\n\n\n\n\n\n5.2.6 Conditional distrubutions\nImagine that 60% of people in a community have a disease. A doctor develops a test to determine if a random person has the disease. However, this test isn’t 100% accurate. There is an 80% probability of correctly returning positive if the person has the disease and 90% probability of correctly returning negative if the person does not have the disease.\nThe probability of a random person having the disease is 0.6. Since each person either has the disease or doesn’t (those are the only two possibilities), the probability that a person does not have the disease is \\(1 - 0.6 = 0.4\\).\n\n\n\n\n\n\nIf the random person has the disease, then we go up the top branch. The probability of an infected person testing positive is 0.8 because the test is 80% sure of correctly returning positive when the person has the disease.\nBy the same logic, if the random person does not have the disease, we go down the bottom branch. The probability of the person incorrectly testing positive is 0.1.\n\nWe decide to go down the top branch if our random person has the disease. We go down the bottom branch if they do not. This is called conditional probability. The probability of testing positive is dependent on whether the person has the disease.\nHow would you express this in statistical notation? \\(P(A|B)\\) is the same thing as the probability of A given B. \\(P(A|B)\\) essentially means the probability of A if we know for sure the value of B. Note that \\(P(A|B)\\) is not the same thing as \\(P(B|A)\\).\nIn summary, we work with three main categories of probability distributions. First, p(A) is the probability distribution for event A. This is sometimes refered to as a univariate probability distribution because there is only one random variable. Second, p(A, B) is the joint probability distribution of A and B. Third, p(A | B) is the conditional probability distribution of A given that B has taken on a specific value. This is often written as p(A | B = b)."
  },
  {
    "objectID": "05-probability.html#list-columns-and-map-functions",
    "href": "05-probability.html#list-columns-and-map-functions",
    "title": "5  Probability",
    "section": "\n5.3 List-columns and map functions",
    "text": "5.3 List-columns and map functions\nBefore working with probability models, we need to expand our collection of R tricks by understanding list-columns and map_* functions. Recall that a list is different from an atomic vector. In atomic vectors, each element of the vector has one value. Lists, however, can contain vectors, and even more complex objects, as elements.\n\nx <- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx\n\n[[1]]\n[1]  4 16  9\n\n[[2]]\n[1] \"A\" \"Z\"\n\n\nx is a list with two elements. The first element is a numeric vector of length 3. The second element is a character vector of length 2. We use [[]] to extract specific elements.\n\nx[[1]]\n\n[1]  4 16  9\n\n\nThere are a number of built-in R functions that output lists. For example, the ggplot objects you have been making store all of the plot information in lists. Any function that returns multiple values can be used to create a list output by wrapping that returned object with list().\n\nx <- rnorm(10)\n\n# range() returns the min and max of the argument \n\ntibble(col_1 = list(range(x))) \n\n# A tibble: 1 × 1\n  col_1    \n  <list>   \n1 <dbl [2]>\n\n\nNotice this is a 1-by-1 tibble with one observation, which is a list of one element. Voila! You have just created a list-column.\nIf a function returns multiple values as a vector, like range() does, you must use list() as a wrapper if you want to create a list-column.\nA list column is a column of your data which is a list rather than an atomic vector. As with stand-alone list objects, you can pipe to str() to examine the column.\n\n# tibble() is what we use to generate a tibble, it acts sort of like the mutate(), but mutate() needs a data frame to add new column, tibble can survive on itself.\ntibble(col_1 = list(range(x))) |>\n  str()\n\ntibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n $ col_1:List of 1\n  ..$ : num [1:2] -1.01 1.42\n\n\nWe can use map_* functions to both create a list-column and then, much more importantly, work with that list-column afterwards.\n\n# .x is col_1 from tibble and ~ sum(.) is the formula\ntibble(col_1 = list(range(x))) |>\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |> \n  str()\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ col_1:List of 1\n  ..$ : num [1:2] -1.01 1.42\n $ col_2: num 0.415\n\n\nmap_* functions, like map_dbl() in this example, take two key arguments, .x (the data which will be acted on) and .f (the function which will act on this data). Here, .x is the data in col_1, which is a list-column. .f is the function sum(). However, we can not simply write map_dbl(col_1, sum). Instead, each use of map_* functions requires the use of a tilde — a ~ — to indicate the start of the function and the use of a dot — a . — to specify where the data goes in the function.\nmap_* functions are a family of functions, with the suffix specifying the type of the object to be returned. map() itself returns a list. map_dbl() returns a double. map_int() returns an integer. map_chr() returns a character, and so on.\nTo summarise map function and map_* functions could both convert an data (vector or list) with the specific denoted functions or formula you set, and always results in list we called the “list Column”. There are two arguments in the map() function as well as map_*() function the .x and .f, the .x and .f placed in the map() functions and map_*()functions like this map(.x,.f), where .x could either be a list or a vector, and .f either be a direct function like .f=mean, or an formula like .f= ~mean(.x), remember to put the ~ if you are using .f an formula, the difference between map and map_* function is when you know and want the outcome of the data to be specific data vector like (double, logical,character,integer) rather than an general list in map(), you can use map_* instead of map to organized your list column.\n\ntibble(ID = 1) |> \n  mutate(col_1 = map(ID, ~range(rnorm(10)))) |>\n  mutate(col_2 = map_dbl(col_1, ~ sum(.))) |> \n  mutate(col_3 = map_int(col_1, ~ length(.))) |> \n  mutate(col_4 = map_chr(col_1, ~ sum(.))) |> \n  str()\n\ntibble [1 × 5] (S3: tbl_df/tbl/data.frame)\n $ ID   : num 1\n $ col_1:List of 1\n  ..$ : num [1:2] -1.45 1.31\n $ col_2: num -0.143\n $ col_3: int 2\n $ col_4: chr \"-0.142989\"\n\n\nConsider a more detailed example:\n\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) |> \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) |> \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) |> \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))\n\n# A tibble: 3 × 4\n     ID draws        max min_max  \n  <int> <list>     <dbl> <list>   \n1     1 <dbl [10]> 0.758 <dbl [2]>\n2     2 <dbl [10]> 1.06  <dbl [2]>\n3     3 <dbl [10]> 1.69  <dbl [2]>\n\n\nThis flexibility is only possible via the use of list-columns and map_* functions. This workflow is extremely common. We start with an empty tibble, using ID to specify the number of rows. With that skeleton, each step of the pipe adds a new column, working off a column which already exists."
  },
  {
    "objectID": "05-probability.html#two-models",
    "href": "05-probability.html#two-models",
    "title": "5  Probability",
    "section": "\n5.4 Two models",
    "text": "5.4 Two models\n\n\n\nThe simplest possible setting for inference involves two models — meaning two possible states of the world — and two outcomes from an experiment. Imagine that there is a disease — Probophobia, an irrational fear of probability — which you either have or don’t have. We don’t know if you have the diseases, but we do assume that there are only two possibilities.\nWe also have a test which is 99% accurate when given to a person who has Probophobia. Unfortunately, the test is only 50% accurate for people who do not have Probophobia. In this experiment, there only two possible outcomes: a positive and a negative result on the test.\nQuestion: If you test positive, what is the probability that you have Probophobia?\nMore generally, we are estimating a conditional probability. Conditional on the outcome of a postive test, what is the probability that you have Probophobia? Mathematically, we want:\n\\[ P(\\text{Probophobia | Test = Postive} ) \\]\nTo answer this question, we need to use the tools of joint and conditional probability from earlier in the Chapter. We begin by building, by hand, the joint distribution of the possible models (you have the Probophobia or you do not) and of the possible outcomes (you test positive or negative). Building the joint distribution involves assuming that each model is true and then creating the distribution of outcomes which might occur if that assumption is true.\nFor example, assume you have Probophobia. There is then a 50% chance that you test positive and a 50% chance you test negative. Similarly, if we assume that the second model is true — that you don’t have Probophobia — then there is 1% chance you test positive and a 99% you chance negative. Of course, for you (or any individual) we do not know for sure what is happening. We do not know if you have the disease. We do not know what your test will show. But we can use these relationships to construct the joint distribution.\n\n\n\n\n\n# Pipes generally start with tibbles, so we start with a tibble which just\n# includes an ID variable. We don't really use ID. It is just handy for getting\n# organized. We call this object `jd_disease`, where the `jd` stands for\n# joint distribution.\n\nsims <- 10000\n\njd_disease <- tibble(ID = 1:sims, have_disease = rep(c(TRUE, FALSE), 5000)) |>\n  mutate(positive_test =\n           if_else(have_disease,\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.99)),\n                   map_int(have_disease, ~ rbinom(n = 1, size = 1, p = 0.5))))\n\n\n\njd_disease\n\n# A tibble: 10,000 × 3\n      ID have_disease positive_test\n   <int> <lgl>                <int>\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     6 FALSE                    0\n 7     7 TRUE                     1\n 8     8 FALSE                    1\n 9     9 TRUE                     1\n10    10 FALSE                    0\n# … with 9,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe first step is to simply create an tibble that consists of the simulated data we need to plot our distribution. Keep in mind that in the setting we have two different probabilities and they are completely separate from each other and we want to keep the two probabilities and the disease results in two and only two columns so that we can graph using the ggplot() function. And that’s why we used the rep and seq functions when creating the table, we used the seq function to set the sequence we wants, in this case is only two numbers, 0.01 (99% accuracy for testing negative if no disease, therefore 1% for testing positive if no disease) and 0.5 (50% accuracy for testing positive/negative if have disease), then we used the rep functions to repeat the process 10,000 times for each probability, in total 20,000 times. Note that this number “20,000” also represent the number of observations in our simulated data, we simulated 20,000 results from testing, where 10,000 results from the have-disease group and 10,000 for the no-disease group, we often use the capital N to represent the population, in this simulated data \\(N=20,000\\).\nPlot the joint distribution:\n\n\n\n\n\nBelow is a joint distribution displayed in 3D. Instead of using the “jitter” feature in R to unstack the dots, we are using a 3D plot to visualize the number of dots in each box. The number of people who correctly test negative is far greater than of the other categories. The 3D plot shows the total number of cases for each section (True positive, True negative, False positive, False negative),the 3D bar coming from those combinations. Now,pay attention to the two rows of the 3D graph, if you trying to add up the length of the 3D bar for the top two sections and the bottom two sections, they should be equal to each other, where each have 10,000 case. This is because we simulate the experience in two independent and separate world one in the have-disease world and one in the no-disease world.\n\n\n\n\n\n\n\n\nThis Section is called “Two Models” because, for each person, there are two possible states of the world: have the disease or not have the disease. By assumption, there are no other outcomes. We call these two possible states of the world “models,” even though they are very simple models.\nIn addition to the two models, we have two possible results of our experiment on a given person: test positive or test negative. Again, this is an assumption. We do not allow for any other outcome. In coming sections, we will look at more complex situations where we consider more than two models and more than two possible results of the experiment. In the meantime, we have built the unnormalized joint distribution for models and results. This is a key point! Look back earlier in this Chapter for discussions about both unnormalized distributions and joint distributions.\nWe want to analyze these plots by looking at different slices. For instance, let’s say that you have tested positive for the disease. Since the test is not always accurate, you cannot be 100% certain that you have it. We isolate the slice where the test result equals 1 (meaning positive).\n\njd_disease |> \n  filter(positive_test == 1)\n\n# A tibble: 7,484 × 3\n      ID have_disease positive_test\n   <int> <lgl>                <int>\n 1     1 TRUE                     1\n 2     2 FALSE                    1\n 3     3 TRUE                     1\n 4     4 FALSE                    1\n 5     5 TRUE                     1\n 6     7 TRUE                     1\n 7     8 FALSE                    1\n 8     9 TRUE                     1\n 9    11 TRUE                     1\n10    12 FALSE                    1\n# … with 7,474 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nMost people test positive are infected This is a result for common diseases like cold. We can easily create an unnormalized conditional distribution with:\n\n\n\n\n\nfilter() transforms a joint distribution into a conditional distribution.\nTurn this unnormalized distribution into a posterior probability distribution:\n\n\n\n\n\nIf we zoom in on the plot, about 70% of people who tested positive have the disease and 30% who tested positive do not have the disease. In this case, we are focusing on one slice of the probability distribution where the test result was positive. There are two disease outcomes: positive or negative. By isolating a section, we are looking at a conditional distribution. Conditional on a positive test, you can visualize the likelihood of actually having the disease versus not.\nNow recalled the question we asked at the start of the session: If you test positive, what is the probability that you have Probophobia?\nBy looking at the posterior graph we just create, we can answer this question easily: With a positive test, you can be almost 70% sure that you have Probophobia, however there is a good chance about 30% that you receive a false positive, so don’t worry too much there is still about a third of hope that you get the wrong result\nNow let’s consider the manipulation of this posterior, here is another question. Question : 10 people walks up to testing center, 5 of them tested negative, 5 of them tested positive, what is the probability of at least 6 people is actually healthy? \n\ntibble(test = 1:100000) |>\n  mutate(person1 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person2 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person3 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person4 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person5 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.3))) |>\n  mutate(person6 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person7 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person8 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person9 =  map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  mutate(person10 = map_int(test, ~ rbinom(n = 1, size = 1, p = 0.7))) |>\n  select(!test) |> \n  \nmutate(sum = rowSums(across(person1:person10))) |>\n  \nggplot(aes(sum)) +\n  geom_histogram(aes(y = after_stat(count/sum(count))),\n                 binwidth = 1,\n                   color = \"white\") +\n  scale_x_continuous(breaks = c(0:10)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()  \n\n\n\n\n\nThis Stat 110 Animations video does a really good job of explaining similar concepts."
  },
  {
    "objectID": "05-probability.html#three-models",
    "href": "05-probability.html#three-models",
    "title": "5  Probability",
    "section": "\n5.5 Three models",
    "text": "5.5 Three models\n\n\n\n\n\n\n\nImagine that your friend gives you a bag with two marbles. There could either be two white marbles, two black marbles, or one of each color. Thus, the bag could contain 0% white marbles, 50% white marbles, or 100% white marbles. Respectively, the proportion, \\(p\\), of white marbles could be 0, 0.5, or 1.\nQuestion: What is the chance of the bag contains exactly two white marbles, given that when we selected the marbles three times, everytime we select a white marble?\n\\[ P(\\text{2 White Marbles in bag | White Marbles Sampled = 3} ) \\]\nJust as during the Probophobia models, in order to answer this question, we need to start up with the simulated data and then graphing out the joint distribution of this sinerio because we need to considered all possible outcomes of this model, and then based on the joint distribution we can slice out the the part we want (Conditional distribution) in the end making an posterior graph as well as normalizing it to see the probability.\nStep 1: Simulate the data into an tibble\nLet’s say you take a marble out of the bag, record whether it’s black or white, then return it to the bag. You repeat this three times, observing the number of white marbles you see out of three trials. You could get three whites, two whites, one white, or zero whites as a result of this trial. We have three models (three different proportions of white marbles in the bag) and four possible experimental results. Let’s create 3,000 draws from this joint distribution:\n\n# Create the joint distribution of the number of white marbles in the bag\n# (in_bag) and the number of white marbles pulled out in the sample (in_sample),\n# one-by-one. in_bag takes three possible values: 0, 1 and 2, corresponding to\n# zero, one and two white marbles potentially in the bag.\n\nset.seed(3)\nsims <- 10000\n\n# We also start off with a tibble. It just makes things easier\n\njd_marbles <- tibble(ID = 1:sims) |> \n  \n  # For each row, we (randomly!) determine the number of white marbles in the\n  # bag. We do not know why the `as.integer()` hack is necessary. Shouldn't\n  # `map_int()` automatically coerce the result of `sample()` into an integer?\n  \n  mutate(in_bag = map_int(ID, ~ as.integer(sample(c(0, 1, 2), \n                                                  size = 1)))) |>\n  \n  # Depending on the number of white marbles in the bag, we randomly draw out 0,\n  # 1, 2, or 3 white marbles in our experiment. We need `p = ./2` to transform\n  # the number of white marbles into the probability of drawing out a white\n  # marble in a single draw. That probability is either 0%, 50% or 100%.\n  \n  mutate(in_sample = map_int(in_bag, ~ rbinom(n = 1, \n                                              size = 3, \n                                              p = ./2))) \n\njd_marbles\n\n# A tibble: 10,000 × 3\n      ID in_bag in_sample\n   <int>  <int>     <int>\n 1     1      0         0\n 2     2      1         3\n 3     3      2         3\n 4     4      1         1\n 5     5      2         3\n 6     6      2         3\n 7     7      1         0\n 8     8      2         3\n 9     9      0         0\n10    10      1         2\n# … with 9,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nStep 2: Plot the joint distribution:\n\n# The distribution is unnormalized. All we see is the number of outcomes in each\n# \"bucket.\" Although it is never stated clearly, we are assuming that there is\n# an equal likelihood of 0, 1 or 2 white marbles in the bag.\n\njd_marbles |>\n  ggplot(aes(x = in_sample, y = in_bag)) +\n    geom_jitter(alpha = 0.5) +\n    labs(title = \"Black and White Marbles\",\n         subtitle = \"More white marbles in bag mean more white marbles selected\",\n         x = \"White Marbles Selected\",\n         y = \"White Marbles in the Bag\") +\n    scale_y_continuous(breaks = c(0, 1, 2)) +\n  theme_classic()\n\n\n\n\nHere is the 3D visualization:\n\n\n\n\n\n\nThe y-axes of both the scatterplot and the 3D visualization are labeled “Number of White Marbles in the Bag.” Each value on the y-axis is a model, a belief about the world. For instance, when the model is 0, we have no white marbles in the bag, meaning that none of the marbles we pull out in the sample will be white.\nNow recalls the question, we essentially only care about the fourth column in the joint distribution (x-axis=3) because the question is asking us to create a conditional distribution given that fact that 3 marbles were selected. Therefore, we could isolate the slice where the result of the simulation involves three white marbles and zero black ones. Here is the unnormalized probability distribution.\nStep 3: Plot the unnormalized conditional distribution.\n\n# The key step is the filter. Creating a conditional distribution from a joint\n# distribution is the same thing as filtering that joint distribution for a\n# specific value. A conditional distribution is a \"slice\" of the joint\n# distribution, and we take that slice with filter().\n\njd_marbles |> \n  filter(in_sample == 3) |> \n  ggplot(aes(in_bag)) +\n    geom_histogram(binwidth = 0.5, color = \"white\") +\n    labs(title = \"Unnormalized Conditional Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Count\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    theme_classic()\n\n\n\n\nStep 4: Plot the normalize posterior distribution. Next, let’s normalize the distribution.\n\njd_marbles |> \n  filter(in_sample == 3) |> \n  ggplot(aes(in_bag)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   binwidth = 0.5, \n                   color = \"white\") +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of white marbles in bag given that three were selected in the sample\",\n         x = \"Number of White Marbles in the Bag\",\n         y = \"Probability\") +\n    coord_cartesian(xlim = c(0, 2)) +\n    scale_x_continuous(breaks = c(0, 1, 2)) +\n    scale_y_continuous(labels = \n                         scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nThis plot makes sense because when all three marbles you draw out of the bag are white, there is a pretty good chance that there are no black marbles in the bag. But you can’t be certain! It is possible to draw three white even if the bag contains one white and one black. However, it is impossible that there are zero white marbles in the bag.\nLastly let’s answer the question: What is the chance of the bag contains exactly two white marbles, given that when we selected the white marbles three times, everytime we select a white marble?\nAnswer: As the Posterior Probability Distribution shows (x-axis=2), the chance of the bag contains exactly two white marbles given that we select 3 white marbles out of three tries is about 85%."
  },
  {
    "objectID": "05-probability.html#n-models",
    "href": "05-probability.html#n-models",
    "title": "5  Probability",
    "section": "\n5.6 N models",
    "text": "5.6 N models\n\n\n\n\n\n\n\n\nAssume that there is a coin with \\(\\rho_h\\). We guarantee that there are only 11 possible values of \\(\\rho_h\\): \\(0, 0.1, 0.2, ..., 0.9, 1\\). In other words, there are 11 possible models, 11 things which might be true about the world. This is just like situations we have previously discussed, except that there are more models to consider.\nWe are going to run an experiment in which you flip the coin 20 times and record the number of heads. What does this result tell you about the value of \\(\\rho_h\\)? Ultimately, we will want to calculate a posterior distribution of \\(\\rho_h\\), which is written as p(\\(\\rho_h\\)).\nQuestion: What is the probability of getting exactly 8 heads out of 20 tosses?\nTo start, it is useful to consider all the things which might happen if, for example, \\(\\rho_h = 0.4\\). Fortunately, the R functions for simulating random variables makes this easy.\n\n\n\n\n\nFirst, notice that many different things can happen! Even if we know, for certain, that \\(\\rho_h = 0.4\\), many outcomes are possible. Life is remarkably random. Second, the most likely result of the experiment is 8 heads, as we would expect. Third, we have transformed the raw counts of how many times each total appeared into a probability distribution. Sometimes, however, it is convenient to just keep track of the raw counts. The shape of the figure is the same in both cases.\n\n\n\n\n\nEither way, the figures show what would have happened if that model — that \\(\\rho_h = 0.4\\) — were true.\nWe can do the same thing for all 11 possible models, calculating what would happen if each of them were true. This is somewhat counterfactual since only one of them can be true. Yet this assumption does allow us to create the joint distribution of models which might be true and of data which our experiment might generate. Let’s simplify this is p(models, data), although you should keep the precise meaning in mind.\n\n\n\n\n\nHere is the 3D version of the same plot.\n\n\n\n\n\n\nIn both of these diagrams, we see 11 models and 21 outcomes. We don’t really care about the p(\\(models\\), \\(data\\)), the joint distribution of the models-which-might-be-true and the data-which-our-experiment-might-generate. Instead, we want to estimate \\(p\\), the unknown parameter which determines the probability that this coin will come up heads when tossed. The joint distribution alone can’t tell us that. We created the joint distribution before we had even conducted the experiment. It is our creation, a tool which we use to make inferences. Instead, we want the conditional distribution, p(\\(models\\) | \\(data = 8\\)). We have the results of the experiment. What do those results tell us about the probability distribution of \\(p\\)?\nTo answer this question, we simply take a vertical slice from the joint distribution at the point of the x-axis corresponding to the results of the experiment.\nThis animation shows what we want to do with joint distributions. We take a slice (the red one), isolate it, rotate it to look at the conditional distribution, normalize it (change the values along the current z-axis from counts to probabilities), then observe the resulting posterior.\n\n\nThis is the only part of the joint distribution that we care about. We aren’t interested in what the object looks like where, for example, the number of heads is 11. That portion is irrelevant because we observed 8 heads, not 11. By using the filter function on the simulation tibble we created, we can conclude that there are a total of 465 times in our simulation in which 8 heads were observed.\nAs we would expect, most of the time when 8 coin tosses came up heads, the value of \\(p\\) was 0.4. But, on numerous occasions, it was not. It is quite common for a value of \\(p\\) like 0.3 or 0.5 to generate 8 heads. Consider:\n\n\n\n\n\nYet this is a distribution of raw counts. It is an unnormalized density. To turn it into a proper probability density (i.e., one in which the sum of the probabilities across possible outcomes sums to one) we just divide everything by the total number of observations.\n\n\n\n\n\nSolution:\nThe most likely value of \\(\\rho_h\\) is 0.4, as before. But, it is much more likely that \\(p\\) is either 0.3 or 0.5. And there is about an 8% chance that \\(\\rho_h \\ge 0.6\\).\nYou might be wondering: what is the use of a model? Well, let’s say we toss the coin 20 times and get 8 heads again. Given this result, we can ask: What is the probability that future samples of 20 flips will result in 10 or more heads?\nThere are three main ways you could go about solving this problem with simulations.\nThe first wrong way to do this is assuming that \\(\\rho_h\\) is certain because we observed 8 heads after 20 tosses. We would conclude that 8/20 gives us 0.4. The big problem with this is that you are ignoring your uncertainty when estimating \\(\\rho_h\\). This would lead us to the following code.\n\nsims <- 10000000\n\nodds <- tibble(sim_ID = 1:sims) |>\n  mutate(heads = map_int(sim_ID, ~ rbinom(n = 1, size = 20, p = .4))) |> \n  mutate(above_ten = if_else(heads >= 10, TRUE, FALSE))\n\nodds\n\n# A tibble: 10,000,000 × 3\n   sim_ID heads above_ten\n    <int> <int> <lgl>    \n 1      1    10 TRUE     \n 2      2     5 FALSE    \n 3      3     2 FALSE    \n 4      4    10 TRUE     \n 5      5     5 FALSE    \n 6      6    10 TRUE     \n 7      7     7 FALSE    \n 8      8    11 TRUE     \n 9      9     9 FALSE    \n10     10     9 FALSE    \n# … with 9,999,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nodds |>\n  ggplot(aes(x=heads,fill=above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Wrong Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nUsing this Posterior distribution derived from the (wrong way) simulated data, the probability results in 10 or more head is\n\nodds |>\n  summarize(success = sum(above_ten)/sims)\n\n# A tibble: 1 × 1\n  success\n    <dbl>\n1   0.245\n\n\nabout 24.5%.\nThe second method involves sampling the whole posterior distribution vector we previously created. This would lead to the following correct code.\n\np_draws <- tibble(p = rep(seq(0, 1, 0.1), 1000)) |>\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |>\n  filter(heads == 8)\n  \nodds_2nd <- tibble(p = sample(p_draws$p, size = sims, replace = TRUE)) |>\n  mutate(heads = map_int(p, ~ rbinom(n = 1, size = 20, p = .))) |> \n  mutate(above_ten = if_else(heads >= 10, TRUE, FALSE)) \n\nodds_2nd\n\n# A tibble: 10,000,000 × 3\n       p heads above_ten\n   <dbl> <int> <lgl>    \n 1   0.4     7 FALSE    \n 2   0.3     8 FALSE    \n 3   0.5    13 TRUE     \n 4   0.4    10 TRUE     \n 5   0.4     6 FALSE    \n 6   0.5     8 FALSE    \n 7   0.5     9 FALSE    \n 8   0.5    12 TRUE     \n 9   0.5    10 TRUE     \n10   0.4     8 FALSE    \n# … with 9,999,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nodds_2nd |>\n  ggplot(aes(x = heads,fill = above_ten))+\n           geom_histogram(aes(y = after_stat(count/sum(count))),bins = 50)+\n  scale_fill_manual(values = c('grey50', 'red'))+\n  labs(title = \"Posterior Probability Distribution (Right Way)\",\n         subtitle = \"Number of heads in 20 tosses\",\n         x = \"Number of heads\",\n         y = \"Probability\",\n         fill = \"Above ten heads\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nUsing this Posterior distribution derived from the (right way 1st) simulated data, the probability results in 10 or more head is\n\nodds_2nd |>\n  summarize(success = sum(above_ten)/sims)\n\n# A tibble: 1 × 1\n  success\n    <dbl>\n1   0.351\n\n\nabout 32.8%\nAs you may have noticed, if you calculated the value using the first method, you would believe that getting 10 or more heads is less likely than it really is. If you were to run a casino based on these assumptions, you will lose all your money. It is very important to be careful about the assumptions you are making. We tossed a coin 20 times and got 8 heads. However, you would be wrong to assume that \\(\\rho_h\\) = 0.4 just based on this result."
  },
  {
    "objectID": "05-probability.html#working-with-probability-distributions",
    "href": "05-probability.html#working-with-probability-distributions",
    "title": "5  Probability",
    "section": "\n5.7 Working with probability distributions",
    "text": "5.7 Working with probability distributions\n\nA probability distribution is not always easy to work with. It is a complex object. And, in many contexts, we don’t really care about all that complexity. So, instead of providing the full probability distribution, we often just use a summary measure, a number or two or three which captures those aspects of the entire distribution which are relevant to the matter at hand. Let’s explore these issues using the 538 posterior probability distribution, as of August 13, 2020, for the number of electoral votes which will be won by Joe Biden. Here is a tibble with 1,000,000 draws from that distribution:\n\n\n\n\ndraws\n\n# A tibble: 1,000,000 × 2\n      ID electoral_votes\n   <int>           <int>\n 1     1             210\n 2     2             277\n 3     3             227\n 4     4             397\n 5     5             378\n 6     6             428\n 7     7             350\n 8     8             385\n 9     9             325\n10    10             463\n# … with 999,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nA distribution and a sample of draws from that distribution are different things. But, if you squint, they are sort of the same thing, at least for our purposes. For example, if you want to know the mean of the distribution, then the mean of the draws will be a fairly good estimate, especially if the number of draws is large enough.\n\nRecall from Chapter @ref(wrangling) how we can draw randomly from specified probability distributions:\n\nrnorm(10)\n\n [1]  2.23367571  2.23194835 -0.62407149 -0.01425707  2.36987572  0.42803447\n [7] -0.55631165  0.68773344 -0.63241510  0.55645222\n\n\n\nrunif(10)\n\n [1] 0.8557192 0.9369414 0.4096510 0.6725821 0.2898478 0.2128326 0.5443893\n [8] 0.3711834 0.4837470 0.8043280\n\n\nThe elements of these vectors are all “draws” from the specified probability distributions. In most applied situations, our tools will produce draws rather than summary objects. Fortunately, a vector of draws is very easy to work with. Start with summary statistics:\n\n# recall mean, media, standard deviation and mad functions.\n\nkey_stats <- draws |> \n  summarize(mn = mean(electoral_votes),\n            md = median(electoral_votes),\n            sd = sd(electoral_votes),\n            mad = mad(electoral_votes))\n\nkey_stats\n\n# A tibble: 1 × 4\n     mn    md    sd   mad\n  <dbl> <dbl> <dbl> <dbl>\n1  325.   326  86.9  101.\n\n\nCalculate a 95% interval directly:\n\nquantile(draws$electoral_votes, probs = c(0.025, 0.975))\n\n 2.5% 97.5% \n  172   483 \n\n\nApproximate the 95% interval in two ways:\n\nc(key_stats$mn - 2 * key_stats$sd, \n  key_stats$mn + 2 * key_stats$sd)\n\n[1] 151.5461 499.0198\n\nc(key_stats$md - 2 * key_stats$mad, \n  key_stats$md + 2 * key_stats$mad)\n\n[1] 124.3664 527.6336\n\n\nIn this case, using the mean and standard deviation produces a 95% interval which is closer to the true interval. In other cases, the median and scaled median absolute deviation will do better. Either approximation is generally “good enough” for most work. But, if you need to know the exact 95% interval, you must use quantile()."
  },
  {
    "objectID": "05-probability.html#cardinal-virtues",
    "href": "05-probability.html#cardinal-virtues",
    "title": "5  Probability",
    "section": "\n5.8 Cardinal Virtues",
    "text": "5.8 Cardinal Virtues\n\n\n\n\n\n\n\n\n\n\nThe four Cardinal Virtues are Wisdom, Justice, Courage, and Temperance. Because data science is, ultimately, a moral act, we use these virtues to guide our work. Every data science project begins with a question.\n\nWisdom starts by creating the Preceptor Table. What data, if we had it, would allow us to answer our question easily? If the Preceptor Table has one outcome, then the model is predictive. If it has more than one (potential) outcome, then the model is causal. We then explore the data we have. You can never look too closely at your data. Key question: Are the data we have close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population? If not, we can’t proceed further. Key in making that decision is the assumption of validity. Do the columns in the Preceptor Table match the columns in the data?\nJustice starts with the Population Table – the data we want to have, the data which we actually have and all the other data from that same population. Each row of the Population Table is defined by a unique Unit/Time combination. We explore two key issues about the Population Table. First, does the relationship among the variables demonstrate stability, meaning is the model stable across different time periods? Third, are the rows associated with the data representative of all the units which we might have had data for from that time period? Justice concludes by making an assumption about the data generating mechanism. What general mathematical formula connects the outcome variable we are interested in with the other data that we have?\nCourage allows us to explore different models. Even though Justice has provided the basic mathematical structure of the model, we still need to decide which variables to include and to estimate the values of unknown parameters. We avoid hypothesis tests. We check our models for consistency with the data we have. We select one model.\nTemperance guides us in the use of the model we have created to answer the questions we began with. We create posteriors of quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.\n\n\n5.8.1 Wisdom\n\n\n\n\nWisdom.\n\n\n\n\nWisdom helps us decide if we can even hope to answer our question with the data we have.\nFirst, start with the Preceptor Table. What rows and columns of data do you need such that, if you had them all, the calculation of the quantity of interest would be trivial? If you want to know the average height of an adult in India, then the Preceptor Table would include a row for each adult and a column for their height. With no missing data, the average is easy to determine, as are a wide variety of other estimands, other unknown values.\nOne key aspect of this Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. For example, if we want to know the causal effect of exposure to Spanish-speakers on attitude toward immigration then we need a causal model, one which estimates that attitude for each person under both treatment and control. The Preceptor Table would require two columns for the outcome. If, on the other hand, we only want to predict someone’s attitude, or compare one person’s attitude to another’s, then we would only need a Preceptor Table with one column for the outcome.\nAre we are modeling (just) for prediction or are we (also) modeling for causation? Predictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model.\nEvery model is predictive, in the sense that, if we give you new data — and it is drawn from the same population — then you can create a predictive forecast. But only a subset of those models are causal, meaning that, for a given individual, you can change the value of one input and figure out what the new output would be and then, from that, calculate the causal effect by looking at the difference between two potential outcomes.\nWith prediction, all we care about is forecasting Y given X on some as-yet-unseen data. But there is no notion of “manipulation” in such models. We don’t pretend that, for Joe, we could turn variable X from a value of 5 to a value of 6 by just turning some knob and, by doing so, cause Joe’s value of Y to change from 17 to 23. We can compare two people (or two groups of people), one with X equal to 5 and one with X equal to 6, and see how they differ in Y. The basic assumption of predictive models is that there is only one possible Y for Joe. There are not, by assumption, two possible values for Y, one if X equal 5 and another if X equals 6. The Preceptor Table has a single column under Y.\nWith causal inference, however, we can consider the case of Joe with \\(X = 5\\) and Joe with \\(X = 6\\). The same mathematical model can be used. And both models can be used for prediction, for estimating what the value of Y will be for a yet-unseen observation with a specified value for X. But, in this case, instead of only a single column in the Preceptor Table for Y, we have at least two (and possibly many) such columns, one for each of the potential outcomes under consideration.\nThe difference between prediction models and causal models is that the former have one column for the outcome variable and the latter have more than one.\nSecond, we look at the data we have and perform an exploratory data analysis, an EDA. You can never look at your data too much. The most important variable is the one we most want to understand/explain/predict. In the models we create in later chapters, this variable will go on the left-hand side of our mathematical equations. Some academic fields refer to this as the “dependent variable.” Others use terms like “regressor” or “outcome.” Whatever the terminology, we need to explore the distribution of this variable, its min/max/range, its mean and median, its standard deviation, and so on.\n@roas write:\n\nMost important is that the data you are analyzing should map to the research question you are trying to answer. This sounds obvious but is often overlooked or ignored because it can be inconvenient. Optimally, this means that the outcome measure should accurately reflect the phenomenon of interest, the model should include all relevant predictors, and the model should generalize to the cases to which it will be applied.\n\n\nFor example, with regard to the outcome variable, a model of incomes will not necessarily tell you about patterns of total assets. A model of test scores will not necessarily tell you about child intelligence or cognitive development. …\n\nWe care about other variables as well, especially those that are most correlated/connected with the outcome variable. The more time that we spend looking at these variables, the more likely we are to create a useful model.\nThird, a key concept is the population. We need the data we want — the Preceptor Table — and the data we have to be similar enough that we can consider them as all having come from the same statistical population. From Wikipedia:\n\nIn statistics, a population is a set of similar items or events which is of interest for some question or experiment. A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all opening hands in all the poker games in Las Vegas tomorrow).\n\nMechanically, assuming that the Precetor Table and the data are drawn from the same population is the same thing as “stacking” the two on top of each other. For that to make sense, the variables must mean the same thing — at least mostly — in both sources. This is the assumption of validity. For example, if the Preceptor Table concerns who people will vote for in the election next week and the data is from a survey taken last week, it is not obvious that we can consider the data as coming from the same population. After all, voting and survey responses are not exactly the same thing. We can only assume that they are — which is precisely what everyone who forecasts elections does — if we assume that both are “valid” measures of the same underlying construct.\nIf we assume that the data we have is drawn from the same population as the data in the Preceptor Table, then we can use information about the former to make inferences about the latter. We can combine the Preceptor Table and the data into a single Population Table. If we can’t do that, if we can’t assume that the two sources come from the same population, then we can’t use our data to answer our questions. We have no choice but to walk away. The heart of Wisdom is knowing when to walk away. As John Tukey noted:\n\nThe combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.\n\n\n5.8.2 Justice\n\n\n\n\nJustice.\n\n\n\n\nAfter Wisdom, we have an Population Table. It includes rows for the data we have and the data we want to have. It has missing values, most importantly for potential outcomes which were not observed. The central problem in inference is to fill in the question marks in the Population Table.\nThere are two key issues to explore in any Population Table: stability and representativeness.\n\nStability assumes that the relationship between the outcome variable and the covariates is consistent over time. Never forget that temporal nature of almost all real data science problems. Our Preceptor Table will focus on rows for today or for the near future. The data we have will always be from before today. We must almost always assume that the future will be like the past in order to use data from the past to make predictions about the future.\nRepresentativeness is a two-sided concern. We want the data we have to be representative of the population for which we need to calculate parameters. Ideally, we would love for our data to be randomly sampled from the population, but this is almost never the case. But this is concern, not just with our data, but also for our Preceptor Table. If the data we want is not representative of the entire population then we will need to be careful in the inferences which we draw.\n\nValidity is about the columns in our Population Table. Stability and representativeness are about the rows.\nThe last step of Justice is to make an assumption about the structure of the data generating mechanism (DGM): the mathematical formula, and associated error term, which relates our outcome variable to our covariates.\nJustice requires math. Consider a model of coin-tossing:\n\\[ H_i  \\sim B(\\rho_H, n = 20) \\]\nThe total number \\(H\\) of Heads in experiment \\(i\\) with 20 flips of a single coin, \\(H_i\\), is distributed as a binomial with \\(n = 20\\) and an unknown probability \\(\\rho_h\\) of the coin coming up Heads.\nNote:\n\nThis is a cheat and a simplification! We are Bayesians but we have not specified the full Bayesian machinery. We really need priors on the unknown parameter \\(\\rho_h\\) as well. But that is too complex for an introductory textbook, so we wave our hands, accept the default sensible parameters built into the R packages we use, and point readers to more advanced books, like @roas.\nDefining \\(\\rho_h\\) as the “the probability that the coin comes up Heads” is a bit of a fudge. If you calculate that by hand and then compare it to what our tools produce, they won’t be the same. Instead, the calculated value will be closer to zero. Why? \\(\\rho_h\\) is really the “long-run percentage of the time the coin comes up Heads.” It is not just the percentage from this experiment.\n\n\nIn this simple case, we are fortunate that the parameter \\(\\rho_h\\) has such a (mostly!) simple analog to a real world quantity. Most of the time, parameters are not so easy to interpret. In a more complex model, especially with one with interaction terms, we focus less on parameters and more on actual predictions.\n\n5.8.3 Courage\n\n\n\n\nCourage.\n\n\n\n\nThe three languages of data science are words, math and code, and the most important of these is code. We need to explain the structure of our model using all three languages, but we need Courage to implement the model in code.\nCourage requires us to take the general mathematical formula provide by Justice and then make it specific. Which variables should we include in the model and which do we exclude? Every data science project involves the creation of several models. For each, we specify the precise data generating mechanism. Using that formula, and some R code, we create a fitted model. All models have parameters. We can never know the true values of the parameters, but we can create, and explore, posterior distributions for those unknown true values.\nCode allows us to “fit” a model by estimating the values of the unknown parameters, like \\(\\rho_h\\). Sadly, we can never know the true values of these parameters. But, like all good data scientists, we can express our uncertain knowledge in the form of posterior probability distributions. With those distributions, we can compare the actual values of the outcome variable with the “fitted” or “predicted” results of the model. We can examine the “residuals,” the difference between the fitted and actual values.\nEvery outcome is the sum of two parts: the model and what is not in the model:\n\\[outcome = model + what\\ is\\ not\\ in\\ the\\ model\\]\nIt doesn’t matter what the outcome is. It could be the result of a coin flip, the weight of a person, the GDP of a country. Whatever outcome we are considering is always made up of two parts. The first is the model we have created. The second is all the stuff — all the blooming and buzzing complexity of the real world — which is not a part of the model.\nSome of our uncertainty is driven by our ignorance about \\(\\rho_h\\).\nA parameter is something which does not exist in the real world. (If it did, or could, then it would be data.) Instead, a parameter is a mental abstraction, a building block which we will use to to help us accomplish our true goal: To replace at least some of the questions marks in the actual Preceptor Table. Since parameters are mental abstractions, we will always be uncertain as to their value, however much data we might collect.\nBut some, often most, of the uncertainty comes from forces that are, by assumption, not in the model. For example, if the coin is fair, we expect \\(T_i\\) to equal 10. But, often, it will be different, even if we are correct and \\(\\rho_h\\) equals exactly 0.5.\nSome randomness is intrinsic in this fallen world.\n\n5.8.4 Temperance\n\n\n\n\nTemperance.\n\n\n\n\nThere are few more important concepts in statistics and data science than the “Data Generating Mechanism.” Our data — the data that we collect and see — has been generated by the complexity and confusion of the world. God’s own mechanism has brought His data to us. Our job is to build a model of that process, to create, on the computer, a mechanism which generates fake data consistent with the data which we see. With that DGM, we can answer any question which we might have. In particular, with the DGM, we provide predictions of data we have not seen and estimates of the uncertainty associated with those predictions. Justice gave us the structure of the DGM. Courage created the DGM, the fitted model. Temperance will guide us in its use.\nHaving created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.\nSadly, our models are never as good as we would like them to be. First, the world is intrinsically uncertain.\n\n\n\n\nDonald Rumsfeld.\n\n\n\n\n\nThere are known knowns. There are things we know we know. We also know there are known unknowns. That is to say, we know there are some things we do not know. But there are also unknown unknowns, the ones we do not know we do not know. – Donald Rumsfeld\n\nWhat we really care about is data we haven’t seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn’t change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that are forecasts are more uncertain that a naive use of our model might suggest.\n\n\n\n\nThree Card Monte.\n\n\n\n\nWhat does this mean? Well imagine a crowd playing Three Card Monte in the streets of New York. The guy running the game runs a demo and shows you all the cards to make you confident. They earn money by making you overconfident and persuading you to bet. Your odds may seem good during the demo round, but that doesn’t actually say anything about what will likely happen when the real, high stakes game begins. The person running the game does many simulations, making the “victim” forget that they cannot actually make any conclusions about the odds of winning. There are some variables that we simply do not know even if we put a lot of effort into making posterior probability distributions. People can be using slight of hand, for instance.\nWe need patience in order to study and understand the unknown unknowns in our data. Patience is also important when we analyze the “realism” of our models. When we created the mathematical probability distribution for presidential elections, for instance, we assumed that the Democratic candidate would have a 50% chance of winning each vote in the electoral college. By comparing the mathematical model to our empirical cases, however, we recognize that the mathematical model is unlikely to be true. The mathematical model suggested that getting fewer than 100 votes is next to impossible, but many past Democratic candidates in the empirical distribution received less than 100 electoral votes.\nIn Temperance, the key distinction is between the true posterior distribution — what we will call “Preceptor’s Posterior” — and the estimated posterior distribution. Recall our discussion from Section @ref(distributions). Imagine that every assumption we made in Wisdom and Justice were correct, that we correctly understand every aspect of how the world works. We still would not know the unknown value we are trying to estimate — recall the Fundamental Problem of Causal Inference — but the posterior we created would be perfect. That is Preceptor’s Posterior. Sadly, even if our estimated posterior is, very close to Preceptor’s Posterior, we can never be sure of that fact, because we can never know the truth, never be certain that all the assumptions we made are correct.\nEven worse, we must always worry that our estimated posterior, despite all the work we put into creating it, is far from the truth. We, therefore, must be cautious in our use of that posterior, humble in our claims about its accuracy. Using our posterior, despite its fails, is better than not using it. Yet it is, as best, a distorted map of reality, a glass through which we must look darkly. Use your posterior with humility."
  },
  {
    "objectID": "05-probability.html#summary",
    "href": "05-probability.html#summary",
    "title": "5  Probability",
    "section": "\n5.9 Summary",
    "text": "5.9 Summary\n\nThroughout this Chapter, we spent time going through examples of conditional distributions. However, it’s worth noting that all probability distributions are conditional on something. Even in the most simple examples, when we were flipping a coin multiple times, we were assuming that the probability of getting heads versus tails did not change between tosses.\nWe also discussed the difference between empirical, mathematical, and posterior probability distributions. Even though we developed these heuristics to better understand distributions, every time we make a claim about the world, it is based on our beliefs - what we think about the world. We could be wrong. Our beliefs can differ. Two reasonable people can have conflicting beliefs about the fairness of a die.\nAt the start of this chapter we briefly discuss the definition of an random variable, yet we sort of let it go for the rest of the chapter, but it’s hiding almost everywhere whenever we create an distribution. For example, in two models we have two random variables, the have disease and the don’t have disease, in three models we have three random variables, either to have 0 or 1 or 2 white marbles in bad. Essentially is just like the missing values in the Preceptor table what random variables do you need to know the values of to answer the question? \nIt is useful to understand the three types of distributions and the concept of conditional distributions, but almost every probability distribution is conditional and posterior. We can leave out both words in future discussions, as we generally will in this book. They are implicit.\nIf you are keen to learn more about probability, here is a video featuring Professor Gary King. This is a great way to review some of the concepts we covered in this chapter, albeit at a higher level of mathematics."
  },
  {
    "objectID": "06-one-parameter.html",
    "href": "06-one-parameter.html",
    "title": "6  One Parameter",
    "section": "",
    "text": "This chapter is being re-drafted.\nThe Hunger Games is a dystopian novel in which children are chosen via lottery to fight to the death. Primrose Everdeen is selected from the urn. Why does she have the misfortune of being selected? Or, as we data scientists say, sampled?\nIn Chapter @ref(probability), we learned about probability, the framework for quantifying uncertainty. In this chapter, we will learn about sampling, the beginning of our journey toward inference. When we sample, we take some units from a population, calculate statistics based on those units, and make inferences about unknown parameters associated with the population.\nThe urn below has a certain number of red and a certain number of white beads all of equal size, mixed well together. What proportion, \\(\\rho\\), of this urn’s beads are red?\nOne way to answer this question would be to perform an exhaustive count: remove each bead individually, count the number of red beads, count the number of white beads, and divide the number of red beads by the total number of beads. Call that ratio \\(\\rho\\), the proportion of red beads in the urn. However, this would be a long and tedious process. Therefore, we will use sampling! Consider two questions:\nIf we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion \\(\\rho\\) of the beads in the urn are red?\nWhat is the probability, using that same urn, that we will draw more than 8 red beads if we use a shovel of size 20?\nTo begin this chapter, we will look at a real sampling activity: the urn. Then, we will simulate the urn example using R code. This will help us to understand the standard error and the ways in which uncertainty factors into our predictions. We then attempt to estimate a single parameter, the proportion of red beads in the urn.\nUse the tidyverse package."
  },
  {
    "objectID": "06-one-parameter.html#sampling-activity",
    "href": "06-one-parameter.html#sampling-activity",
    "title": "6  One Parameter",
    "section": "\n6.1 Real sampling activity",
    "text": "6.1 Real sampling activity\n\n\n\n\nAn urn with red and white beads.\n\n\n\n\n\n6.1.1 Using the shovel method once\nInstead of performing an exhaustive count, let’s insert a shovel into the urn and remove \\(5 \\cdot 10 = 50\\) beads. We are taking a sample of the total population of beads.\n\n\n\n\nInserting a shovel into the urn.\n\n\n\n\n\n\n\n\nRemoving 50 beads from the urn.\n\n\n\n\nObserve that 17 of the 50 sampled beads are red and thus \\(17/50 = 0.34 = 34\\%\\) of the shovel’s beads are red. We can view the proportion of beads that are red in this shovel as a guess of the proportion of beads that are red in the entire urn. While not as exact as doing an exhaustive count of all the beads in the urn, our guess of 34% took much less time and energy to make.\nRecall that \\(\\rho\\) is the true value of the proportion of red beads. There is only one \\(\\rho\\). Our guesses at the proportion of red beads are known as \\(\\hat{\\rho}\\) (pronounced p hat), where \\(\\hat{\\rho}\\) is the estimated value of \\(\\rho\\) which comes from taking a sample. There are many possible \\(\\hat{\\rho}\\)’s. You and I will often differ in our estimates. We each have a different \\(\\hat{\\rho}\\) even though we agree that there is only one \\(\\rho\\).\nStart this activity over from the beginning, placing the 50 beads back into the urn. Would a second sample include exactly 17 red beads? Maybe, but probably not.\nWhat if we repeated this activity many times? Would our guess at the proportion of the urn’s beads that are red, \\(\\hat{\\rho}\\), be exactly 34% every time? Surely not.\nLet’s repeat this exercise with the help of 33 groups of friends to understand how the value of \\(\\hat{\\rho}\\) varies across 33 independent trials.\n\n6.1.2 Using the shovel 33 times\nEach of our 33 groups of friends will do the following:\n\nUse the shovel to remove 50 beads each.\nCount the number of red beads and compute the proportion of the 50 beads that are red.\nReturn the beads into the urn.\nMix the contents of the urn to not let a previous group’s results influence the next group’s.\n\nEach of our 33 groups of friends make note of their proportion of red beads from their sample collected. Each group then marks their proportion of their 50 beads that were red in the appropriate bin in a hand-drawn histogram as seen below.\n\n\n\n\nConstructing a histogram of proportions.\n\n\n\n\nHistograms allow us to visualize the distribution of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends’ results can be seen in the figure below.\n\n\n\n\nHand-drawn histogram of first 10 out of 33 proportions.\n\n\n\n\nObserve the following details in the histogram:\n\nAt the low end, one group removed 50 beads from the urn with proportion red between 0.20 and 0.25.\nAt the high end, another group removed 50 beads from the urn with proportion between 0.45 and 0.5 red.\nHowever, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution.\nThe distribution is somewhat bell-shaped.\n\ntactile_sample_urn saves the results from our 33 groups of friends.\n\n\n# A tibble: 33 × 4\n   group           red_beads prop_red group_ID\n   <chr>               <dbl>    <dbl>    <int>\n 1 Damani, Melissa        15     0.3         1\n 2 Siobhan, Jane          11     0.22        2\n 3 Maeve, Josh            18     0.36        3\n 4 Esther, Henry          17     0.34        4\n 5 Sanjana, Yuko          19     0.38        5\n 6 Dania, Derek           21     0.42        6\n 7 Mark, Jane             21     0.42        7\n 8 Yuki, Harry            21     0.42        8\n 9 Ellie, Terrance        17     0.34        9\n10 Mark, Ramses           21     0.42       10\n# … with 23 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nFor each group, we are given their names, the number of red_beads they obtained, and the corresponding proportion out of 50 beads that were red, called prop_red. We also have an group_ID variable which gives each of the 33 groups a unique identifier. Each row can be viewed as one instance of a replicated activity: using the shovel to remove 50 beads and computing the proportion of those beads that are red.\nLet’s visualize the distribution of these 33 proportions using geom_histogram() with binwidth = 0.05. This is a computerized and complete version of the partially completed hand-drawn histogram you saw earlier. Setting boundary = 0.4 indicates that we want a binning scheme such that one of the bins’ boundary is at 0.4. color = \"white\" modifies the color of the boundary for visual clarity.\n\ntactile_sample_urn |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # Add scale_y_continuous with breaks by 1, as the default shows the y-axis\n  # from 1 to 10 with breaks at .5. Breaks by 1 is better for this plot, as all\n  # resulting values are integers.\n  \n  scale_y_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n  \n  # The call expression() is used to insert a mathematical expression, like\n  # p-hat. The paste after expression allows us to paste text prior to said\n  # expression.\n  \n  labs(x = expression(paste(\"Proportion of 50 beads that were red \", hat(rho))),\n       y = \"Count\",\n       title = \"Proportions Red in 33 Samples\") \n\n\n\n\n\n6.1.3 What did we just do?\nWhat we just demonstrated in this activity is the statistical concept of sampling. We want to know the proportion of red beads in the urn, with the urn being our population. Performing an exhaustive count of the red and white beads would be too time-consuming. Therefore, it is much more practical to extract a sample of 50 beads using the shovel. Using this sample of 50 beads, we estimated the proportion of the urn’s beads that are red to be about 34%.\nMoreover, because we mixed the beads before each use of the shovel, the samples were random and independent. Because each sample was drawn at random, the samples were different from each other. This is an example of sampling variation. For example, what if instead of selecting 17 beads in our first sample we had selected just 11? Does that mean that the population proportion of the beads is 11/50 or 22%? No! Because we performed 33 trials we can look to our histogram, and see that the peak of the distribution occurs when \\(.35 < \\hat{\\rho} < .4\\) , so it is very likely that the proportion of red beads in the entire population will also fall in or near this range."
  },
  {
    "objectID": "06-one-parameter.html#virtual-sampling",
    "href": "06-one-parameter.html#virtual-sampling",
    "title": "6  One Parameter",
    "section": "\n6.2 Virtual sampling",
    "text": "6.2 Virtual sampling\nWe just performed a tactile sampling activity. We used a physical urn of beads and a physical shovel. We did this by hand so that we could develop our intuition about the ideas behind sampling. In this section, we mimic this physical sampling with virtual sampling, using a computer.\n\n6.2.1 Using the virtual shovel once\nVirtual sampling requires a virtual urn and a virtual shovel. Create a tibble named urn. The rows of urn correspond exactly to the contents of the actual urn.\n\n# set.seed() ensures that the beads in our virtual urn are always in the same\n# order. This ensures that the figures in the book match their written\n# descriptions. We want 40% of the beads to be red.\n\nset.seed(10)\n\nurn <- tibble(color = c(rep(\"red\", 400), \n                        rep(\"white\", 600))) |>\n  \n  # sample_frac() keeps all the rows in the tibble but rearranges their order.\n  # We don't need to do this. A virtual urn does not care about the order of the\n  # beads. But we find it aesthetically pleasing to mix them up.\n  \n  sample_frac() |> \n  mutate(bead_ID = 1:1000) \n\nurn  \n\n# A tibble: 1,000 × 2\n   color bead_ID\n   <chr>   <int>\n 1 white       1\n 2 white       2\n 3 red         3\n 4 red         4\n 5 white       5\n 6 white       6\n 7 white       7\n 8 white       8\n 9 white       9\n10 white      10\n# … with 990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nObserve that urn has 1,000 rows, meaning that the urn contains 1,000 beads. The first variable bead_ID is used as an identification variable. None of the beads in the actual urn are marked with numbers. The second variable color indicates whether a particular virtual bead is red or white.\nNote that in this section, we used the variable bead_ID to keep track of each bead in our urn, while in the last section we used group_ID to keep track of the samples drawn by the 33 individual teams. This is a better strategy than naming both variables ID, as it would be much more likely for us to get them confused later on.\nOur virtual urn needs a virtual shovel. We use slice_sample() and list-column mapping wizardry learned in Section @ref(list-columns-and-map-functions) to take a sample of 50 beads from our virtual urn.\n\n# Define trial_ID as one instance of us sampling 50 beads from the urn. When\n# trial_ID is called within map(), we are performing slice_sample() upon our urn\n# once, and taking a sample of 50 beads. \n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50)))\n\n# A tibble: 1 × 2\n  trial_ID shovel           \n     <dbl> <list>           \n1        1 <tibble [50 × 2]>\n\n\nAs usual, map functions and list-columns are powerful but confusing. The str() function is a good way to explore a tibble with a list-column.\n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  str()\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ trial_ID: num 1\n $ shovel  :List of 1\n  ..$ : tibble [50 × 2] (S3: tbl_df/tbl/data.frame)\n  .. ..$ color  : chr [1:50] \"white\" \"white\" \"white\" \"red\" ...\n  .. ..$ bead_ID: int [1:50] 812 903 227 283 229 160 523 893 66 277 ...\n\n\nThere are two levels. There is one row in the tibble for each sample. So far, we have only drawn one sample. Within each row, there is a second level, the tibble which is the sample. That tibble has two variables: trial_ID and color. This is the advantage to using slice_sample(), because it selects all columns of our urn, whereas sample() can only sample from a single column. While identifying each individual bead may be irrelevant in our urn scenario, with other problems it could be very useful to have additional data about each individual.\nNow let’s add a column which indicates the number of red beads in the sample taken from the shovel.\n\n\n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  \n# To count the number of red beads in each shovel, we can use a lesser \n# known property of the sum() function: By passing in a comparison \n# expression, sum() will count the number of occurrences within a vector. \n# In this case, we count the total number occurrences of the word red\n# in the color column of shovel.\n\n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\")))\n\n# A tibble: 1 × 3\n  trial_ID shovel            numb_red\n     <dbl> <list>               <int>\n1        1 <tibble [50 × 2]>       20\n\n\nHow does this work? R evaluates if color == red, and treats TRUE values like the number 1 and FALSE values like the number 0. So summing the number of TRUEs and FALSEs is equivalent to summing 1’s and 0’s. In the end, this operation counts the number of beads where color equals “red”.\nFinally, calculate the proportion red by dividing numb_red (The number of red beads observed in the shovel), by the shovel size (We are using a shovel of size 50).\n\ntibble(trial_ID = 1) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 50)\n\n# A tibble: 1 × 4\n  trial_ID shovel            numb_red prop_red\n     <dbl> <list>               <int>    <dbl>\n1        1 <tibble [50 × 2]>       23     0.46\n\n\nCareful readers will note that the numb_red is changing in each example above. The reason, of course, is that each block re-runs the shovel exercise, and slice_sample will return a random number of red beads. If we wanted the same number in each block, we would need to use set.seed() each time, always providing the same seed each time.\nLet’s now perform the virtual analog of having 33 groups of students use the sampling shovel!\n\n6.2.2 Using the virtual shovel 33 times\nIn our tactile sampling exercise in Section @ref(sampling-activity), we had 33 groups of students use the shovel, yielding 33 samples of size 50 beads. We then used these 33 samples to compute 33 proportions.\nLet’s use our virtual sampling to replicate the tactile sampling activity in a virtual format. We’ll save these results in a data frame called virtual_samples.\n\nset.seed(9)\n\n virtual_samples <- tibble(trial_ID = 1:33) |>\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |>\n    mutate(prop_red = numb_red / 50) \n\nvirtual_samples\n\n# A tibble: 33 × 4\n   trial_ID shovel            numb_red prop_red\n      <int> <list>               <int>    <dbl>\n 1        1 <tibble [50 × 2]>       21     0.42\n 2        2 <tibble [50 × 2]>       19     0.38\n 3        3 <tibble [50 × 2]>       17     0.34\n 4        4 <tibble [50 × 2]>       15     0.3 \n 5        5 <tibble [50 × 2]>       17     0.34\n 6        6 <tibble [50 × 2]>       21     0.42\n 7        7 <tibble [50 × 2]>        9     0.18\n 8        8 <tibble [50 × 2]>       21     0.42\n 9        9 <tibble [50 × 2]>       16     0.32\n10       10 <tibble [50 × 2]>       20     0.4 \n# … with 23 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nLet’s visualize this variation in a histogram:\n\nvirtual_samples |> \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.05, \n                 boundary = 0.4, \n                 color = \"white\") +\n  \n  # To use mathematical symbols in titles and labels, use the expression()\n  # function, as here.\n  \n  labs(x = expression(hat(rho)),\n       y = \"Count\",\n       title = \"Distribution of 33 proportions red\") +\n  \n  # Label the y-axis in an attractive fashion. Without this code, the axis\n  # labels would include 2.5, which makes no sense because all the values are\n  # integers.\n  \n  scale_y_continuous(breaks = seq(2, 10, 2))\n\n\n\n\nSince binwidth = 0.05, this will create bins with boundaries at 0.30, 0.35, 0.45, and so on. Recall that \\(\\hat{\\rho}\\) is equal to the proportion of beads which are red in each sample.\nObserve that we occasionally obtained proportions red that are less than 30%. On the other hand, we occasionally obtained proportions that are greater than 45%. However, the most frequently occurring proportions were between 35% and 45%. Why do we have these differences in proportions red? Because of sampling variation.\nNow we will compare our virtual results with our tactile results from the previous section. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.\n\n\n\n\nComparing 33 virtual and 33 tactile proportions red. Note that, though the figures differ slightly, both are centered around .35 to .45. This shows that, in both sampling distributions, the most frequently occuring proportion red is between 35% and 45%.\n\n\n\n\nThis visualization allows us to see how our results differed between our tactile and virtual urn results. As we can see, there is some variation between our results. This is not a cause for concern, as there is always expected sampling variation between results.\n\n6.2.3 Using the virtual shovel 1,000 times\n\n\n\n\nSo much sampling, so little time.\n\n\n\n\nAlthough we took 33 samples from the urn in the previous section, we should never do that again! The advantage of modern technology is that we can use virtual simulation as many times as we choose, so we have no restrictions on resources. No longer are the days where we have to recruit our friends to tirelessly sample from the physical urn. We are now data scientists! 33 samples are useless to us. Instead, we use our simulations hundreds or thousands of times to create mathematical models that we can combine with our knowledge to answer our questions. In this section we’ll examine the effects of sampling from the urn 1,000 times.\nWe can reuse our code from above, making sure to replace 33 trials with 1,000.\n\nset.seed(9)\n\n virtual_samples <- tibble(trial_ID = 1:1000) |>\n    mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n    mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |>\n    mutate(numb_beads = map_int(shovel, ~ length(.$color))) |> \n    mutate(prop_red = numb_red / numb_beads) \n\nNow we have 1,000 values for prop_red, each representing the proportion of 50 beads that are red in a sample. Using the same code as earlier, let’s visualize the distribution of these 1,000 replicates of prop_red in a histogram:\n\nvirtual_samples |> \nggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.01, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = expression(hat(rho)),\n       y = \"Count\",\n       title = \"Distribution of 1,000 proportions red\") \n\n\n\n\nWhy the empty spaces among the bars? Recall that, with only 50 beads, there are only 51 possible values for \\(\\hat{\\rho}\\): 0, 0.02, 0.04, …, 0.98, 1. A value of 0.31 or 0.47 is impossible, hence the gaps.\nThe most frequently occurring proportions of red beads occur, again, between 35% and 45%. Every now and then we observe proportions much higher or lower. This occurs because as we increase the number of trials, tails develop on our distribution as we are more likely to witness extreme \\(\\hat{\\rho}\\) values. The symmetric, bell-shaped distribution shown in the histogram is well approximated by the normal distribution.\nNow that we have a good understanding of virtual sampling, we can apply our knowledge to examine the effects of changing our virtual shovel size.\n\n6.2.4 The effect of different shovel sizes\n\n\n\n\nWhat happens if we use different sized shovels to sample?\n\n\n\n\nInstead of just one shovel, imagine we have three choices of shovels to extract a sample of beads with: shovels of size 25, 50, and 100. Using our newly developed tools for virtual sampling, let’s unpack the effect of having different sample sizes. Start by creating a tibble with 1,000 rows, each row representing an instance of us sampling from the urn with our chosen shovel size. Then, compute the resulting 1,000 replicates of proportion red. Finally, plot the distribution using a histogram.\n\n# Within slice_sample(), n = 25 represents our shovel of size 25. We also divide\n# by 25 to compute the proportion red.\n\nvirtual_samples_25 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 25))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 25)\n\nvirtual_samples_25 |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 25 beads that were red\", \n       title = \"25\") \n\n\n\n\nWe will repeat this process with a shovel size of 50.\n\nvirtual_samples_50 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 50))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 50)\n\n\nvirtual_samples_50  |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 50 beads that were red\", \n       title = \"50\")  \n\n\n\n\nWe choose a bin width of .04 for all histograms to more easily compare different shovel sizes. Using a smaller bin size would result in gaps between the bars, as a shovel of size 50 has more possible \\(\\hat{\\rho}\\) values than a shovel of size 25.\nFinally, we will perform the same process with 1000 replicates to map the histogram using a shovel size of 100.\n\nvirtual_samples_100 <- tibble(trial_ID = 1:1000) |> \n  mutate(shovel = map(trial_ID, ~ slice_sample(urn, n = 100))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / 100)\n\n\nvirtual_samples_100 |>\n  ggplot(aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, \n                 boundary = 0.4, \n                 color = \"white\") +\n  labs(x = \"Proportion of 100 beads that were red\", \n       title = \"100\") \n\n\n\n\nFor easy comparison, we present the three resulting histograms in a single row with matching x and y axes:\n\n# Use bind_rows to combine the data from our three saved virtual sampling\n# objects. Use mutate() in each to clarify the n as the necessary number\n# of samples taken. This makes our data easier to interpret and prevents\n# duplicate elements.\n\nvirtual_prop <- bind_rows(virtual_samples_25 |> \n                            mutate(n = 25), \n                          virtual_samples_50 |> \n                            mutate(n = 50), \n                          virtual_samples_100 |> \n                            mutate(n = 100))\n\n# Plot our new object with the x-axis showing prop_red. Add elements binwidth,\n# boundary, and color for stylistic clarity. Use labs() to add an x-axis label\n# and title. Facet_wrap() splits the graph into multiple plots by the variable\n# (~n).\n\ncomparing_sampling_distributions <- ggplot(virtual_prop, aes(x = prop_red)) +\n  geom_histogram(binwidth = 0.04, boundary = 0.4, color = \"white\") +\n  labs(x = \"Proportion of shovel's beads that are red\", \n       title = \"Comparing distributions of proportions red for three different shovel sizes.\") +\n  facet_wrap(~ n) \n\n# Inspect our new faceted graph. \n\ncomparing_sampling_distributions\n\n\n\nComparing the distributions of proportion red for different sample sizes (25, 50, 100). The important takeaway is that our center becomes more concentrated as our sample size increases, indicating a smaller standard deviation between our guesses.\n\n\n\n\nObserve that as the sample size increases, the histogram becomes taller and narrower. This is because the variation of the proportion red for each sample decreases. Remember: A large variation means there are a wide range of values that can be achieved, while smaller variations are concentrated around a specific value.\nYou have also witnessed famous theorem, or mathematically proven truth, called the Central Limit Theorem. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow. In other words, the sampling distribution increasingly follows a normal distribution and the variation of these sampling distributions gets smaller, as quantified by their standard errors.\nWhy does variation decrease as sample size increases? If we use a large sample size like 100 or 500, our sample is much more representative of the population simply because more of the population is included. As a result, the proportion red in our sample (\\(\\hat{\\rho}\\)) will be closer to the population proportion (\\(\\rho\\)). On the other hand, smaller samples have much more variation because of our old friend chance. We are much more likely to have extreme samples that are not representative of our population.\nLet’s attempt to visualize the concept of variation a different way. For each sample size, let’s plot the proportion red for all 1,000 samples. With 3 different shovel sizes, we will have 3,000 total points, with each point representing an instance of sampling from the urn with a specific shovel size.\n\nvirtual_prop |>\n  ggplot(aes(x = n, y = prop_red, color = as.factor(n))) +\n  geom_jitter(alpha = .15) + \n  labs(title = \"Results of 1,000 samples for 3 different shovel sizes.\",\n       subtitle = \"As shovel size increases, variation decreases.\",\n       y = \"Proportion red in sample\",\n       color = \"Shovel size\") +\n  # We do not need an x axis, because the color of the points denotes the shovel size. \n   theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\nThis graph illustrates the exact same concept as the histogram. With the smallest shovel size there is significant variance from sample to sample, as samples take on a wide variety of sample proportions! However, as we increase the sample size, the points become more concentrated, or less variance.\nThere is also a third way to understand variation! We can be numerically explicit about the amount of variation in our three sets of 1,000 values of prop_red by using the standard deviation. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable. For all three sample sizes, let’s compute the standard deviation of the 1,000 proportions red.\n\n\n\n\n\n\n\nComparing standard deviations of proportions red for three different shovels\n    \n\nNumber of slots in shovel\n      Standard deviation of proportions red\n    \n\n\n25\n0.099\n\n\n50\n0.067\n\n\n100\n0.045\n\n\n\n\nComparing the number of slots in the shovel with the standard deviation of proportions red. Here, we see that standard deviation decreases with higher sample sizes. Larger sample sizes yield more precise estimates.\n\n\n \nAs the sample size increases, the sample to sample variation decreases, and our guesses at the true proportion of the urn’s beads that are red get more precise. The larger the shovel, the more precise the result.\n\n\n\n\nVariance appears everywhere in data science.\n\n\n\n\nLet’s take a step back from all the variance. The reality is that our code needs to be better optimized, as it is bad practice to make a separate tibble for each sample size. To make comparisons easier, let’s attempt to put all 3 shovel sizes in the same tibble using mapping.\n\ntibble(trial_ID = 1:1000) |>\n  mutate(shovel_ID = map(trial_ID, ~c(25, 50, 100))) |>\n  unnest(shovel_ID) |>\n  mutate(samples = map(shovel_ID, ~slice_sample(urn, n = .))) |>\n  mutate(num_red = map_int(samples, ~sum(.$color == \"red\"))) |>\n  mutate(prop_red = num_red/shovel_ID)\n\n# A tibble: 3,000 × 5\n   trial_ID shovel_ID samples            num_red prop_red\n      <int>     <dbl> <list>               <int>    <dbl>\n 1        1        25 <tibble [25 × 2]>        7     0.28\n 2        1        50 <tibble [50 × 2]>       26     0.52\n 3        1       100 <tibble [100 × 2]>      37     0.37\n 4        2        25 <tibble [25 × 2]>        8     0.32\n 5        2        50 <tibble [50 × 2]>       15     0.3 \n 6        2       100 <tibble [100 × 2]>      46     0.46\n 7        3        25 <tibble [25 × 2]>        6     0.24\n 8        3        50 <tibble [50 × 2]>       21     0.42\n 9        3       100 <tibble [100 × 2]>      40     0.4 \n10        4        25 <tibble [25 × 2]>        9     0.36\n# … with 2,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nTo those of us who do not completely understand mapping, do not fret! The tidyr package provides the expand_grid() function as a neat alternative. We can use expand_grid() and a new variable, shovel_size, to create a tibble which will organize our results. Instead of using 1,000 trials, let’s use 3 to get a feel for the function.\n\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100))\n\n# A tibble: 9 × 2\n  trial_ID shovel_size\n     <int>       <dbl>\n1        1          25\n2        1          50\n3        1         100\n4        2          25\n5        2          50\n6        2         100\n7        3          25\n8        3          50\n9        3         100\n\n\nThe above sets the stage for simulating three samples for each of three different shovel sizes. Similar code as above can be used.\n\nexpand_grid(trial_ID = c(1:3), shovel_size = c(25, 50, 100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red/shovel_size) \n\n# A tibble: 9 × 5\n  trial_ID shovel_size shovel             numb_red prop_red\n     <int>       <dbl> <list>                <int>    <dbl>\n1        1          25 <tibble [25 × 2]>         7     0.28\n2        1          50 <tibble [50 × 2]>        19     0.38\n3        1         100 <tibble [100 × 2]>       39     0.39\n4        2          25 <tibble [25 × 2]>         8     0.32\n5        2          50 <tibble [50 × 2]>        19     0.38\n6        2         100 <tibble [100 × 2]>       34     0.34\n7        3          25 <tibble [25 × 2]>         7     0.28\n8        3          50 <tibble [50 × 2]>        19     0.38\n9        3         100 <tibble [100 × 2]>       36     0.36\n\n\nAgain, we changed the second line to use shovel_size rather than trial_ID as the mapping variable since we can no longer hard code the shovel size into the call to slice_sample(). Expand to 1,000 simulations for each value of shovel_size and finish with a calculation of standard deviation.\n\nexpand_grid(trial_ID = c(1:1000), shovel_size = c(25, 50, 100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red/shovel_size) |>\n  group_by(shovel_size) |> \n  summarize(st_dev_p_hat = sd(prop_red))\n\n# A tibble: 3 × 2\n  shovel_size st_dev_p_hat\n        <dbl>        <dbl>\n1          25       0.0967\n2          50       0.0682\n3         100       0.0453\n\n\nThis is, approximately, the same result as we saw above, but with 1 re-factored tibble instead of 3 separate ones. We can also functions like expand_grid() in the future to make our code more concise.\nNow that we have this framework, there’s no need to limit ourselves to the sizes 25, 50, and 100. Why not try all integers from 1 to 100? We can use the same code, except we’ll now set shovel_size = 1:100.\n\n\n\n\n\n\nshovels_100 <- expand_grid(trial_ID = c(1:1000), shovel_size = c(1:100)) |> \n  mutate(shovel = map(shovel_size, ~ slice_sample(urn, n = .))) |> \n  mutate(numb_red = map_int(shovel, ~ sum(.$color == \"red\"))) |> \n  mutate(prop_red = numb_red / shovel_size) |> \n  group_by(shovel_size) |> \n  summarize(st_dev_p_hat = sd(prop_red))\n\nglimpse(shovels_100)\n\nRows: 100\nColumns: 2\n$ shovel_size  <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ st_dev_p_hat <dbl> 0.48436516, 0.34093399, 0.27629253, 0.24069833, 0.2140195…\n\n\nNow, we have the standard deviation of prop_red for all shovel sizes from 1 to 100. Let’s plot that value to see how it changes as the shovel gets larger:\n\n\n\n\nComparing standard deviations of proportions red for 100 different shovels. The standard deviation decreases at the same rate as the square root of shovel size. The red line shows the standard error.\n\n\n\n\nThe red line here represents an important statistical concept: standard error (SE). As the shovel size increases, and thus our sample size increases, we find that the standard error decreases. If this is confusing right now, fear not! We will delve into the explanation of standard error in our next section.\n\n\n\n\nTo any poets and philosophers confused about this: don’t worry! It won’t be on a problem set.\n\n\n\n\nThis is the power of running many analyses at once using map functions and list columns: before, we could tell that the standard deviation was decreasing as the shovel size increased, but when only looking at shovel sizes of 25, 50, and 100, it wasn’t clear how quickly it was decreasing."
  },
  {
    "objectID": "06-one-parameter.html#standard-errors",
    "href": "06-one-parameter.html#standard-errors",
    "title": "6  One Parameter",
    "section": "\n6.3 Standard error",
    "text": "6.3 Standard error\n\n\n\n\nStandard errors are just the way old people talk about confidence intervals.\n\n\n\n\nStandard errors (SE) quantify the effect of sampling variation on our estimates. In other words, they quantify how much we can expect the calculated proportions of a shovel’s beads that are red to vary from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.\nThe standard error is the standard deviation of a sample statistic (aka point estimate), such as the proportion. For example, the standard error of the mean refers to the standard deviation of the distribution of sample means taken from a population.\nThe relationship between the standard error and the standard deviation is that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size. Accordingly, the standard error is inversely proportional to the sample size. The larger the sample size, the smaller the standard error.\nIf this sounds confusing, don’t worry! It is. Before we can explain this in more depth, it is important to understand some terminology.\n\n6.3.1 Terminology and notation\n\n\n\n\nLet Yoda’s wisdom dull the pain of this terminology section.\n\n\n\n\nAll of the concepts underlying this terminology, notation, and definitions tie directly to the concepts underlying our tactile and virtual sampling activities. It will simply take time and practice to master them.\nFirst, a population is the set of relevant units. The population’s size is upper-case \\(N\\). In our sampling activities, the population is the collection of \\(N\\) = 1,000 identically sized red and white beads in the urn. This is about the simplest possible population. Other examples are all the adult men in the US, all the classrooms in a school, all the wheelbarrows in Massachusetts, all the values of your blood pressure, read at five minute intervals, for your entire life. Often, the population is extends over time, as with your blood pressure readings and is, therefore, more amorphous. Consider all the people who have run for governor of a US state since 1900, or all the people who will run for governor through 2050. Those are also populations.\nSecond, a population parameter is a numerical summary quantity about the population that is unknown, but you wish you knew. For example, when this quantity is the mean, the population parameter of interest is the population mean. This is mathematically denoted with the Greek letter \\(\\mu\\) pronounced “mu.” In our earlier sampling from the urn activity, however, since we were interested in the proportion of the urn’s beads that were red, the population parameter is the population proportion, denoted with \\(\\rho\\).\nThird, a census is an exhaustive enumeration or counting of all \\(N\\) units in the population in order to compute the population parameter’s value exactly. In our sampling activity, this would correspond to counting the number of beads out of \\(N = 1000\\) that are red and computing the population proportion \\(\\rho\\) that are red exactly. When the number \\(N\\) of individuals or observations in our population is large as was the case with our urn, a census can be quite expensive in terms of time, energy, and money. A census is impossible for any populations which includes the future, like our blood pressure next year or candidates for governor in 2040. There is a truth but we could not, even in theory, calculate it.\nFourth, sampling is the act of collecting a sample from the population when we can not, or do not want to, perform a census. The sample size is lower case \\(n\\), as opposed to upper case \\(N\\) for the population’s size. Typically the sample size \\(n\\) is much smaller than the population size \\(N\\). In our sampling activities, we used shovels with varying slots to extract samples of size \\(n\\) = 1 through \\(n\\) = 100.\nFifth, a point estimate, also known as a sample statistic, is a measure computed from a sample that estimates an unknown population parameter. In our sampling activities, recall that the unknown population parameter was the proportion of red beads and that this is mathematically denoted with \\(\\rho\\). Our point estimate is the sample proportion: the proportion of the shovel’s beads that are red. In other words, it is our guess at the proportion of the urn’s beads that are red. The point estimate of the parameter \\(\\rho\\) is \\(\\hat{\\rho}\\). The “hat” on top of the \\(\\rho\\) indicates that it is an estimate of the unknown population proportion \\(\\rho\\).\n\nSixth, a sample is said to be representative if it roughly looks like the population. In other words, are the sample’s characteristics a good representation of the population’s characteristics? In our sampling activity, are the samples of \\(n\\) beads extracted using our shovels representative of the urn’s \\(N\\) = 1000 beads?\nSeventh, a sample is generalizable if any results based on the sample can generalize to the population. In our sampling activity, can we generalize the sample proportion from our shovels to the entire urn? Using our mathematical notation, this is akin to asking if \\(\\hat{\\rho}\\) is a “good guess” of \\(\\rho\\)?\nEighth, biased sampling occurs if certain individuals or observations in a population have a higher chance of being included in a sample than others. We say a sampling procedure is unbiased if every observation in a population had an equal chance of being sampled. Had the red beads been much smaller than the white beads, and therefore more prone to falling out of the shovel, our sample would have been biased. In our sampling activities, since we mixed all \\(N = 1000\\) beads prior to each group’s sampling and since each of the equally sized beads had an equal chance of being sampled, our samples were unbiased.\nNinth, a sampling procedure is random if we sample randomly from the population in an unbiased fashion. In our sampling activities, this would correspond to sufficiently mixing the urn before each use of the shovel.\n\n\n\n\nFear not if you look like Spongebob after reading this section. We will re-cap right now!\n\n\n\n\nIn general:\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can draw inferences about the population using sampling.\n\nSpecific to our sampling activity:\n\nIf we extract a sample of \\(n=50\\) beads at random, in other words, we mix all of the equally sized beads before using the shovel, then\nthe contents of the shovel are an unbiased representation of the contents of the urn’s 1000 beads, thus\nany result based on the shovel’s beads can generalize to the urn, thus\nthe sample proportion \\(\\hat{\\rho}\\) of the \\(n=50\\) beads in the shovel that are red is a “good guess” of the population proportion \\(\\rho\\) of the \\(N=1000\\) beads that are red, thus\ninstead of manually going over all 1,000 beads in the urn, we can make inferences about the urn by using the results from the shovel.\n\n6.3.2 Statistical definitions\nNow, for some important statistical definitions related to sampling. As a refresher of our 1,000 repeated/replicated virtual samples of size \\(n\\) = 25, \\(n\\) = 50, and \\(n\\) = 100 in Section @ref(virtual-sampling), let’s display our figure showing the difference in proportions red according to different shovel sizes.\n\n\n\n\nPreviously seen three distributions of the sample proportion \\(\\hat{ ho}\\).\n\n\n\n\nThese types of distributions have a special name: sampling distributions; their visualization displays the effect of sampling variation on the distribution of a point estimate; in this case, the sample proportion \\(\\hat{\\rho}\\). Using these sampling distributions, for a given sample size \\(n\\), we can make statements about what values we typically expect.\nFor example, observe the centers of all three sampling distributions: they are all roughly centered around \\(0.4 = 40\\%\\). Furthermore, observe that while we are somewhat likely to observe sample proportions of red beads of \\(0.2 = 20\\%\\) when using the shovel with 25 slots, we will almost never observe a proportion of 20% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \\(n\\) increases from 25 to 50 to 100, the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40%. We quantified this variation using the standard deviation of our sample proportions, seeing that the standard deviation decreases with the square root of the sample size:\n\n\n\n\nPreviously seen comparing standard deviations of proportions red for 100 different shovels\n\n\n\n\nSo as the sample size increases, the standard deviation of the proportion of red beads decreases. This type of standard deviation has another special name: standard error\n\n6.3.3 What is a “standard error”?\nThe “standard error” (SE) is a term that measures the accuracy with which a sample distribution represents a population through the use of standard deviation. Specifically, SE is used to refer to the standard deviation of a sample statistic (aka point estimate), such as the mean or median. For example, the “standard error of the mean” refers to the standard deviation of the distribution of sample means taken from a population.\n\nIn statistics, a sample mean deviates from the actual mean of a population; this deviation is the standard error of the mean.\n\nMany students struggle to differentiate the standard error from the standard deviation. The relationship between the standard error and the standard deviation is such that, for a given sample size, the standard error equals the standard deviation divided by the square root of the sample size. Accordingly, the standard error is inversely proportional to the sample size; the larger the sample size, the smaller the standard error because the statistic will approach the actual value.\n\nThe more data points involved in the calculations of the mean, the smaller the standard error tends to be. When the standard error is small, the data is said to be more representative of the true mean. In cases where the standard error is large, the data may have some notable irregularities. Thus, larger sample size = smaller standard error = more representative of the truth.\nTo help reinforce these concepts, let’s re-display our previous figure but using our new sampling terminology, notation, and definitions:\n\n\n\n\nThree sampling distributions of the sample proportion \\(\\hat{ ho}\\). Note the increased concentration on the bins around .4 as our sample size increases.\n\n\n\n\nFurthermore, let’s display the graph of standard errors for \\(n = 1\\) to \\(n = 100\\) using our new terminology, notation, and definitions relating to sampling.\n\n\n\n\nStandard errors of the sample proportion based on sample sizes of 1 to 100\n\n\n\n\nRemember the key message of this last table: that as the sample size \\(n\\) goes up, the “typical” error of your point estimate will go down, as quantified by the standard error.\n\n6.3.4 The moral of the story\nIf we could only know two pieces of information from our data, what would they be? First, you need a measure of the center of the distribution. This would include the mean or median, which shows the center of our data points. Second, we need a measure of the variability of the distribution. To understand our center, we must understand how different (or how spread) our data points are from one another. Thus, we need a measure like sd() or MAD. These are summary statistics which are necessary to understanding a distribution. Do those two figures encompass all you need to know about a distribution? No! But, if you are only allowed two numbers to keep, those are the most valuable.\n\nThe mean or median is a good estimate for the center of the posterior and the standard error or mad is a good estimate for the variability of the posterior, with +/- 2 standard errors covering 95% of the outcomes.\n\nThe standard error measures the accuracy of a sample distribution as compared to the population by using the standard deviation. Specifically, the standard deviation of our data points divided by the square root of the sample size. As such, we find that larger sample sizes = lower standard errors = more accurate and representative guesses.\nTo really drive home our point: standard error is just a fancy term for your uncertainty about something you don’t know. Standard error == our (uncertain) beliefs.\n\n\n\n\nIf you are wondering how much you need to know, follow this helpful guide of the information we have learned this chapter!\n\n\n\n\n\nThis hierarchy represents the knowledge we need to understand standard error (SE). At the bottom, we have math. It’s the foundation for our understanding, but it doesn’t need to be what we take away from this lesson. As we go up, we simplify the topic. The top of the pyramid are the basic levels of understanding that will help you to remember in the future.\nIf I know your estimate plus or minus two standard errors, I know your 95% confidence interval. This is valuable information. Standard error is really just a measure for how uncertain we are about something we do not know, the thing we are estimating. When we recall SE, we should remember that, all in all, it’s a complicated concept that can be distilled into: the way old people talk about confidence intervals.\n\nRecall that \\(\\hat{\\rho}\\) is the estimated value of p which comes from taking a sample. There can be billions and billions of \\(\\hat{\\rho}\\)’s. We look at a large group of \\(\\hat{\\rho}\\)’s, create a distribution of results to represent the possible values of p based on our findings, and then we compute a standard error to account for our own uncertainty about our predictions. Our 95% confidence interval for our prediction == our estimate plus or minus two standard errors.\nIn regards to the fifth layer of the hierarchy, we may wonder:\n\n“I thought that MADs were the same thing as standard deviations. Now you say they are the same things as standard errors. Which is it?”\n\nMADs and standard deviations are, more or less, the same thing. They are both measures of the variability of a distribution. In most cases, they have very similar values. A standard error is also a standard deviation. Specifically, it is the standard deviation of the distribution of the estimates, and that distribution of estimates is, more or less, your posterior. Therefore, we can use MAD, like standard error, to describe that distribution and the variability of that distribution."
  },
  {
    "objectID": "06-one-parameter.html#cardinal-virtues",
    "href": "06-one-parameter.html#cardinal-virtues",
    "title": "6  One Parameter",
    "section": "\n6.4 Cardinal Virtues",
    "text": "6.4 Cardinal Virtues\nRecall the questions we asked at the beginning of this Chapter:\n\nIf we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion \\(\\rho\\) of the beads in the urn are red?\n\n\nWhat is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?\n\nUse the Cardinal Virtues to guide your thinking.\n\n6.4.1 Wisdom\n\n\n\n\n\nWisdom requires the creation of a Preceptor Table, an examination of our data, and a determination as to whether or not we can (reasonably!) assume that the two come from the same population.\n\n\n\n\nWhen you see another Precetor Table section.\n\n\n\n\nA Preceptor Table is a table with rows and columns, such that, if no data is missing, we can easily answer our questions.\n\n\n\n\n\n\n\nPreceptor Table\n    \n\nID\n      Color\n    \n\n\n1\nwhite\n\n\n2\nwhite\n\n\n...\n...\n\n\n200\nred\n\n\n201\nwhite\n\n\n...\n...\n\n\n2078\nred\n\n\n2079\nwhite\n\n\n...\n...\n\n\n\n\n\n\n \nNote that the beads do not have ID numbers printed on them. The numbering is arbitrary. Having an ID just reminds us that there are actual units under consideration, even if we can not tell them apart, other than by color. We also include the ID to help visualize the fact that we don’t know the total number of beads in the urn, because our question never tells us! There could be 1,000 beads like our physical urn from earlier, or there could be a million beads. The ellipse at the bottom of the Preceptor Table denotes our uncertainty regarding urn size.\nThere is only one outcome column, “Color,” because this is not a causal model, for which we need to have (at least) two potential outcomes. Predictive models require only one outcome.\nIf we know the color of every bead, then calculating the proportion of beads which are red, \\(\\rho\\), is simple algebra. Once we know \\(\\rho\\) we can simulate the answers to other questions.\n\nThe data we have, unfortunately, only provides the color for 50 beads.\n\n\n\n\n\n\n\nData from Shovel\n    \n\nID\n      Color\n    \n\n\n2\nwhite\n\n\n...\n...\n\n\n200\nred\n\n\n...\n...\n\n\n2079\nwhite\n\n\n...\n...\n\n\n\n\n\n\n \nAgain, there are no ID numbers. But keeping track of which beads were in the sample and which beads were not is helpful.\nThe last step of Wisdom is to consider whether or not we can consider the units from the Preceptor Table and the units from the data to have been drawn from the same population. In this case, as with many sampling scenarios, it is trivial that we may make this assumption. If all the rows from the data are also rows in the Preceptor Table, we may assume that they are drawn from the same distribution.\n\nWe also consider why some beads do get sampled, while others do not. This is a consequence of the sampling mechanism. We hope that our sampling mechanism has near equal access to all members of the population, or else our samples would be biased. Almost all samples have some bias, but we must make a judgement call to see if the data we have is close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population. Our sample of 50 beads is taken from a mixed urn, so hopefully there is a near equal chance of selecting each bead, and our samples are representative of the population.\n\n\n\n6.4.2 Justice\n\n\n\n\n\nJustice starts with the construction of a Population Table. We use this table to explore the issues of validity, stability and representativeness. We then make an assumption about the form of the data generating mechanism.\n\n6.4.2.1 Population Table\nWe use The Population Table to acknowledge the wider source we could have collected data from.\nIt includes rows from three sources: the data for units we want to have (the Preceptor Table), the data for units which we have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\n\n\n\n\n\n\n\n\n\nSource\n      Location\n      Time\n      ID\n      Color\n    \n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nTime of sample - 2 years\n\n\n1\n\n\n?\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nTime of sample - 3 weeks\n\n\n200\n\n\nred\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nTime of sample - 10 days\n\n\n976\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nData\n\n\nKnown, specific urn\n\n\nTime of sample\n\n\n2\n\n\nwhite\n\n\n\n\nData\n\n\nKnown, specific urn\n\n\nTime of sample\n\n\n200\n\n\nred\n\n\n\n\nData\n\n\nKnown, specific urn\n\n\nTime of sample\n\n\n1080\n\n\nwhite\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nTime of sample + 10 days\n\n\n1\n\n\n?\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nTime of sample + 3 weeks\n\n\n200\n\n\nred\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nTime of sample + 2 years\n\n\n2025\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPreceptor Table\n\n\nKnown, specific urn\n\n\nNow\n\n\n1\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nKnown, specific urn\n\n\nNow\n\n\n200\n\n\nred\n\n\n\n\nPreceptor Table\n\n\nKnown, specific urn\n\n\nNow\n\n\n2078\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nNow + 10 days\n\n\n1\n\n\n?\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nNow + 3 weeks\n\n\n200\n\n\nred\n\n\n\n\nPopulation\n\n\nKnown, specific urn\n\n\nNow + 2 years\n\n\n2300\n\n\n?\n\n\n\n\n\n\n\n \nEach specific row represents one subject, which are individual beads in our urn scenario. Because there could be thousands or even millions of beads, we provide 3 examples for each category, and use an elipse to denote that there are many more subjects that we have yet to record.\nEach Population Table will have a minimum of 3 types of columns: covariates, time, and outcome(s):\nWhen we construct a Preceptor Table to answer our question, we must select some covariates that we want all of our subjects to have. To answer our questions, we in theory should collect data from subjects with our desired covariates. However, life is tough, and the data we get is not always ideal. Our urn scenario only has one covariate: location. Rather intuitively, urns can have drastically different contents, so to easily answer our question it’s important to identify the location of one specific urn for which to find the proportion red.\n\nBecause we draw our sample from the exact same urn our question asks us about, the data we collect comes directly from the Preceptor Table, all subjects in our population have the same location (“Known, specific urn”). The Preceptor Table and Population categories are essentially identical. This is the perfect scenario for us, but this rarely occurs in real life.\n\nPopulation Tables always have a column for time. When answering a question we must specify the time that it’s being asked, because life changes over time.\n\nWe must acknowledge that the sample from the urn could have been taken at any time, so the contents of the urn in the past (our data) could be different from the contents of the urn when we want to answer our question now (the Preceptor Table). As such, there is a wider population we could have collected our data from: any time before collecting the sample, or anytime after collecting it.\n\nFinally, Population Tables have an outcome. Sometimes there will be multiple outcome columns, like in the case of casual models where we ideally want data on a subject before and after treatment. In our urn problem, we have two outcomes: bead ID and bead color.\n\nBecause we know that the covariates of our data match the covariates of the Preceptor Table, the Data, Preceptor Table, and wider Population all come from the same urn. As such, we can use bead ID as a tool to notate which members of the population we have data on, and which ones are unknown. We always know the color for bead 200 because we sampled it. We also track bead 1 across the Preceptor Table and Population, and never know its value because it was never sampled. Beads 978, 1080, 2025, 2078, and 2300 all change over time simply because different beads could be in the urn at different times. For all we know, someone could be sneakily dumping beads into our urn each week. Either way, because we did not sample those beads, we don’t know their colors.\n\n6.4.2.2 Validity, stability, and representativeness\nNow that we have created our Population Table, we can analyze some key issues: validity, stability, and representativeness.\nValidity involves the columns of our data set. Is the meaning of our columns consistent across the different data sources? In our urn scenario, does bead color in our sampled data and bead color in our Preceptor Table mean the same thing? The answer is yes, and validity can be assumed very easily. If some beads in the urn are old and the color has worn off, it may be more difficult to conclude whether the color column always means the same thing.\nStability involves time. Is the model — meaning both the mathematical formula and the value of the parameters — stable over time? Realistically, an urn will always be the same over time. However, what if someone dumps some red beads into the urn after we take our sample? Then we cannot assume stability, because the proportion of red beads in the urn, \\(\\rho\\), the instant before the dump is different than the proportion red in the urn after. We will assume no one is tampering with our urn, and assert stability across time periods.\nRepresentativeness involves the data rows, specifically the rows for which we have data versus the rows which might have been. Are the rows that we do have data for representative of the rows for which we do not have data? For the sample proportion to be similar to the actual population proportion, we ideally want the data we have to be a random, unbiased selection from our population, using a considerable sample size. In the context of our problem, the sampling mechanism of using a shovel of size 50 to sample beads from an urn in which the beads are thoroughly mixed should be enough to consider our sample representative of the population, and we can move on.\n\n6.4.2.3 The DGM\nThe final aspect of Justice is assuming/creating a mathematical formula — a data generating mechanism, or DGM — which mimics the process by which the data comes to us. The DGM for sampling scenarios with only two possible values is often denoted as follows:\n\\[ T_i  \\sim B(\\rho, n = 50) \\]\nThe total number of red beads selected in a sample of 50 beads, \\(T_i\\), is equal to one draw from a binomial distribution with \\(n = 50\\) observations and an unknown probability \\(\\rho\\) of the proportion of red beads in the urn.\n\n6.4.3 Courage\n\n\n\n\n\nWe’ve evaluated our data and questions and chosen a mathematical formula. Now it’s time to work on our code to create a model that can simulate the real world.\n\n6.4.3.1 Bayesian framework\nWe are Bayesian data scientists that make Bayesian models. This means that we make specific assumptions and consider different things to be either fixed or variable compared to other data science frameworks. One of the most important distinctions is that in Bayesian data science, we don’t know the values of our parameters. Consider the DGM created in Justice:\n\\[ T_i  \\sim B(\\rho, n = 50) \\]\nSome non-Bayesian frameworks are concerned with the probability distribution of our observed data, but do not care much about the probability distribution for \\(\\rho\\) and assume it to be fixed. If \\(\\rho\\) is fixed, the equation above becomes one simple binomial distribution. Think of this as a standard 2 dimensional plot.\nUs Bayesians consider our observed data to be fixed. We don’t consider alternate realities where our observed data is different due to sampling variation. Instead, we are concerned with the probability distribution of our parameter. In our urn scenario, \\(\\rho\\) is variable, so we have to create a separate binomial distribution for each possible value of \\(\\rho\\). Think of this as a 3 dimensional joint distribution like what we created in Section @ref(n-models).\nIt is essential to understand the joint distribution and the posterior, two concepts Bayesians use to solve problems. We will provide quick a quick review here, including statistical notation that may be helpful to some.\nThe joint distribution, \\(p(y|\\theta)\\), models the outcome \\(y\\) given one or more unknown parameter(s), \\(\\theta\\). The equation illustrates exact same concept we addressed while discussing the distinctions of Bayesian science: because our parameters are variable, we have to create separate distributions for each potential value. Combining all these distributions together creates a joint distribution that is 3 dimensional when plotted.\nThe posterior, \\(p(\\theta|y)\\), is the probability distribution of our parameter(s) \\(\\theta\\), created using data \\(y\\) that updates our beliefs. We have referenced the posterior many times before, and this definition does not change its meaning.\nIn our urn scenario, obtaining the posterior involves first creating many binomial distributions for each possible population proportion. This is the joint distribution, and it is a 3 dimensional model. We then select the distribution that corresponds with our data: 17 red beads are sampled. We can represent the posterior with the following:\n\\[\\text{Prob}(\\text{models} | \\text{data} = 17)\\]\nThis is equivalent to taking a 2 dimensional slice of the 3 dimensional model. We are left with a probability distribution for our parameter, \\(\\rho\\).\n\n6.4.3.2 stan_glm()\nGiven a data set to use and a mathematical formula to work with, the next step is to write some code. We will use the rstanarm package, which provides a user friendly interface to work with the statistical language Stan.\nrstanarm and Stan are appealing because they are powerful. Functions such as stan_glm() can do everything we did by hand in Chapter 5 in a few lines of code. Because we will use a professional statistical library, the objects we make will become more complex. In this Chapter, we provide the steps for answering our questions. Chapter @ref(two-parameters) will provide a more detailed explanation of the objects we will make. To be clear, you do not need to fully understand this section or how this code works. This is an introduction, not a formal lesson.\n\nlibrary(rstanarm)\n\nWarning: package 'Rcpp' was built under R version 4.2.1\n\nfit_1 <- stan_glm(formula = red ~ 1, \n                  data = tibble(red = c(rep(1, 17), \n                                        rep(0, 33))),\n                  family = binomial,\n                  refresh = 0,\n                  seed = 10) \n\nRecall that we assumed a binomial model for the data generating mechanism. In stan_glm() we denote this with family = binomial. In addition to the type of the distribution, we also need to analyze the outcome and predictor variables involved. The outcome is the quantity we are measuring, in this case the total number of red beads in our sample. Because we have no predictors, we use the argument formula = red ~ 1, which means that we only model the outcome based on the unknown proportion of red beads in the urn, \\(\\rho\\).\nWe pass in data in a binomial format: the 1’s represent the number of successes (red beads drawn), and the 0’s represent the number of failures (white beads drawn). As such, we pass a tibble with 17 red beads and 33 white beads into data.\nWe use refresh = 0 to suppress the behavior of printing to the console, and seed = 10 so that we get the same output every time we run the code. The resulting model is:\n\nfit_1\n\nstan_glm\n family:       binomial [logit]\n formula:      red ~ 1\n observations: 50\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) -0.7    0.3  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWe will learn the meaning of this output in Chapter @ref(two-parameters). Once we have the fit_1 object, it is easy to answer two sorts of questions: the posterior probability distribution for \\(\\rho\\) and predictions for new draws from the urn. The key new functions are posterior_epred() for the former and posterior_predict() for the latter.\nLet’s recreate our posterior using posterior_epred():\n\nppd_for_p <- posterior_epred(fit_1, \n                newdata = tibble(constant = 1)) |> \n  as_tibble() |>\n  rename(p = `1`)\n  \n# posterior_epred() will unhelpfully name the column of our tibble to \"1\". We\n# have two options: either refer to the column name as `1`, or rename the column\n# to make it less confusing. We will rename the column to \"p\" in this chapter, but you\n# will oftentimes see `1` in later chapters.\n\nppd_for_p\n\n# A tibble: 4,000 × 1\n       p\n   <dbl>\n 1 0.176\n 2 0.165\n 3 0.173\n 4 0.246\n 5 0.258\n 6 0.258\n 7 0.243\n 8 0.242\n 9 0.307\n10 0.300\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nBecause we do not wish to create our distribution conditional on some covariate, we want to pass in a tibble with 1 row into the newdata argument. An empty tibble has 0 rows, so we instead pass in a tibble with some junk data. constant = 1 is completely meaningless. Now we plot the result:\n\nppd_for_p |> \n  ggplot(aes(x = p)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Distribution is centered at .34\",\n         x = \"Proportion p of Red Beads in Urn\",\n         y = \"Probability\") + \n  \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nWe have successfully created the posterior distribution and can finally answer the question we started the chapter with:\n\nIf we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion \\(\\rho\\) of the beads in the urn are red?\n\nLook to the distribution we have created! We can see that the bulk of the area under the posterior occurs approximately when \\(\\rho\\) is between .28 and .42, so the answer to our question is that it is likely that 28% to 42% of the beads in the urn in the red. Another reader may believe that the bulk of the area under the posterior occurs when \\(\\rho\\) is between .25 and .45, and may conclude it likely that 25% to 45% of beads are red. They would be perfectly correct! Any range that is large enough to acknowledge the uncertainty we have regarding the exact value of \\(\\rho\\) is acceptable.\nAlthough the most likely probability (the highest bar on the histogram) occurs when \\(\\rho\\) is around .32, The answer is not a single number. Our posterior distribution is just that: a distribution. From our sample, we have many different results for the proportion of red beads in the entire urn. Certain proportions, like the extremes close to 0% or 100%, are essentially impossible due to our sample value being 34%. On the other hand, we could have just as easily sampled 16 or 18 beads from the urn, and sample proportions such as 32% and 36% are very plausible.\nThis means that, while we can provide a range of possibilities (and we can estimate which of those possibilities occur most frequently), we can never say that we know the total number of red beads with certainty. We know that there is the most chance that \\(\\rho\\) is between .28 and about .42, some chance that \\(\\rho\\) is between .15 and .24 or between .42 and .56, and almost no chance that \\(\\rho\\) is below .15 or above .56. With the posterior we can visualize all of these probabilities at once.\nAnother important question remains:\n\nWhy are there 4,000 rows in the stan_glm() tibble?\n\nBy default, stan_glm() will sample from the posterior in 2 sets of 2,000 iterations. If needed we can change the default number of iterations using the iter argument, but there are few reasons to do so. Some of us may still want to know why we sample from the posterior in the first place. Why not use the entire posterior? The answer is that the posterior is a theoretical beast, which makes it difficult to work with.\nFor example, what if we wanted to know the probability that \\(\\rho\\) is between .3 and .4? To answer this using the pure posterior, we would need to calculate the area under the distribution from when \\(.3 < \\rho < .4\\). This is more difficult then it seems, as the posterior is a distribution, so it has no individual observations to work with as it’s continuous!\nInstead, we can work with draws from the posterior. With enough draws we create a close approximation of the posterior which models the counts of our observations. This is an approximation; it is not exactly the posterior, but close enough for our purposes. We can easily convert our posterior distribution into a posterior probability distribution, by making the area under the graph sum to 1. The posterior probability distribution is often used as a visual aid, as percentages are more easy to conceptualize than raw numbers. One way to convert a posterior distribution into a probability distribution is to group by each value of \\(\\rho\\) and turn the counts into probabilities:\n\nppd_for_p |>\n  round(digits = 2) |>\n  group_by(p) |>\n  summarize(prob = n()/nrow(ppd_for_p)) |>\n  arrange(desc(prob))\n\n# A tibble: 46 × 2\n       p   prob\n   <dbl>  <dbl>\n 1  0.35 0.0615\n 2  0.32 0.0605\n 3  0.36 0.0588\n 4  0.34 0.0585\n 5  0.33 0.058 \n 6  0.31 0.054 \n 7  0.37 0.0535\n 8  0.39 0.047 \n 9  0.3  0.0462\n10  0.38 0.0442\n# … with 36 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe can also accomplish a similar effect by passing aes(y = after_stat(count/sum(count)) into geom_histogram() when plotting. Oftentimes, like in answering the probability that \\(\\rho\\) is between .3 and .4, we can work with the posterior distribution to the very end. Just divide the number of draws that meet our condition (are between .3 and .4), by the total number of draws.\n\nsum(ppd_for_p$p > .3 & ppd_for_p$p < .4)/nrow(ppd_for_p)\n\n[1] 0.5445\n\n\nThere is approximately a 54% chance that \\(\\rho\\) is between .3 and .4. Give me draws from the posterior and I can show you the world!\n\n\n\n\n\n6.4.4 Temperance\n\n\n\n\n\nWith the fitted model object fit_1, we can now make predictions into the future and answer our questions.\nRecall the second question we started with:\n\nWhat is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?\n\n\n6.4.4.1 Using the posterior\nWhenever someone asks you a question, you need to decide what posterior probability distribution would make it easy for you to answer that question. In this case, if we know the posterior probability distribution for the number of red beads in a shovel of size 20, then a question about the likelihood of drawing more than 8 (or any other value) is easy to answer.\nThe posterior probability distribution for a probability is a tricky thing. It is much easier just to estimate the posterior probability distribution for the outcome — number of red beads out of 20 — and then work with that distribution in order to answer probability-type questions.\nTo predict these future unknown samples, we use posterior_predict(). We pass the posterior created using stan_glm() as the first argument, and because we want to estimate the number of red draws with a shovel size of 20, we use pass a tibble with 20 rows into newdata.\n\nposterior_predict(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble()\n\n# A tibble: 4,000 × 20\n     `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`  `12`  `13`\n   <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int> <int>\n 1     0     0     0     0     1     0     0     0     1     0     0     0     1\n 2     0     0     0     0     1     0     1     0     0     0     1     0     0\n 3     0     1     0     0     0     0     0     0     0     0     1     0     0\n 4     0     1     0     1     1     0     0     1     0     0     0     0     1\n 5     0     0     0     1     0     0     1     0     0     0     0     1     0\n 6     0     0     0     0     0     1     1     0     0     0     0     0     1\n 7     0     1     1     1     0     0     1     1     0     0     0     0     1\n 8     1     1     1     1     0     0     1     0     1     1     0     0     0\n 9     1     0     1     1     0     0     0     0     0     1     0     1     1\n10     0     0     1     1     0     0     0     0     0     0     0     0     0\n# … with 3,990 more rows, and 7 more variables: `14` <int>, `15` <int>,\n#   `16` <int>, `17` <int>, `18` <int>, `19` <int>, `20` <int>\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nEach of our 4,000 rows represent one instance of us predicting a future sample from the urn, and each column represents the color bead in a shovel slot. We will create a new column called total and use rowSums() to calculate the total number of red beads drawn in the sample. Finally, we will graph the resulting distribution.\n\nppd_reds_in_20 <- posterior_predict(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |> \n  select(total)\n\n\nppd_reds_in_20   |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in 20-slot shovel\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\") +  \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nWe have successfully created the posterior probability distribution for the number of red beads drawn in a shovel of size 20. But before we answer our question, some of us may be wondering why we made our predictions using posterior_predict() instead of posterior_epred(). Let’s examine what happens if we use posterior_epred() instead.\n\npost_epred <- posterior_epred(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |> \n  select(total)\n\npost_epred  |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n    labs(title = \"Posterior probability distribution using posterior_epred()\",\n         subtitle = \"In our scenario, using posterior_epred() is incorrect\",\n         x = \"Number of red beads\",\n         y = \"Probability\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\nUsing posterior_epred()\n\n\n\n\nWhat happened? posterior_epred() shows the distribution of the entire population, which is continuous. The expected predictions can be fractional, because posterior_epred() returns draws from the posterior (which can be fractional) contingent on some covariate. In our scenario we have no covariates for which to create expected predictions from, so posterior_epred() just returns the posterior, but re-scaled to between 0 and 20 beads instead of between 0 and 1 as before. The shape of the distributions are identical:\n\npost_epred |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) + \n  labs(x = \"Number of red beads sampled out of 20\",\n       y = \"Probability\") + \n  ppd_for_p  |>\n  ggplot(aes(x = p)) + \n  geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n  labs(x = \"Proportion red in urn\",\n       y = \"Probability\") + \n  plot_annotation(title = 'Expected prediction for sample of size 20 on left, posterior on right.',\n                  subtitle = \"The two distributions have an identical shape.\")\n\n\n\n\nOn the other hand, posterior_predict() models the posterior distribution for future individuals. In our scenario, we model the binomial distribution of a discrete random variable. The bars only appear at real numbers between 1 and 16, because we are predicting the probability of individual samples. We cannot draw fractions of beads in our sample. Using posterior_predict() essentially replicates the DGM, taking many virtual draws from our urn and summarizing all the results.\nIn summary, use posterior_predict() when to predict the outcome of individual(s) in the future, and use posterior_epred() to model the probability across the entire population using the posterior. To answer our question, we want to know the probability of outcomes using a single shovel of size 20. We should use posterior_predict() to model taking individual samples many times, and we can then analyze the probabilities. If this is confusing do not fret! We will have plenty of practice with these 2 functions for the remainder of this Primer.\nNow let’s attempt to actually answer our question:\n\nWhat is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?\n\nBecause posterior_predict() takes predictive draws for us, we can simply count the number of draws that have more than 8 red beads, and divide by the total number of draws.\n\n# Same code as earlier, included as a refresher. \nppd_reds_in_20 <- posterior_predict(fit_1, \n                  newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |> \n  select(total)\n\n# Calculating the probability\nsum(ppd_reds_in_20$total > 8)/length(ppd_reds_in_20$total)\n\n[1] 0.24925\n\n\nThere is approximately a 25% chance that we will draw more than 8 red beads out of a sample size of 20.\nTo visualize this probability graphically, we will reuse our posterior, and add a new column called above_eight that is TRUE if total > 8.\n\nppd_reds_in_20 <- posterior_predict(fit_1, \n                    newdata = tibble(constant = rep(1, 20))) |> \n  as_tibble() |> \n  mutate(total = rowSums(across(`1`:`20`))) |>\n  select(total) |>\n  mutate(above_eight = ifelse(total > 8, TRUE, FALSE))\n\nppd_reds_in_20\n\n# A tibble: 4,000 × 2\n   total above_eight\n   <dbl> <lgl>      \n 1     4 FALSE      \n 2     3 FALSE      \n 3     5 FALSE      \n 4     6 FALSE      \n 5     3 FALSE      \n 6     4 FALSE      \n 7     6 FALSE      \n 8     3 FALSE      \n 9     7 FALSE      \n10     6 FALSE      \n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe can then can set the fill of our histogram to when above_eight == TRUE to visualize the probability of drawing more than 8 red beads.\n\nppd_reds_in_20   |> \n  \n  # Set fill as above_eight. \n  \n  ggplot(aes(x = total, fill = above_eight)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 50) +\n  \n  # Scale_fill_manual()  is calling grey for the first color and red for the\n  # second color. This is going to highlight the portion of the curve that we\n  # want to highlight in red.\n\n  scale_fill_manual(values = c('grey50', 'red')) +\n    labs(title = \"Posterior Probability Distribution\",\n         subtitle = \"Number of red beads in 20-slot shovel\",\n         x = \"Number of Red Beads\",\n         y = \"Probability\",\n         fill = \"More than 8 Red Beads Drawn?\") +  \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nThe red bars illustrate the area under a specific section of the curve, as compared to the entire area under the curve. Each question requires looking at a new area under the curve. When someones asks you a question, they are doing two things. First, they are providing instructions as to the posterior your should create. Here, the results with a shovel of 20 slots. Second, they are asking a question about the area under the curve in a specific region. Here, the region where the number of red beads is greater than 8 is highlighted in red. Therefore, the area below the curve that is red is how we get our estimate.\nSee Chapter @ref(two-parameters) for a thorough discussion of the use of rstanarm. This package will be our main tool for the rest of the Primer.\n\n6.4.4.2 Assesing uncertainty\nThe process Cardinal Virtues process may seem tedious, but we use them for good reason. In Wisdom we determined what data we need to answer our question, and in Justice we analyzed whether or not the data we have can answer our question. If we had skipped these steps, we would be at risk of creating models with “bad data” in Courage and Temperance, and draw misleading conclusions.\nWe have successfully answered our questions using the Cardinal Virtues, but we must also realize that there are limitations to our models. Earlier we claimed there is a 26% chance that we will draw more than 8 red beads out of a sample size of 20. However, we must be modest about our claims. Is this question about this particular urn, or all urns? Clearly, as our predictions are based upon the posterior from our specific urn, our conclusions can only be applied to this particular urn; our conclusions are not generalizable to all urns.\nFurthermore, how certain are we in our prediction from Temperance that 26% of samples out of a shovel size of 20 using our urn would produce more than 8 red beads?\nFirst let’s model our situation:\n\\[ T_i  \\sim B(\\rho, n = 20) \\]\nThe total number of red beads selected in a sample of 20 beads, \\(T_i\\), is modeled by a binomial distribution with \\(n = 20\\) observations and an unknown probability \\(\\rho\\) of the proportion of red beads in the urn.\nWe must realize that our 26% estimate is based on an entire posterior. We are uncertain about the exact value of \\(\\rho\\), so posterior_predict() takes our uncertainty into account when creating the model. However, there is in fact some actual value for \\(\\rho\\), even if we don’t know it.\nIf the actual value of \\(\\rho\\) was 30%, we can calculate the correct answer to our question using the binomial probability distribution function:\n\npbinom(q = 8, size = 20, prob = .3, lower.tail = FALSE)\n\n[1] 0.1133315\n\n\nOr maybe the actual value of \\(\\rho\\) is 40%:\n\npbinom(q = 8, size = 20, prob = .4, lower.tail = FALSE)\n\n[1] 0.4044013\n\n\nSadly, the answer to our question could be 11%, 40%, a value in between, or even a more extreme value. We will never know the correct answer, and must settle with our best estimate of 26% that factors in our uncertainty regarding the true value of \\(\\rho\\). Long story short, our models are never the truth.\n\n6.4.5 Bad practices\nAlthough the process was long, we have successfully gone through the cardinal virtues and answered our questions like real data scientists. In this section we will address some pitfalls to avoid.\n\n6.4.5.1 Confidence intervals\nLet’s go back to our first question:\n\nIf we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion \\(\\rho\\) of the beads in the urn are red?\n\nIf we had decided to, we could have answered this question using confidence intervals and statistics. First calculate the standard error:\n\\[  SE = \\frac{\\sigma\\text{ of data}}{\\sqrt{\\text{sample size}}} = \\frac{.4785}{\\sqrt{50}} \\approx .067\\]\nWe calculate the numerator by taking the standard deviation of a dataset with 50 observation, where 1’s represent red beads and 0’s represent white beads:\n\nsd(c(rep(1, 17), rep(0, 33)))\n\n[1] 0.4785181\n\n\nYou can then use the standard error to create a 95% confidence interval:\n\\[  CI = \\bar{x} \\hspace{.1cm} \\pm 2SE = .34 \\hspace{.1cm} \\pm .134\\]\nWith 95% confidence, the proportion of red beads in the urn is between 21% and 47%.\nThis is correct, but quite difficult to conceptualize. If our boss needs a quick answer, by all means we can use a confidence interval to save us from doing all the work we did earlier! However, we must be careful with confidence intervals. If we had chosen to answer the first question using them, we would be unable to answer any questions in Temperance. The confidence interval gives us a general range of our uncertainty, but to answer a question that requires knowledge about the value of a parameter, the confidence interval does us very little good. As data scientists we create the posterior distribution to quantify our uncertainty and answer all our questions.\n\n6.4.5.2 Hypothesis tests\nStatisticians, like how they use confidence intervals, also use hypothesis tests to quickly try to answer questions. Recall our view on hypothesis tests: Amateurs test. Professionals summarize. Traditionally, most scientific papers are not so much interested in estimating \\(\\rho\\). They are interested in testing specific hypotheses. What do we mean by that?\nLet’s look at a possible hypothesis in our urn paradigm: there are equal number of red and white beads in the urn. The null hypothesis, denoted by \\(H_0\\), is the theory we are testing, while the alternative hypothesis, denoted by \\(H_a\\), represents the opposite of our theory. Therefore, our hypothesis is designed as such:\n\\(H_0\\): There are an equal number of red and white beads in the urn.\n\\(H_a\\): There are not an equal number of red and white beads in the urn.\nCan we reject that hypothesis? Convention: if the 95% confidence interval excludes the null hypothesis, then we reject it. Here, that would mean if our posterior estimate (plus or minus 2 standard errors) excluded the possibility of the red and white beads being equal (\\(\\rho = .5\\)) we can reject the null hypothesis. In the previous section we determined that the 95% confidence interval is between 21% and 47%. Because 50% is outside of this interval, we could reject the null hypothesis, and conclude that it is unlikely that the proportion of beads in the urn is 50%.\nIf we were testing the theory that \\(\\rho = .45\\) instead, our null hypothesis would fall within the confidence interval. This does not mean that we accept the null hypothesis, we just don’t reject it. When we can’t reject the null hypothesis our test is essentially useless, as we have no idea whether or not our theory is true. In our scenario we only know that there is some possibility that \\(\\rho = .45\\). We’re just back where we started! This is why we never test — unless your boss demands a test. Use your judgment, make your models, summarize your knowledge of the world, and use that summary to make decisions."
  },
  {
    "objectID": "06-one-parameter.html#sampling-case-study",
    "href": "06-one-parameter.html#sampling-case-study",
    "title": "6  One Parameter",
    "section": "\n6.5 Case study: Polls",
    "text": "6.5 Case study: Polls\nLet’s now switch gears to a more realistic sampling scenario: a poll. In practice, pollsters do not take 1,000 repeated samples as we did in our previous sampling activities, but rather take only a single sample that’s as large as possible.\nOn December 4, 2013, National Public Radio in the US reported on a poll of President Obama’s approval rating among young Americans aged 18-29 in an article, “Poll: Support For Obama Among Young Americans Eroding.” The poll was conducted by the Kennedy School’s Institute of Politics at Harvard University. A quote from the article:\n\nAfter voting for him in large numbers in 2008 and 2012, young Americans are souring on President Obama.\nAccording to a new Harvard University Institute of Politics poll, just 41 percent of millennials — adults ages 18-29 — approve of Obama’s job performance, his lowest-ever standing among the group and an 11-point drop from April.\n\nLet’s tie elements of the real life poll in this new article with our “tactile” and “virtual” urn activity from Sections @ref(sampling-activity) and @ref(virtual-sampling) using the terminology, notations, and definitions we learned in Section @ref(standard-errors). You’ll see that our sampling activity with the urn is an idealized version of what pollsters are trying to do in real life.\nFirst, who is the (Study) Population of \\(N\\) individuals or observations of interest?\n\nUrn: \\(N\\) = 1000 identically sized red and white beads\nObama poll: \\(N\\) = ? young Americans aged 18-29\n\nSecond, what is the population parameter?\n\nUrn: The population proportion \\(\\rho\\) of all the beads in the urn that are red.\nObama poll: The population proportion \\(\\rho\\) of all young Americans who approve of Obama’s job performance.\n\nThird, what would a census look like?\n\nUrn: Manually going over all \\(N\\) = 1000 beads and exactly computing the population proportion \\(\\rho\\) of the beads that are red.\nObama poll: Locating all \\(N\\) young Americans and asking them all if they approve of Obama’s job performance. In this case, we don’t even know what the population size \\(N\\) is!\n\nFourth, how do you perform sampling to obtain a sample of size \\(n\\)?\n\nUrn: Using a shovel with \\(n\\) slots.\nObama poll: One method is to get a list of phone numbers of all young Americans and pick out \\(n\\) phone numbers. In this poll’s case, the sample size of this poll was \\(n = 2089\\) young Americans.\n\nFifth, what is your point estimate (AKA sample statistic) of the unknown population parameter?\n\nUrn: The sample proportion \\(\\hat{\\rho}\\) of the beads in the shovel that were red.\nObama poll: The sample proportion \\(\\hat{\\rho}\\) of young Americans in the sample that approve of Obama’s job performance. In this poll’s case, \\(\\hat{\\rho} = 0.41 = 41\\%\\), the quoted percentage in the second paragraph of the article.\n\nSixth, is the sampling procedure representative?\n\nUrn: Are the contents of the shovel representative of the contents of the urn? Because we mixed the urn before sampling, we can feel confident that they are.\nObama poll: Is the sample of \\(n = 2089\\) young Americans representative of all young Americans aged 18-29? This depends on whether the sampling was random (which samples rarely are)\n\nSeventh, are the samples generalizable to the greater population?\n\nUrn: Is the sample proportion \\(\\hat{\\rho}\\) of the shovel’s beads that are red a “good guess” of the population proportion \\(\\rho\\) of the urn’s beads that are red? Given that the sample was representative, the answer is yes.\nObama poll: Is the sample proportion \\(\\hat{\\rho} = 0.41\\) of the sample of young Americans who supported Obama a “good guess” of the population proportion \\(\\rho\\) of all young Americans who supported Obama at this time in 2013? In other words, can we confidently say that roughly 41% of all young Americans approved of Obama at the time of the poll? Again, this depends on whether the sampling was random.\n\nEighth, is the sampling procedure unbiased? In other words, do all observations have an equal chance of being included in the sample?\n\nUrn: Since each bead was equally sized and we mixed the urn before using the shovel, each bead had an equal chance of being included in a sample and hence the sampling was unbiased.\nObama poll: Did all young Americans have an equal chance at being represented in this poll? Again, this depends on whether the sampling was random.\n\nNinth and lastly, was the sampling done at random?\n\nUrn: As long as you mixed the urn sufficiently before sampling, your samples would be random.\nObama poll: Was the sample conducted at random? We can’t answer this question without knowing about the sampling methodology used by Kennedy School’s Institute of Politics at Harvard University. We’ll discuss this more at the end of this section.\n\nIn other words, the poll by Kennedy School’s Institute of Politics at Harvard University can be thought of as an instance of using the shovel to sample beads from the urn. Furthermore, if another polling company conducted a similar poll of young Americans at roughly the same time, they would likely get a different estimate than 41%. This is due to sampling variation.\nLet’s now revisit the sampling paradigm from Subsection @ref(terminology-and-notation):\nIn general:\n\nIf the sampling of a sample of size \\(n\\) is done at random, then\nthe sample is unbiased and representative of the population of size \\(N\\), thus\nany result based on the sample can generalize to the population, thus\nthe point estimate is a “good guess” of the unknown population parameter, thus\ninstead of performing a census, we can infer about the population using sampling.\n\nSpecific to the urn:\n\nIf we extract a sample of \\(n = 50\\) beads at random, in other words, we mix all of the equally sized beads before using the shovel, then\nthe contents of the shovel are an unbiased representation of the contents of the urn’s 1000 beads, thus\nany result based on the shovel’s beads can generalize to the urn, thus\nthe sample proportion \\(\\hat{\\rho}\\) of the \\(n = 50\\) beads in the shovel that are red is a “good guess” of the population proportion \\(\\rho\\) of the \\(N = 1000\\) beads that are red, thus\ninstead of manually going over all 1000 beads in the urn, we can infer about the urn using the shovel.\n\nSpecific to the Obama poll:\n\nIf we had a way of contacting a randomly chosen sample of 2089 young Americans and polling their approval of President Obama in 2013, then\nthese 2089 young Americans would be an unbiased and representative sample of all young Americans in 2013, thus\nany results based on this sample of 2089 young Americans can generalize to the entire population of all young Americans in 2013, thus\nthe reported sample approval rating of 41% of these 2089 young Americans is a good guess of the true approval rating among all young Americans in 2013, thus\ninstead of performing an expensive census of all young Americans in 2013, we can infer about all young Americans in 2013 using polling.\n\nSo as you can see, it was critical for the sample obtained by Kennedy School’s Institute of Politics at Harvard University to be truly random in order to infer about all young Americans’ opinions about Obama. Was their sample truly random? It’s hard to answer such questions without knowing about the sampling methodology they used.\nFor example, what if Kennedy School’s Institute of Politics at Harvard University conducted this poll using only mobile phone numbers? People without mobile phones would be left out and therefore not represented in the sample. This flaw is an example of censoring, the exclusion of certain datapoints due to an issue with data collection. This results in an incomplete observation and increases the prediction uncertainty of the estimand, Obama’s approval rating among young Americans. Ensuring that our samples were random was easy to do in our sampling urn exercises; however, in a real life situation like the Obama poll, this is much harder to do."
  },
  {
    "objectID": "06-one-parameter.html#summary",
    "href": "06-one-parameter.html#summary",
    "title": "6  One Parameter",
    "section": "\n6.6 Summary",
    "text": "6.6 Summary\nKey lesson:\nThere is a truth! There is a true value for \\(\\rho\\) which we do not know! We want to create a posterior probability distribution which summarizes our knowledge. We care about the posterior probability distribution of p. The center of that distribution is around the mean or median of the proportion in your sample. The sd (or mad) of that posterior is the standard deviation divided by the square root of our sample size! Note that this is the same thing as the standard deviation of the repeated samples.\nKey lesson:\nWe have a journey from reality, to our predictions, to the standard error of our predictions, to the posterior probability distribution for p. This is our sequence:\np (the truth) \\(\\Rightarrow\\) \\(\\hat{\\rho}\\) (my estimate) \\(\\Rightarrow\\) the standard error of \\(\\hat{\\rho}\\) (black box of math mumbo jumbo and computer simulation magic) \\(\\Rightarrow\\) our posterior probability distribution for p (our beliefs about the truth).\nThis journey shows how our beliefs about the truth develop through our work. We begin with p; p is the truth, the true but unknown value we are estimating. \\(\\hat{\\rho}\\) is our estimate for p. There can be millions and millions of \\(\\hat{\\rho}\\)’s. Next, we must take the standard error of our estimates (our \\(\\hat{\\rho}\\)’s) to account for our uncertainty with our predictions. Finally – the thing we need most – we create a posterior probability distribution for p. This distribution is used to answer key questions about p. \nOther highlights:\n\n6.6.1 Tactial sampling\n\nSampling allows us to make guesses at an unknown, difficult-to-obtain value by looking at a smaller subset of data and generalizing it to the larger population.\nSampling is preferable in the urn example because counting out 1000 beads from an urn is intensive and tedious.\nSampling is preferable in the real-world because it is often impossible to sample “all the beads” (all the people) in a population. With sampling, you see variations in your results. This is known as sampling variation and is expected, especially if you draw samples at random (unbiased).\n\n6.6.2 Virtual sampling\n\nBy creating a virtual analog of our urn and our shovel, we were able to look at even more samples and observe the effects of sampling size on our results.\nMore samples yield more even distributions which should resemble a bell.\nLarger sample sizes decrease standard deviation, meaning that the resulting proportions red are closer to one another than when the sample sizes are smaller. This means that larger samples = lower SD = more precise guesses.\nIf we want to repeat the same task multiple times, like comparing the distributions of 3 sample sizes, use mapping and functions like expand_grid() to cut down on repetative code.\n\n6.6.3 Standard error\n\nStandard error is just a fancy term for your uncertainty about something you don’t know. Standard error \\(\\approx\\) our (uncertain) beliefs.\nThe standard error measures the accuracy of a sample distribution as compared to the population by using the standard deviation.\nWe find that larger sample sizes \\(\\implies\\) lower standard errors \\(\\implies\\) more accurate estimates.\n\nIf we could only know two pieces of information from our data, we would need a measure of the center of the distribution (like mean or median) and a measure of the variability of the distribution (like sd or MAD).\nSE refers to the standard deviation of a sample statistic (aka point estimate), such as the mean or median. Therefore, the “standard error of the mean” refers to the standard deviation of the distribution of sample means taken from a population.\n\n6.6.4 Cardinal Virtues\n\n\nstan_glm() can create a joint distribution and then estimate the posterior probability distribution, conditional on the data which was passed in to the data argument. This is a much easier way to create the posterior distribution, and will be explored in more detail in Chapter @ref(two-parameters).\nWe use the posterior distribution to answer our questions.\n\n6.6.5 Case study: Polls\n\nPolling is a more complex real life application of the binomial distribution.\nReal life sampling is extremely prone to error. This cannot be emphasized enough. We are often forced to choose between precision and accuracy during real-life sampling.\n\nIn this chapter, we performed both tactile and virtual sampling exercises to infer about an unknown parameter We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion \\(\\hat{\\rho}\\) to estimate the population proportion \\(\\rho\\). However, we are not just limited to scenarios related to proportions. In other words, we can use sampling to estimate other population parameters using other point estimates as well.\nAs we continue our journey, recall the case of Primrose Everdeen and what she represents: no matter how realistic is our model is, our predictions are never certain."
  },
  {
    "objectID": "08-three-parameters.html",
    "href": "08-three-parameters.html",
    "title": "8  Three Parameters",
    "section": "",
    "text": "Models have parameters. In Chapter @ref(one-parameter) we created models with a single parameter \\(p\\), the proportion of red beads in an urn. In Chapter @ref(two-parameters), we used models with two parameters: \\(\\mu\\) (the average height in the population, generically known as a model “intercept”) and \\(\\sigma\\) (the variation in height in the population). Here — can you guess where this is going? — we will build models with three parameters: \\(\\sigma\\) (which serves the same role throughout the book) and two “coefficients.” In models which relate a continuous predictor to the outcome, those two parameters will be labeled \\(\\beta_0\\) and \\(\\beta_1\\). In models which estimate two averages, the parameters will be \\(\\beta_1\\) and \\(\\beta_2\\). All this notation is confusing, not least because different academic fields use inconsistent schemes. Follow the Cardinal Virtues and tackle your problem step by step."
  },
  {
    "objectID": "08-three-parameters.html#wisdom",
    "href": "08-three-parameters.html#wisdom",
    "title": "8  Three Parameters",
    "section": "\n8.1 Wisdom",
    "text": "8.1 Wisdom\n\n8.1.1 Preceptor Table\nWisdom begins with considering the questions we desire to answer and the data set we are given. In this chapter, we are going to ask a series of questions involving train commuters’ ages, party affiliations, incomes, and political ideology, as well as the causal effect of exposure to Spanish-speakers on their attitude toward immigration. These questions will pertain to all train commuters in the US today. Given these types of questions, the Preceptor Table would be:\n\n\nWarning: package 'tidyverse' was built under R version 4.2.1\n\n\nWarning: package 'tibble' was built under R version 4.2.1\n\n\nWarning: package 'patchwork' was built under R version 4.2.1\n\n\n\n\n\n\n\n\n\nPreceptor Table\n    \n\nID\n      Age\n      Party\n      Income\n      Liberal\n      Control Ending Attitude\n      Treated Ending Attitude\n    \n\n\n\nCommuter 1\n\n\n23\n\n\nDemocrat\n\n\n50000\n\n\nLiberal\n\n\n3\n\n\n8\n\n\n\n\nCommuter 2\n\n\n18\n\n\nRepublican\n\n\n150000\n\n\nNot Liberal\n\n\n7\n\n\n7\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nCommuter 1000\n\n\n49\n\n\nRepublican\n\n\n100000\n\n\nLiberal\n\n\n4\n\n\n8\n\n\n\n\nCommuter 1001\n\n\n38\n\n\nDemocrat\n\n\n200000\n\n\nLiberal\n\n\n9\n\n\n7\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\n\n\n\nRecall: a Preceptor Table is the smallest possible table such that, if there is no missing data, all our questions are easy to answer. To answer questions — like “What is, today, the average age of train commuters in the US?” — we need a row for every train commuter.\nNotice that one of our questions is about a causal effect: What change in immigration attitudes is caused by being exposed to Spanish-speakers? Answering causal questions requires (at least) two potential outcomes: immigration attitude for those who receive the treatment of being exposed to Spanish-speakers and for those who do not.\nHaving created the Preceptor Table, we now look at the data we have: the trains data set from the primer.data package.\n\n8.1.2 EDA for trains\n\nAlways explore your data. Recall the discussion from Chapter @ref(rubin-causal-model). @enos2014 randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. Exposure to Spanish-speakers – the treatment – influenced attitudes toward immigration. These reactions were measured through changes in answers to three survey questions. Load the necessary libraries and look at the data.\n\nlibrary(primer.data)\nlibrary(rstanarm)\n\nWarning: package 'Rcpp' was built under R version 4.2.1\n\nlibrary(skimr)\nlibrary(tidyverse)\n\n\nglimpse(trains)\n\nRows: 115\nColumns: 14\n$ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n$ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n$ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n$ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n$ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n$ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n$ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n$ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n$ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n$ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n$ hisp_perc      <dbl> 0.0264, 0.0154, 0.0191, 0.0191, 0.0191, 0.0231, 0.0304,…\n$ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n$ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…\n\n\nThe data include information about each respondent’s gender, political affiliations, age, income and so on. treatment indicates whether a subject was in the control or treatment group. The key outcomes are their attitudes toward immigration both before (att_start) and after (att_end) the experiment. Type ?trains to read the help page for more information about each variable.\nLet’s restrict attention to a subset of the variables we need to answer our questions, as specified in the Preceptor Table. age is the age of the respondent; party is their political party affiliation; liberal is whether they are liberal or not; income is the income of the respondent; treatment specifies whether the respondent was given the treatment of being exposed to Spanish-speakers in the train station; and att_end tell us about the respondent’s attitude on immigration after the treatment, .\n\nch8 <- trains |> \n  select(age, att_end, party, income, treatment, liberal)\n\nIt is always smart to look at a some random slices of the data:\n\nch8 |> \n  slice_sample(n = 5)\n\n# A tibble: 5 × 6\n    age att_end party      income treatment liberal\n  <int>   <dbl> <chr>       <dbl> <fct>     <lgl>  \n1    44       9 Democrat   300000 Control   FALSE  \n2    23       6 Democrat   300000 Control   FALSE  \n3    46       6 Republican 250000 Control   FALSE  \n4    45       8 Democrat    87500 Control   FALSE  \n5    38      11 Democrat    87500 Treated   FALSE  \n\n\natt_end is a measure of person’s attitude toward immigration. A higher number means more conservative, i.e., a more exclusionary stance toward immigration into the United States.\n\nch8 |> \n  glimpse()\n\nRows: 115\nColumns: 6\n$ age       <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40, 53, …\n$ att_end   <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 13, 8,…\n$ party     <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Democrat\"…\n$ income    <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 135000…\n$ treatment <fct> Treated, Treated, Treated, Treated, Control, Treated, Contro…\n$ liberal   <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, FALSE,…\n\n\nPay attention to the variable types. Do they make sense? Perhaps. But there are certainly grounds for suspicion. Why is att_end a double rather than an integer? All the values in the data appear to be integers, so there is no benefit to having these variables be doubles. Why is party a character variable and treatment a factor variable? It could be that these are intentional choices made by the creator of the tibble, i.e., us. Or, these could be mistakes. Most likely, these choices are a mixture of sensible and arbitrary. Regardless, it is your responsibility to notice them. You can’t make a good model without looking closely at the data which you are using.\n\nch8 |> \n  skim()\n\n\nData summary\n\n\nName\nch8\n\n\nNumber of rows\n115\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nlogical\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\nparty\n0\n1\n8\n10\n0\n2\n0\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\ntreatment\n0\n1\nFALSE\n2\nCon: 64, Tre: 51\n\n\nVariable type: logical\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\nliberal\n0\n1\n0.44\nFAL: 64, TRU: 51\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nage\n0\n1\n42.37\n12.20\n20\n33\n43\n52\n68\n▆▇▇▇▃\n\n\natt_end\n0\n1\n9.14\n2.87\n3\n7\n9\n11\n15\n▂▃▇▃▃\n\n\nincome\n0\n1\n141813.04\n74476.64\n23500\n87500\n135000\n135000\n300000\n▅▇▇▁▆\n\n\n\n\n\nskim() shows us what the different values of treatment are because it is a factor. Unfortunately, it does not do the same for character variables like party. The ranges for age and att_end seem reasonable. Recall that participants were asked three questions about immigration issues, each of which allowed for an answer indicated strength of agreement on a scale form 1 to 5, with higher values indicating more agreement with conservative viewpoints. att_end is the sum of the responses to the three questions, so the most liberal possible value is 3 and the most conservative is 15.\n\n8.1.3 Population\n\n\nThe data that we have are very limited. There are only 115 observations, all from 2012 and involving train commuters to Boston. Can we assume that the the data we have and the data in our Preceptor Table are drawn from the same population? Only your judgment, along with advice from your colleagues, can guide you.\nThere is no truth here. The data is real enough, but you created the Preceptor Table. Whether or not there is a population from which you can assume both the data and the Preceptor Table might have been drawn is not a TRUE/FALSE question.\nThe Preceptor Table is all US train commuters today. However, the data we have involves train commuters in the Boston area in 2012. So, what we must ask ourselves is if this data from Boston in 2012 can be drawn from the same population as all train commuters in the US today.\nThe key concept is the idea of a “population.” From which larger population is the data we have being (conceptually) drawn? If we were only interested in the age of individuals in our data set, we would have no need for inference. We know everyone’s ages already. We only need tools like stan_glm() if we seek to understand individuals not in our data.\nThe issue is always: Do the data we have and the data we want to have come from the same population? If there is no connection between the two, progress is impossible. But, in this case, if we are willing to consider the population to be US residents over the last decade (including today), and if we are willing to assume that there is a single population from which the Preceptor Table and our data set is drawn, then we can use our data to create a model to answer our questions.\n\n8.1.4 Population Table\nHaving determined that the Preceptor Table and the data were drawn from the same population, we can produce a Population Table.\nThe Population Table includes rows from three sources: the Preceptor Table, the actual data, and the remainder of the population.\nThe rows in the Preceptor Table contain the information that we would want to know in order to answer our questions. These rows contain entries for our covariates (city and year) but they do not contain any outcome results. We are trying to answer questions about the train commuter population in 2021, so our city entries for these rows will vary and our year entries of these rows will read “2021”.\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from a study conducted on train commuters around Boston, MA in 2012, so our city entries for these rows will read “Boston, MA” and our year entries of these rows will read “2012”.\nOur population rows contain no data. These are subjects which fall under our desired population, but for which we have no data. As such, all rows are missing. However, this may include, for example, data from a range of years before and after 2012, since we considered little change overtime, so we would also consider this data to be representative and drawn from the larger population of train commuters in years near 2012.\nThe population table includes commuters from our Preceptor Table with the information that we would ideally have to answer the questions and those from the data we have that is specific to Boston, MA. Additionally, the population table also includes the groups of people within the population from which both the data we have and the Preceptor table is drawn from that we don’t have.\n\n\n\n\n\n\n\nPopulation Table\n    \n\nSource\n      City\n      Year\n      Age\n      Party\n      Income\n      Liberal\n      Treatment\n      Control\n    \n\n\n\nPopulation\n\n\n?\n\n\n2008\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n2009\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nData\n\n\nBoston, MA\n\n\n2012\n\n\n43\n\n\nDemocrat\n\n\n150000\n\n\nLiberal\n\n\n6\n\n\n?\n\n\n\n\nData\n\n\nBoston, MA\n\n\n2012\n\n\n52\n\n\nRepublican\n\n\n50000\n\n\nNot Liberal\n\n\n?\n\n\n2\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\n?\n\n\n2013\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n2014\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPreceptor Table\n\n\nWilmington, DE\n\n\n2021\n\n\n45\n\n\nRepublican\n\n\n45000\n\n\nNot Liberal\n\n\n?\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nAtlanta, GA\n\n\n2021\n\n\n65\n\n\nRepublican\n\n\n15000\n\n\nNot Liberal\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\n?\n\n\n2025\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n2027\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n\n\n\nOur Preceptor Table rows are chronologically ordered. The population shows our greater population for which we are making assumptions — this is why the year at the start is earlier than our data and the year at the end is later than our Preceptor Table. This shows the more expansive population that we are inferring about."
  },
  {
    "objectID": "08-three-parameters.html#age-party",
    "href": "08-three-parameters.html#age-party",
    "title": "8  Three Parameters",
    "section": "\n8.2 age ~ party",
    "text": "8.2 age ~ party\n\nWe want to build a model and then use that model to make claims about the world. Our questions about the relationship between age and party are the following:\nWhat is the expected age of a Democrat at the train station?\nIn a group of three Democrats and three Republicans, what will the age difference be between the oldest Democrat and the youngest Republican?\nWe can answer these and similar questions by creating a model that uses party affiliation to predict age\n\n\n\n\n\n8.2.1 Justice\n\n\n\n\nJustice\n\n\n\n\nJustice consists of four topics: validity, stability, representativeness, and the data generating mechanism (DGM).\nTo understand validity in regards to the Population Table, we must first recognize an inherent flaw in any experiment design: no two units receive exactly the same treatment. If this doesn’t ring true, consider our Spanish speaking train experiment. The units on the Spanish-speaking platform received the same treatment, right? No, actually! Every unit heard Spanish at a different decibel level. Every unit also heard Spanish for a different amount of time, depending on when they arrived to the station. Thus, two units can have the same treatment — that is, hearing Spanish on the platform — while having very different versions of that treatment. This is why it is crucial to define one’s estimand precisely: if we are interested in the difference in potential outcomes between Spanish being spoken for 10 minutes at a 60 dB versus control, we can ignore all the other possible columns in the Population Table.\nWe then must consider the stability of our model. Stability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn. With something like height, it is much easier to assume stability over a greater period of time. Changes in global height occur extremely slowly, so height being stable across a span of 20 years is reasonable to assume. With something like political ideology, it is much harder to make the assertion that data collected in 2010 would be stable to data collected in 2025. When we are confronted with this uncertainty, we can consider making our timeframe smaller. However, we would still need to assume stability from 2014 (time of data collection) to today. Stability allows us to ignore the issue of time.\nNext, let’s consider representativeness. Representativeness has to do with how well our sample represents the larger population we are interested in generalizing to. Does the train experiment allow us to calculate a causal effect for people who commute by cars? Can we calculate the causal effect for people in New York City? Before we generalize to broader populations we have to consider if our experimental estimates are applicable beyond our experiment. Generally: if there was no chance that a certain type of person would have been in this experiment, we cannot make an assumption for that person.\n\nNow let’s discuss the kind of DGM we will be using. For our purposes, we will be choosing between linear or logistic, however, there are several different types of distributions which we will not be dealing with at the moment. Let’s recall the difference between linear and logistic models, which is dependent upon the outcome variable. If the outcome variable is continuous, we will use the a linear model, and when the outcome variable is binary, or only has two options, we will use a logistic model. In this case, our outcome variable is age which is continous, so it is a linear model.\nWe can also consider the type of DGM. Any DGM with age as its dependent variable will be predictive, not causal, for the simple reason that nothing, other than time, can change your age. You are X years old. It would not matter if you changed your party registration from Democrat to Republican or vice versa. Your age is your age. When dealing with a non-causal DGM, the focus is on predicting things. The underlying mechanism which connects age with party is less important than the brute statistical fact that there is a connection. Predictive models care little about causality.\n\n\nA good way at looking at this is with a Preceptor Table, as seen below. Unlike the previous table in Chapter @ref(two-parameters), we now have two columns in addition to ID. Since our data does not include all Republicans and Democrats in the world, not every row is filled in.\n\n\n\n\n\n\n\n\nID\n      \n        Predictor\n      \n      \n        Outcome\n      \n    \n\nPolitical Party\n      Age\n    \n\n\n\n1\nD\n31\n\n\n2\n?\n?\n\n\n...\n...\n...\n\n\n473\nD\n58\n\n\n474\n?\n?\n\n\n...\n...\n...\n\n\n3,258\n?\n?\n\n\n3,259\nR\n49\n\n\n...\n...\n...\n\n\nN\n?\n?\n\n\n\n\n\n\n \nWe now know that we are working with a predictive DGM. Recall:\n\\[\\text{outcome} = \\text{model} + \\text{not in the model}\\]\nIn words, any event depends on our explicitly described model as well as on influences unknown to us. Everything that happens in the world is the result of various factors, and we can only consider some of them in our model (because we do not know about some influences, or because we have no data about them).\n\nLet’s make our DGM. The mathematics:\n\\[ y_i = \\beta_1 republican_i + \\beta_2 democrat_i + \\epsilon_i\\]\nwhere \n\\[republican_i, democrat_i \\in \\{0,1\\}\\] \\[republican_i +  democrat_i = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\nDon’t panic dear poets and philosophers, the whole thing is easier than it looks. This will follow the form above in which the outcome is the result of what is in our model and not in our model.\n\nOn the left-hand side we have the outcome, \\(y_i\\), which is the variable to be explained. In our case, this is the age of an individual in the population.\nThe right-hand side contains two parts, that which is contained within the model, and that which isn’t.\n\nFirst, we have the part contained in the model, which consists of the parameter and the data points. The betas are our two parameters: \\(\\beta_1\\) is the average age of Republicans in the population and \\(\\beta_2\\) is the average age of Democrats in the population. \\(republican_i\\) and \\(democrat_i\\) are our explanatory variables and take the values 1 or 0. As shown in our model, \\(\\beta_1 republican_i\\) and \\(\\beta_2 democrat_i\\) are two similar terms which are added together to make our model and each term consists of a parameter and a data point. If person \\(i\\) is a Republican we have \\(republican_i = 1\\) and \\(democrat_i = 0\\). If person \\(i\\) is a Democrat we have \\(republican_i = 0\\) and \\(democrat_i = 1\\). In other words, their values are mutually exclusive – if you are a Democrat, you cannot also be a Republican.\n\n\nThe second part in the right-hand side, \\(\\epsilon_i\\) (“epsilon”), represents the unexplained part of the outcome and is called the error term. This includes all factors that have an influence on someone’s age but are not related to party affiliation. In other words, \\(\\epsilon_i\\) is what influence age that is not factored into our model. We assume that this error follows a normal distribution with an expected value of 0 (meaning it is 0 on average) and it is simply the difference between the outcome and our model predictions.\n\nSome things to note about our model:\n\nThe small \\(i\\)’s are an index to number the observations. It is equivalent to the “ID” column in our Preceptor Table and simply states that the outcome for person \\(i\\) is explained by the modeled and non-modeled factors for person \\(i\\).\nThe model is a claim about how the world works, not just for the 115 individuals for which we have data but for the all the people in the population for which we seek to draw inferences.\nAlthough terminology differs across academic fields, the most common term to describe a model like this is a “regression.” We are “regressing” age on party in order to see if they are associated with each other. The formula above is a “regression formula”, and the model is a “regression model.” This terminology would also apply to our model of height in Chapter @ref(two-parameters).\nThe model in Chapter @ref(two-parameters) is sometimes called “intercept-only” because the only (interesting) parameter is the intercept. Here we have a “two intercept” model because, instead of estimating an average for the whole population, we are estimating two averages.\n\n8.2.2 Courage\n\n\n\n\nCourage\n\n\n\n\nCourage allows us to translate math to code.\nTo get posterior distributions for our three parameters, we will again use stan_glm(), just as we did in Chapter @ref(two-parameters).\n\nfit_1 <- stan_glm(age ~ party - 1, \n                    data = trains, \n                    family = gaussian,\n                    seed = 17,\n                    refresh = 0)\n\n\nThe variable before the tilde, age, is our outcome.\nThe only explanatory variable is party. This variable has only two values, ‘Democrat’ and ‘Republican’.\nRecall that our model is linear. Since we are using a linear model, the family we use will be gaussian.\nWe have also added -1 at the end of the equation, indicating that we do not want an intercept, which would otherwise be added by default.\n\nThe resulting output:\n\nfit_1\n\nstan_glm\n family:       gaussian [identity]\n formula:      age ~ party - 1\n observations: 115\n predictors:   2\n------\n                Median MAD_SD\npartyDemocrat   42.6    1.2  \npartyRepublican 41.2    2.7  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 12.3    0.8  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\npartyDemocrat corresponds to \\(\\beta_1\\), the average age of Democrats in the population. partyRepublican corresponds to \\(\\beta_2\\), the average age of Republicans in the population. Since we don’t really care about the posterior distribution for \\(\\sigma\\), we won’t discuss it here. Graphically:\n\n\nfit_1 |> \n  as_tibble() |> \n  select(-sigma) |> \n  mutate(Democrat = partyDemocrat, Republican = partyRepublican) |>\n  pivot_longer(cols = Democrat:Republican,\n               names_to = \"parameter\",\n               values_to = \"age\") |> \n  ggplot(aes(x = age, fill = parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Average Age\",\n         subtitle = \"More data allows for a more precise posterior for Democrats\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nThe unknown parameters \\(\\beta_1\\) (partyDemocrat) and \\(\\beta_2\\) (partyRepublican) are still unknown. We can never know the true average age of all Democrats in the population. But we can calculate a posterior probability distribution for each parameter. Comments:\n\nDemocrats seem slightly older than Republicans. That was true in the sample and so, almost (but not quite!) by definition, it will be true in our the posterior probability distributions.\nOur estimate for the average age of Democrats in the population is much more precise than that for Republicans because we have five times as many Democrats as Republicans in our sample. A central lesson from Chapter @ref(one-parameter) is that the more data you have related to a parameter, the narrower your posterior distribution will be.\nThere is a great deal of overlap between the two distributions. Would we be surprised if, in truth, the average age of Republicans in the population was greater than that for Democrats? Not really. We don’t have enough data to be sure either way.\nThe phrase “in the population” is doing a great deal of work because we have not said what, precisely, we mean by the “population.” Is it the set of people on those commuter platforms on those days in 2012 when the experiment was done? Is it the set of people on all platforms, including ones never visited? Is it the set of all Boston commuter? All Massachusetts residents? All US residents? Does it include people today, or can we only draw inferences for 2012? We will explore these questions in every model we create.\nThe parameters \\(\\beta_1\\) and \\(\\beta_2\\) can be interpreted in two ways. First, like all parameters, they are a part of the model. We need to estimate them. But, in many cases, we don’t really care what the value of the parameter is. The exact value of \\(\\sigma\\), for example, does not really matter. Second, some parameters have a substantive interpretaion, as with \\(\\beta_1\\) and \\(\\beta_2\\) being the average ages in the population. But this will often not be the case! Fortunately, with such models, we can use functions like posterior_epred() and posterior_predict() to answer our questions.\n\nConsider a table which shows a sample of 8 individuals.\n\n\n\n\n\n\n\n8 Observations from Trains Dataset\n    \n\nAge\n      Party\n      Fitted\n      Residual\n    \n\n\n31\nDemocrat\n42.59\n-11.593526\n\n\n34\nRepublican\n41.19\n-7.186270\n\n\n63\nDemocrat\n42.59\n20.406474\n\n\n45\nDemocrat\n42.59\n2.406474\n\n\n55\nDemocrat\n42.59\n12.406474\n\n\n37\nDemocrat\n42.59\n-5.593526\n\n\n53\nRepublican\n41.19\n11.813730\n\n\n36\nDemocrat\n42.59\n-6.593526\n\n\n\n\n\n\n\n \nThe fitted values are the same for all Republicans and for all Democrats, as the model produces one fitted value for each condition. This table shows how just a sample of 8 individuals captures a wide range of residuals, making it difficult to predict the age of a new individual. We can get a better picture of the unmodeled variation in our sample if we plot these three variables for all the individuals in our data.\nThe following three histograms show the actual outcomes, fitted values, and residuals of all people in trains:\n\noutcome <- ch8 |> \n  ggplot(aes(age)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Age\",\n         y = \"Count\") \n\nfitted <- tibble(age = fitted(fit_1)) |> \n  ggplot(aes(age)) +\n    geom_bar() +\n    labs(x = \"Fitted Values\",\n         y = NULL) +\n    scale_x_continuous(limits = c(20, 70)) \n\nres <- tibble(resids = residuals(fit_1)) |> \n  ggplot(aes(resids)) +\n    geom_histogram(bins = 100) +\n    labs(x = \"Residuals\",\n         y = NULL) \n  \n\noutcome + fitted + res +\n  plot_annotation(title = \"Decomposition of Height into Fitted Values and Residuals\")\n\n\n\n\nThe three plots are structured like our equation and table above. A value in the left plot is the sum of one value from the middle plot plus one from the right plot.\n\nThe actual age distribution looks like a normal distribution. It is centered around 43, and it has a standard deviation of about 12 years.\nThe middle plot for the fitted values shows only two adjacent spikes, which represent the estimates for Democrats and Republicans.\nSince the residuals plot represents the difference between the other two plots, its distribution looks like the first plot.\n\n\n8.2.3 Temperance\n\n\n\n\nTemperance\n\n\n\n\nRecall the first questions with which we began this section:\n\nWhat is the probability that, if a Democrat shows up at the train station, he will be over 50 years old?\n\nSo far we have only tried our model on people from our data set whose real age we already knew. This is helpful to understand the model, but our ultimate goal is to understand more about the real world, about people we don’t yet know much about. Temperance guides us to make meaningful predictions and to become aware of their known and unknown limitations.\nStart with a simple question, what are the chances that a random Democrat is over 50 years old? First, we create a tibble with the desired input for our model. In our case the tibble has a variable named “party” which contains a single observation with the value “Democrat”. This is a bit different than Chapter @ref(two-parameters).\n\nnew_obs <- tibble(party = \"Democrat\")\n\n\nUse posterior_predict() to create draws from the posterior for this scenario. Note that we have a new posterior distribution under consideration here. The unknown parameter, call it \\(D_{age}\\), is the age of a Democrat. This could be the age of a randomly selected Democrat from the population or of the next Democrat we meet or of the next Democrat we interview on the train platform. The definition of “population” determines the appropriate interpretation. Yet, regardless, \\(D_{age}\\) is an unknown parameter. But it is not one — like \\(\\beta_1\\), \\(\\beta_2\\), or \\(\\sigma\\) — for which we have already created a posterior probability distribution. That is why we need posterior_predict().\nposterior_predict() takes two arguments: the model for which the simulations should be run, and a tibble indicating for which and how many parameters we want to run these simulations. In this case, the model is the one from Courage and the tibble is the one we just created.\n\npp <- posterior_predict(fit_1, newdata = new_obs) |>\n    as_tibble() |>\n    mutate_all(as.numeric)\n\nhead(pp, 10)\n\n# A tibble: 10 × 1\n     `1`\n   <dbl>\n 1  27.3\n 2  47.7\n 3  37.7\n 4  41.4\n 5  57.8\n 6  29.0\n 7  60.2\n 8  38.7\n 9  32.2\n10  36.6\n\n\nWe might expect that we can use as_tibble() directly after the object returned posterior_predict(). Sadly, for obscure technical reasons, that won’t quite work. So, we need the incantation mutate_all(as.numeric) to make sure that the resulting tibble is well-behaved. This command ensures that every column in the tibble is a simple numeric vector, which is what we want.\nThe result are draws from the posterior distribution of the age of a Democrat. It is important to understand that this is not a concrete person from the trains dataset - the algorithm in posterior_predict() simply uses the existing data from trains to estimate this posterior distribution.\n\npp |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for a Random Democrat's Age\",\n         subtitle = \"Individual predictions are always more variable than expected values\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nOnce we have the posterior distribution, we can answer (almost) any reasonable question. In this case, the probability that the next Democrat will be over 50 is around 28%.\n\nsum(pp$`1` > 50) / nrow(pp)\n\n[1] 0.283\n\n\nRecall the second question:\n\nIn a group of three Democrats and three Republicans, what will the age difference be between the oldest Democrat and the youngest Republican?\n\nAs before we start by creating a tibble with the desired input. Note that the name of the column (“party”) and the observations (“Democrat”, “Republican”) must always be exactly as they are in the original data set. This tibble as well as our model can then be used as arguments for posterior_predict():\n\nnewobs <- tibble(party = c(\"Democrat\", \"Democrat\", \"Democrat\", \n                        \"Republican\", \"Republican\",\"Republican\"))\n\nposterior_predict(fit_1, newdata = newobs) |>\n    as_tibble() |>\n    mutate_all(as.numeric)\n\n# A tibble: 4,000 × 6\n     `1`   `2`   `3`   `4`   `5`   `6`\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  44.1  61.4  31.5  51.1  52.9  48.8\n 2  46.8  36.9  46.9  50.9  49.3  41.9\n 3  35.9  37.1  66.9  34.9  59.7  41.2\n 4  48.2  45.9  52.7  58.3  49.2  47.3\n 5  28.9  56.9  44.7  41.1  30.2  42.0\n 6  45.4  41.5  30.7  47.2  31.0  49.7\n 7  68.3  42.9  43.7  46.3  49.9  40.2\n 8  41.6  43.9  26.4  47.2  36.7  41.1\n 9  43.0  37.2  42.6  42.6  51.9  53.4\n10  30.7  45.1  63.9  29.1  57.7  36.0\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nWe have 6 columns: one for each person. posterior_predict() does not name the columns, but they are arranged in the same order in which we specified the persons in newobs: D, D, D, R, R, R. To determine the expected age difference, we add code which works with these posterior draws:\n\n\n\npp <- posterior_predict(fit_1, newdata = newobs) |>\n    as_tibble() |>\n    mutate_all(as.numeric) |> \n  \n    # We don't need to rename the columns, but doing so makes the subsequest\n    # code much easier to understand. We could just have worked with columns 1,\n    # 2, 3 and so on. Either way, the key is to ensure that you correctly map\n    # the covariates in newobs to the columns in the posterior_predict object.\n  \n    set_names(c(\"dem_1\", \"dem_2\", \"dem_3\", \n                \"rep_1\", \"rep_2\", \"rep_3\")) |> \n    rowwise() |> \n  \n  # Creating three new columns. The first two are the highest age among\n  # Democrats and the lowest age among Republicans, respectively. The third one\n  # is the difference between the first two.\n  \n  mutate(dems_oldest = max(c_across(dem_1:dem_3)),\n         reps_youngest = min(c_across(rep_1:rep_3)),\n         age_diff = dems_oldest - reps_youngest)\n\npp\n\n# A tibble: 4,000 × 9\n# Rowwise: \n   dem_1 dem_2 dem_3 rep_1 rep_2 rep_3 dems_oldest reps_youngest age_diff\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>       <dbl>         <dbl>    <dbl>\n 1  50.8  43.3  57.0  33.8  26.9  22.0        57.0          22.0    35.0 \n 2  54.9  50.6  23.6  52.1  38.9  42.3        54.9          38.9    16.1 \n 3  37.8  55.9  23.6  18.4  55.8  34.4        55.9          18.4    37.5 \n 4  56.9  51.1  38.4  48.7  38.1  57.6        56.9          38.1    18.8 \n 5  44.7  19.2  26.8  36.6  40.1  36.4        44.7          36.4     8.29\n 6  51.9  66.3  53.2  43.5  23.1  53.2        66.3          23.1    43.2 \n 7  32.0  37.4  54.4  29.2  43.8  55.3        54.4          29.2    25.2 \n 8  48.0  20.8  40.7  60.7  37.2  54.3        48.0          37.2    10.7 \n 9  33.3  24.1  32.9  50.8  78.0  25.1        33.3          25.1     8.18\n10  55.8  31.6  31.9  57.3  39.1  48.9        55.8          39.1    16.7 \n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe plotting code is similar to what we have seen before:\n\npp |>  \n  ggplot(aes(x = age_diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Age Difference\",\n         subtitle = \"Oldest of three Democrats compared to youngest of three Republicans\",\n         x = \"Age\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nIn words, we would expect the oldest Democrat to be about 22 years older than the youngest Republican, but we would not be too surprised if the oldest Democrat was actually younger than the youngest Republican in a group of 6.\n\n8.2.4 Addendum\nInstead of parameterizing this model without an intercept, we could have used one. In that case, the math would be:\n\\[ y_i = \\beta_0  + \\beta_1 democratic_i + \\epsilon_i\\]\nThe interpretations of the parameters are different from the prior model. \\(\\beta_0\\) is now the average age of Republicans. This is the same interpretation as \\(\\beta_1\\) in the original set up. \\(\\beta_1\\) is now the difference between the the average age of Republicans and that of Democrats.\nTo fit this model, we use the exact same code as before, except without the -1 in the formula argument.\n\nstan_glm(age ~ party, \n         data = trains, \n         seed = 98,\n         refresh = 0)\n\nstan_glm\n family:       gaussian [identity]\n formula:      age ~ party\n observations: 115\n predictors:   2\n------\n                Median MAD_SD\n(Intercept)     42.6    1.3  \npartyRepublican -1.5    3.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 12.3    0.8  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThe intercept, 42.6, is the same as the partyDemocrat estimate in the first model. The partyRepublican estimate, which was previously 41.0, is now -1.5, meaning it is the difference (allowing for rounding) between the average age of Democrats and Republicans.\nLittle else about the models will be different. They will have the same fitted values and residuals. posterior_predict() will generate the same posterior predictive probabilty distributions. Which parameterization we use does not matter much. But you should be able to interpret the meaning of the coefficients in both."
  },
  {
    "objectID": "08-three-parameters.html#att_end-treatment",
    "href": "08-three-parameters.html#att_end-treatment",
    "title": "8  Three Parameters",
    "section": "\n8.3 att_end ~ treatment",
    "text": "8.3 att_end ~ treatment\nAbove, we created a predictive model: with someone’s party affiliation, we can make a better guess as to what their age is than we could have in the absence of information about their party. There was nothing causal about that model. Changing someone’s party registration can not change their age. In this example, we build a causal model. Consider these two questions:\nWhat is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?\nWhat is the largest causal effect which still has a 1 in 10 chance of occurring?\nModels help us answer questions better than we could without those models, but only if we follow the Cardinal Virtues.\n\n8.3.1 Justice\n\n\n\n\nJustice\n\n\n\n\nThe four elements of Justice in a data science project remain the same: validity, stability, representativeness, and the model.\nFirst, we must consider the validity of the model, and to do so, let’s look at our treatment variable. What does it mean for somebody to receive the treatment? Let’s say somebody was running late on their way to the train in the morning. Well, this person may have heard the Spanish-speakers for a shorter amount of time than a commuter who arrived earlier. Similarly, some Spanish-speakers may have been speaking louder than others or some commuters may have been closer to the speakers than other commuters. Therefore, these commuters would hear the treatment of being exposed to Spanish-speakers less loudly than the other commuters. Additionally, the Spanish-speakers weren’t the same for every train station in our 2012 data and if we were to get data for now, we wouldn’t be able to hire the exact same Spanish-speakers. When considering validity with reference to treatments, what we must determine is if we can assume that the treatments of being exposed to Spanish-speakers that each commuter may experience a bit differently can be assumed to be the same.\nNext, we must consider the stability of our model for the relationship between att_end and treatment between 2012 and 2021. Is this relationship from 2012, four years before Donald Trump’s election as president, still the same? While we might not know for sure, we have to consider this in order to continue and make assumptions with our data. For our purposes, we will consider the relationship to be stable. Even though we know that there may have been some changes, we will consider the model to be the same in both years.\nLet’s now once again consider whether the data from 2012 train commuters around Boston is representative of 2012 train commuters in the US. In our last model, we discussed the issue about how Boston may be different from other cities and therefore not representative of the US, and we will now consider the issue of random sampling that may lead to representativeness issues.\nLet’s say that even though Boston is different from other US cities, we considered Boston to be perfectly representative of the US. Great, but this 2012 data could still not be representative. This is because there could be bias within those who are chosen to give the survey, in that the commuters who are approached and receive the surveys may not be random or representative. What if the individuals giving out the surveys were younger and also tended to choose people to approach with a survey that were similar in age to them? A scenario like this could end up overestimating younger train commuters in the population, which could influence our answers to any of our questions. Specifically, when considering the relationship between att_end and treatment, this could influence the results of the model because younger individuals may have similar attitudes on immigration.\n\n\nNow, let’s determine whether our DGM will be linear or logistic. Since our outcome variable att_end is a continuous variable since it has a range of possible values, we will use a linear model.\nThe math for this model is exactly the same as the math for the predictive model in the first part of this chapter, although we change the notation a bit for clarity.\n\\[ y_i = \\beta_1 treatment_i + \\beta_2 control_i + \\epsilon_i\\]\nwhere \n\\[treatment_i, control_i \\in \\{0,1\\}\\] \\[treatment_i +  control_i = 1\\] \n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\n\nNothing has changed, except for the meaning of the data items and the interpretations of the parameters.\nOn the left-hand side we still have the outcome, \\(y_i\\), however in this case, this is a person’s attitude toward immigration after the experiment is complete. \\(y_i\\) takes on integer values between 3 and 15 inclusive.\nOn the right-hand side, the part contained in the model will consist of the terms \\(\\beta_1 treatment_i\\) and \\(\\beta_2 control_i\\). These two terms stand for Treated and Control and as before, each term consists of a parameter and a data point. \\(\\beta_1\\) is the average attitude toward immigration for treated individuals — those exposed to Spanish-speakers — in the population. \\(\\beta_2\\) is the average attitude toward immigration for control individuals — those not exposed to Spanish-speakers — in the population.These are both our parameters. The \\(x\\)’s are our explanatory variables and take the values 1 or 0. If person \\(i\\) is Treated, \\(treatment_i = 1\\) and \\(control_i = 0\\). If person \\(i\\) is Control, \\(treatment_i = 0\\) and \\(control_i = 1\\). In other words, these are binary variables and are mutually exclusive – if you are Treated, you cannot also be Control.\nThe last part, \\(\\epsilon_i\\) (“epsilon”), represents the part that is not explained by our model and is called the error term. It is simply the difference between the outcome and our model predictions. In our particular case, this includes all factors that have an influence on someone’s attitude toward immigration but are not explained by treatment status. We assume that this error follows a normal distribution with an expected value of 0.\n\nNote that the formula applies to everyone in the population, not just the 115 people for whom we have data. The index \\(i\\) does not just go from 1 through 115. It goes from 1 through \\(N\\), where \\(N\\) is the number of individuals in the population. Conceptually, everyone has an att_end under treatment and under control.\n\n\nThe small \\(i\\)’s are an index for the data set. It is equivalent to the “ID” column in our Preceptor Table and simply states that the outcome for person \\(i\\) is explained by the predictor variables (\\(treatment\\) and \\(control\\)) for person \\(i\\), along with an error term.\n\n8.3.2 Courage\n\n\n\n\nCourage\n\n\n\n\nWith Justice satisfied, we gather our Courage and fit the model. Note that, except for the change in variable names, the code is exactly the same as it was above, in our predictive model for age. Predictive models and causal models use the same math and the same code. The differences, and they are very important, lie in the interpretation of the results, not in their creation.\n\nfit_2 <- stan_glm(att_end ~ treatment - 1, \n                      data = trains, \n                      seed = 45,\n                      refresh = 0)\n\nfit_2\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ treatment - 1\n observations: 115\n predictors:   2\n------\n                 Median MAD_SD\ntreatmentTreated 10.0    0.4  \ntreatmentControl  8.5    0.3  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.8    0.2   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nNote that once again, since we are using a linear model, we set the family argument to “gaussian”.\ntreatmentTreated corresponds to \\(\\beta_1\\). As always, R has, behind the scenes, estimated the entire posterior probability distribution for \\(\\beta_1\\). We will graph that distribution in the next section. But the basic print method for these objects can’t show the entire distribution, so it gives us summary numbers: the median and the MAD SD. Speaking roughly, we would expect about 95% of the values in the posterior to be within two MAD SD’s of the median. In other words, we are 95% confident that the true, but unknowable, average attitude toward immigration among the Treated in the population to be between 9.2 and 10.8.\ntreatmentControl corresponds to \\(\\beta_2\\). The same analysis applies. We are about 95% confident that the true value for the average attitude toward immigration for Control in the population is between 7.9 and 9.1.\nUp until now, we have used the Bayesian interpretation of “confidence interval.” This is also the intuitive meaning which, outside of academia, is almost universal. There is a truth out there. We don’t know, and sometimes can’t know, the truth. A confidence interval, and its associated confidence level, tells us how likely the truth is to lie within a specific range. If your boss asks you for a confidence interval, she almost certainly is using this interpretation.\nBut, in contemporary academic research, the phrase “confidence interval” is usually given a “Frequentist” interpretation. (The biggest divide in statistics is between Bayesians and Frequentist interpretations. The Frequentist approach, also known as “Classical” statistics, has been dominant for 100 years. Its power is fading, which is why this textbook uses the Bayesian approach.) For a Frequentist, a 95% confidence interval means that, if we were to apply the procedure we used in an infinite number of future situations like this, we would expect the true value to fall within the calculated confidence intervals 95% of the time. In academia, a distinction is sometimes made between confidence intervals (which use the Frequentist interpretation) and credible intervals (which use the Bayesian interpretation). We won’t worry about that difference in this Primer.\nLet’s look at the full posteriors for both \\(\\beta_1\\) and \\(\\beta_2\\).\n\n\nfit_2 |> \n  as_tibble() |> \n  select(-sigma) |> \n  pivot_longer(cols = treatmentTreated:treatmentControl,\n               names_to = \"Parameter\",\n               values_to = \"attitude\") |> \n  ggplot(aes(x = attitude, fill = Parameter)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   alpha = 0.5, \n                   bins = 100, \n                   position = \"identity\") +\n    labs(title = \"Posterior for Expected Attitude Toward Immigration\",\n         subtitle = \"Treated individuals are more conservative\",\n         x = \"Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) + \n    theme_classic()\n\n\n\n\nIt appears that the affect of the treatment is to change people’s attitudes to be more conservative about immigration issues. Which is somewhat surprising!\nWe can decompose the the dependent variable, att_end into two parts: the fitted values and the residuals. There are only two possible fitted values, one for the Treated and one for the Control. The residuals, as always, are simply the difference between the outcomes and the fitted values.\n\n\n\n\n\nThe smaller the spread of the residuals, the better a job the model is doing of explaining the outcomes.\n\n\n8.3.3 Temperance\n\n\n\n\nTemperance\n\n\n\n\nRecall the first question with which we began this section:\n\nWhat is the average treatment effect, of exposing people to Spanish-speakers, on their attitudes toward immigration?\n\nChapter @ref(rubin-causal-model) defined the average treatment effect. One simple estimator of the average treatment effect is the difference between \\(\\beta_1\\) and \\(\\beta_2\\). After all, the definition of \\(\\beta_1\\) is the average attitude toward immigration, of the population, for anyone, under exposure to the treatment. So, \\(\\beta_1 - \\beta_2\\) is the average treatment effect for the population, roughly 1.5. However, estimating the posterior probability distribution for this parameter is tricky, unless we make use of the posterior distributions of \\(\\beta_1\\) and \\(\\beta_2\\). With that information, the problem is simple:\n\nfit_2 |> \n  as_tibble() |> \n  mutate(ate = treatmentTreated - treatmentControl) |> \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nCould the true value of the average treatment effect be as much as 2 or as little as 1? Of course! The most likely value is around 1.5, but the variation in the data and the smallness of our sample cause the estimate to be imprecise. However, it is quite unlikely that the true average treatment effect is below zero.\n\nWe can use posterior_epred() to answer this question. Create a tibble and use it as we have done before:\n\nnewobs <- tibble(treatment = c(\"Treated\", \"Control\"))\n\npe <- posterior_epred(fit_2, newobs) |> \n    as_tibble() |> \n    mutate(ate = `1` - `2`)\n\npe\n\n# A tibble: 4,000 × 3\n     `1`   `2`   ate\n   <dbl> <dbl> <dbl>\n 1 10.0   8.56 1.47 \n 2  9.94  8.33 1.61 \n 3 10.0   8.48 1.56 \n 4 10.6   8.28 2.36 \n 5 10.6   8.29 2.32 \n 6  9.26  8.81 0.449\n 7 10.4   8.42 1.96 \n 8 10.5   8.28 2.26 \n 9 10.8   8.51 2.25 \n10 10.3   8.37 1.88 \n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe posterior probability distribution created with posterior_epred() is the same as the one produced by manipulating the parameters directly.\n\npe |> \n  ggplot(aes(x = ate)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Treatment Effect\",\n         subtitle = \"Exposure to Spanish-speakers shifts immigration attitudes rightward\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\n\nOur second question:\n\nWhat is the largest effect size which still has a 1 in 10 chance of occurring?\n\nCreate a tibble which we can pass to posterior_predict(). The variables in the tibble which will be passed in as newdata. Fortunately, the tibble we created above is just what we need for this question also.\nConsider the result of posterior_predict() for two people, one treated and one control. Take the difference.\n\n\npp <- posterior_predict(fit_2, \n                        newdata = newobs) |>\n    as_tibble() |>\n    mutate_all(as.numeric) |> \n    mutate(te = `1` - `2`)\n  \npp\n\n# A tibble: 4,000 × 3\n     `1`   `2`     te\n   <dbl> <dbl>  <dbl>\n 1  9.81  9.27  0.537\n 2  3.09  9.27 -6.18 \n 3 15.9  12.4   3.54 \n 4  6.99  5.84  1.16 \n 5 10.6   9.48  1.15 \n 6 11.0   4.51  6.54 \n 7 12.3   8.20  4.14 \n 8 11.7   7.51  4.20 \n 9 11.2   5.00  6.24 \n10 10.6   9.77  0.858\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCreate a graphic:\n\npp |> \n  ggplot(aes(x = te)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Treatment Effect for One Person\",\n         subtitle = \"Causal effects are more variable for indvduals\",\n         x = \"Difference in Attitude\",\n         y = \"Probability\") +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()\n\n\n\n\nIn this case, we are looking at the distribution of the treatment effect for a single individual. This is very different than the average treatment effect. In particular, it is much more variable. We are looking at one row in the Preceptor Table. For a single individual, att_end can be anywhere from 3 to 15, both under treatment and under control. The causal effect — the difference between the two potential outcomes can, in theory, be anywhere from -12 to +12. Such extreme values are rare, but not impossible.\nThe question, however, was interested in the value at the 90th percentile.\n\nquantile(pp$te, prob = 0.9)\n\n     90% \n6.610226 \n\n\nWe would not expect a treatment effect of this magnitude to be common, but, at the same time, effects this big and bigger will occur about 10% of the time."
  },
  {
    "objectID": "08-three-parameters.html#income-age",
    "href": "08-three-parameters.html#income-age",
    "title": "8  Three Parameters",
    "section": "\n8.4 income ~ age",
    "text": "8.4 income ~ age\nSo far, we have only created models in which the predictor variable is discrete, with two possible values. party is either “Democrat” or “Republican”. treatment is either “Treated” or “Control”. Often times, however, the predictor variable will be continuous. Fortunately, the exact same approach works in this case. Consider:\nWhat would you expect the income to be for a 40-year old?\n\n8.4.1 Justice\nOnce again, in Justice, we must consider validity, stability, representativeness, and the data generating mechanism.\nLet’s look at the validity of age and income. Recall that validity refers to whether or not we can consider the columns age and income to have the same meaning in our data set of 2012 Boston train commuters and in our Preceptor Table. While age doesn’t really change meaning over time, income can be impacted by inflation. After all, $100,000 in 2012 doesn’t have the same worth as $100,000 now due to inflation and now would have less purchasing power. This would result in income being underestimated within our model. However, since there hasn’t been drastic inflation that dramatically changed the buying power, we will consider income to be valid. If there had been like 300% inflation, however, our conclusion would probably be different.\nNow, let’s consider the stability of our model and if we believe that our relationship between age and income has changed between 2012 and now. Once again, let’s consider inflation and how that could impact income. If incomes were to increase at the rate of inflation, then the income distribution would be different than that of 2012. However, wages don’t tend to change as quickly as inflation does, so they likely did not change significantly and we can consider this model to be stable.\nNext, let’s consider another issue that we may have with representativeness. What if we were now to assume that Boston train commuters are perfectly representative of US train commuters AND those who were approached to respond to the survey were also random? Even if this were true, we could still not assume representativeness because those who actually complete and submit the survey are not random. Instead of having more young people chosen to respond by those handing out surveys like we discussed in our last model, what if were to assume that when the surveys are handed out randomly, younger people tended to fill out and submit them more than those who are older? Well, this would still skew the age distribution and overestimate younger people in the population, and if younger people also tend to have a lower income than older people, this could also alter our answers to our current questions.\nIf we had reason to believe this is true, one way that we could fix this issue of representativeness is to alter our population to be train commuters in the US who would respond to the survey. In doing so, our population would then accommodate for the skewed age distribution under the assumption that younger individuals tend to respond to surveys at higher rates than older people.\n\n\n\n\nJustice\n\n\n\n\nThe mathematics for a continuous predictor is unchanged from the intercept-including example we explored in Section @ref(addendum):\n\\[y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]\nWhen comparing two people (persons 1 and 2), the first one year older than the second, \\(\\beta_1\\) is the expected difference in their incomes. The algebra is simple. Start with the two individuals.\n\\[y_1 = \\beta_0  + \\beta_1 age_1\\]\n\\[y_2 = \\beta_0  + \\beta_1 age_2\\]\nWe want the difference between them, so we subtract the second from the first, performing that subtraction on both sides of the equals sign.\n\\[y_1 - y_2 = \\beta_0  + \\beta_1 age_1 - \\beta_0 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 age_1 - \\beta_1 age_2\\\\\ny_1 - y_2 = \\beta_1 (age_1 - age_2)\\]\nSo, if person 1 is one year older than person 2, we have:\n$$y_1 - y_2 = _1 (age_1 - age_2)\\\ny_1 - y_2 = _1 (1)\\ y_1 - y_2 = _1$$\nThe algebra demonstrates that \\(\\beta_1\\) is the same for all ages. The difference in expected income between two people aged 23 and 24 is the same as the difference between two people aged 80 and 81. Is that plausible? Maybe. The algebra does not lie. When we create a model like this, this is the assumption we are making.\nNote how careful we are not to imply that increasing age by one year “causes” an increase in income. That is nonsense! No causation without manipulation. Since it is impossible to change someone’s age, there is only one potential outcome. With only one potential outcome, a causal effect is not defined.\n\n8.4.2 Courage\n\n\n\n\nCourage\n\n\n\n\nThe use of stan_glm() is the same as usual.\n\nfit_3 <- stan_glm(income ~ age, \n                  data = trains, \n                  seed = 28,\n                  refresh = 0)\n\nprint(fit_3, details = FALSE)\n\nstan_glm\n family:       gaussian [identity]\n formula:      income ~ age\n observations: 115\n predictors:   2\n------\n            Median   MAD_SD  \n(Intercept) 104243.6  24038.6\nage            898.6    554.8\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74057.2  4892.1\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWhen comparing two individuals, one 30 years old and one 40, we expect the older to earn about $9,000 more. But we are far from certain: the 95% confidence interval ranges from -$3,000 to $20,000.\nThe above is a good summary of the models.\n\nIt is brief! No one wants to listen to too much of your prattle. One sentence gives a number of interest. The second sentence provides a confidence interval.\nIt rounds appropriately. No one wants to hear a bunch of decimals. Use sensible units.\nIt does not just blindly repeat numbers in the printed display. A one year difference in age, which is associated with a $900 difference in income, is awkward. (We think.) A decade comparison is more sensible.\n“When comparing” is a great phrase to start the summary of any non-causal model. Avoid language like “associated with” or “leads to” or “implies” or anything which even hints at a causal claim.\n\nConsider our usual decomposition of the outcome into two parts: the model and the error term.\n\n\n\n\n\nThere are scores of different fitted values. Indeed, there are a greater number of different fitted values than there are different outcome values! This is often true for models which have continuous predictor variables, we have here with age.\n\n\n\n8.4.3 Temperance\n\n\n\n\nTemperance\n\n\n\n\nRecall our question:\nWhat would you expect the income to be for a random 40 year old?\nGiven that we are looking for an expected value, we use posterior_epred().\n\nnewobs <- tibble(age = 40)\n\npe <- posterior_epred(fit_3, newdata = newobs) |> \n  as_tibble() \n\npe\n\n# A tibble: 4,000 × 1\n       `1`\n     <dbl>\n 1 130128.\n 2 143647.\n 3 134193.\n 4 145239.\n 5 140869.\n 6 144703.\n 7 145607.\n 8 129210.\n 9 145597.\n10 144985.\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nPlotting is the same as always.\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Expected Income\",\n         subtitle = \"A 40-years old commuter earns around $140,000\",\n         x = \"Income\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::dollar_format()) +\n    scale_y_continuous(labels = scales::percent_format()) +\n    theme_classic()"
  },
  {
    "objectID": "08-three-parameters.html#liberal-income",
    "href": "08-three-parameters.html#liberal-income",
    "title": "8  Three Parameters",
    "section": "\n8.5 liberal ~ income",
    "text": "8.5 liberal ~ income\nSo far in this chapter, we have only considered continuous outcome variables. age, att_end and income all take on a variety of values. None of them are, truly, continuous, of course. age is only reported as an integer value. att_end can only, by definition, take on 13 distinct values. However, from a modeling perspective, what matters is that they have more than 2 possible values.\nliberal, however, only takes on two values: TRUE and FALSE. In order to model it, we must use the binomial family. We begin, as always, with some questions:\nAmong all people who have an income $100,000, what proportion are liberal?\nAssume we have a group of eight people, two of whom make $100,000, two $200,000, two $300,000 and two $400,000. How many will be liberal?\n\n8.5.1 Justice\nLet’s now consider Justice for our relationship between liberal and income.\nFirst, let’s look at validity, especially pertaining to liberal. As we know, this column has the values “Liberal” and “Not Liberal” to convey the political ideology of the train commuters. What we must determine if the column liberal and therefore the meaning of being liberal is the same in Boston in 2012 and Boston in 2021. If we can determine these to be the same, then we can assume validity, and in this case, because the core beliefs of being liberal have not changed very much between 2012 and 2021, we can determine that the data is valid.\nNow, let’s consider stability. To do so, we must look at the relationship between liberal and income and determine whether or not we believe that this relationship has changed between 2012 and 2021. With our knowledge of the world, do we have any reason to believe that this has changed? What if between 2012 and 2021, income increased for those who are liberal? Well, then our model for the the relationship between income and liberal would have changed over the years, and therefore so would the model. However, since with our knowledge of the world we have no reason to believe that something of the sorts happened to affect this relationship, we can consider our model to be stable.\nIn our past three models, we have considered 3 possible issues that we could have with the representativeness of our model, such as the difference between Boston and other cities, problems with random sampling, and bias in those who respond, and we will now consider how in this survey there were surveys given before and after treatment, and some people may have filled out only one of the surveys. People who only filled out one of the surveys could affect the representativeness of the data because they could not be included in the data and if those who only filled out one survey tended to be liberal, then this would affect our data because it would underestimate the amount of liberals in the survey. This is something we must consider when looking at representativeness, since we could otherwise not determine if this data from train commuters in Boston in 2012 is representative enough of train commuters in the US now to continue using our data.\nLet’s consider whether this model is linear or logistic. Unlike our previous models this chapter, the outcome variable, liberal, for this model, only has two options, “Liberal” or “Not Liberal”. Therefore, this will be logistic because there are only 2 possible outcomes and the outcome variable isn’t continuous.\nRecall the discussion in Section @ref(zero-one-outcomes) about the logistic regression model which we use whenever the outcome or dependent variable is binary/logical. The math is there, if you care about math. We don’t, at least not too much. Reminder:\n\\[p(\\text{Liberal}_i = \\text{TRUE}) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{income}_i)}\\]\nThis model only has two parameters, \\(\\beta_0\\) and \\(\\beta_1\\). But these parameters do not have simple interpretations, unlike the parameters in a linear (or gaussian) model.\nRecall the fundamental structure of all data science problems:\n\\[\\text{outcome} = \\text{model} + \\text{what is not in the model}\\]\nThe exact mathematics of the model — the parameters, their interpretations — are all just dross in the foundry of our inferences: unavoidable but not worth too much of our time.\nEven if the math is ignorable, the causal versus predictive nature of the model is not. Is this a causal model or a predictive model? It depends! It could be causal if you assume that we can manipulate someone’s income, if, that is, there are at least two potential outcomes: person \\(i\\)’s liberal status if she makes X dollars and person \\(i\\)’s liberal status if she makes Y dollars. Remember: No causation without manipulation. The definition of a causal effect is the difference between two potential outcomes. If you only have one outcome, then your model can not be causal.\nIn many circumstances, we don’t really care if a model is causal or not. We might only want to forecast/predict/explain the outcome variable. In that case, whether we can interpret the influence of a variable as causal is irrelevant to our use of that variable.\n\n8.5.2 Courage\nFitting a logistic model is easy. We use all the same arguments as usual, but with family = binomial added.\n\nfit_4 <- stan_glm(data = ch8,\n                  formula = liberal ~ income,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 365)\n\nHaving fit the model, we can look at a printed summary. Note the use of the digits argument to display more digits in the printout.\n\nprint(fit_4, digits = 6)\n\nstan_glm\n family:       binomial [logit]\n formula:      liberal ~ income\n observations: 115\n predictors:   2\n------\n            Median    MAD_SD   \n(Intercept)  0.562837  0.426033\nincome      -0.000006  0.000003\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nFitted models tell us about the posterior distributions for the parameters in the formula which defines the model we have estimated. We are assuming that the model is true. And, as discussed in Chapter @ref(one-parameter), that assumption is always false! Our model is never a perfectly accurate representation of reality. But, if it were perfect, then the posterior distributions which we have created for \\(\\beta_0\\), \\(\\beta_1\\), and so on would be perfect as well.\nWhen working with a linear model, we will often interpret the meaning of the parameters, as we have already done in the first three sections of this chapter. Such interpretations are much harder with logistic models because the math is much less convenient. So, we won’t even bother to try to understand the meaning of these parameters. However, we can note that \\(\\beta_1\\) is negative, suggesting that people with higher incomes are less likely to be liberal.\n\n8.5.3 Temperance\nAmong all people who have an income $100,000, what proportion are liberal?\nAlthough our model is now logistic, all the steps in answering a question like this are the same as with a linear/guassian model.\n\nnewobs <- tibble(income = 100000)\n\npe <- posterior_epred(fit_4, \n                      newdata = newobs) |> \n  as_tibble()\n\npe is a tibble with a single vector. That vector is 4,000 draws from the posterior distribution of proportion of people, among those who make $100,000, who are liberal. The population proportion is the same thing as the probability for any single individual.\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Proportion Liberal Among $100,000 Earners\",\n         subtitle = \"The population proportion is the same as the probability for any individual\",\n         x = \"Income\",\n         y = \"Probability of Being Liberal\") +\n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\nAssume we have a group of eight people, two of whom make $100,000, two $200,000, two $300,000 and two $400,000. How many will be liberal?\nBecause we are trying to predict the outcome for a small number of units, we use posterior_predict(). The more complex the questions we ask, the more care we need to devote to making the newobs tibble. We use the same rowwise() and c_across() tricks as earlier in the chapter.\n\nnewobs <- tibble(income = c(rep(100000, 2),\n                            rep(200000, 2),\n                            rep(300000, 2),\n                            rep(400000, 2)))\n                 \n\npp <- posterior_predict(fit_4, \n                        newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) |> \n  rowwise() |> \n  mutate(total = sum(c_across()))\n\npp\n\n# A tibble: 4,000 × 9\n# Rowwise: \n     `1`   `2`   `3`   `4`   `5`   `6`   `7`   `8` total\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     1     1     1     1     0     0     0     0     4\n 2     1     1     1     0     0     1     0     0     4\n 3     1     0     1     1     1     1     0     0     5\n 4     1     0     0     1     0     1     1     0     4\n 5     0     0     0     0     0     1     0     0     1\n 6     1     0     1     0     1     0     0     0     3\n 7     1     0     0     1     0     0     0     0     2\n 8     0     1     1     0     0     0     0     0     2\n 9     1     1     0     0     0     0     0     0     2\n10     1     0     0     0     0     0     0     0     1\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nStudy the pp tibble. Understand its component parts. The first column, for example, is 4,000 draws from the posterior distribution for the liberal status of a random person with an income of $100,000. Note how all the draws are zeroes or ones. That is very different from the draws we have seen before! But it also makes sense. We are making a prediction about a binary variable, a variable which only have two possible values: zero or one. So, any (reasonable!) predictions will only be zero or one.\nThe second column is the same thing as the first column. Both are 4,000 draws from the posterior distribution for the liberal status of a random person with an income of $100,000. Yet they also have different values. They are both the same thing and different things, in the same way that rnorm(10) and rnorm(10) are the same thing — both are 10 draws from the standard normal distribution — and different things in that the values vary.\nThe third and fourth columns are different from the first two columns. They are both 4,000 draws from the posterior distribution for the liberal status of a random person with an income of $200,000. And so on for later columns. We can answer very difficult questions by putting together simple building blocks, each of them a set of draws from a posterior distribution. Recall the discussion in Section @ref(distributions).\nThe total column is simply the sum of the first eight columns. Having created the building blocks with 8 columns of draws from four different posterior distributions, we can switch our focus to each row. Consider row 2. It has a vector of 8 numbers: 1 1 1 0 0 1 0 0. We can treat that vector as a unit of analysis. This is what might happen with our 8 people. The first three might be liberal, the fourth not liberal and so on. This row is just one example of what might happen, one draw from the posterior distribution of possible outcomes for groups of eight people with these incomes.\nWe can simplify this draw by taking the sum, or doing anything else which might answer the question with which we are confronted. Posterior distributions are as flexible as individual numbers. We can, more or less, just use algebra to work with them.\nGraphically we have:\n\npp |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100)  +\n    labs(title = \"Posterior for Number of Liberals in Group with Varied Incomes\",\n         subtitle = \"Two is the most likely number, but values from 0 to 5 are plausible\",\n         x = \"Number of Liberals\",\n         y = \"Probability\") +\n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nAs always, there is some truth. If, tomorrow, we were to meet 8 new people, with the specified incomes, a certain number of them would be liberal. If we had the ideal Preceptor Table, we could just look up that number. No data science required. Alas, we don’t know the truth. The bets we can do is to create a posterior distribution for that unknown value, as we have done here. We then need to translate that posterior into English — “The most likely number of liberals is 2 or 3, but a total as low as zero or as high as 5 is also plausible. Having 6 liberals would be really surprising. Having 7 or 8 is almost impossible.”\nAre these two posterior probability distributions perfect? No! This is the central message of the virtue of Temperance. We must demonstrate our humility when we use our models. Recall the distinction between the unknown true distribution and the estimated distribution. The first is the posterior distribution we would create of we understood every detail of the process and could accurately model it. We would still not know the true unknown number, but our posterior distribution for that number would be perfect. Yet, our model is never perfect. We are making all sorts of assumptions behind the scenes. Some of those assumptions are plausible. Others are less so. Either way, the estimated distribution is what we have graphed above.\nThe central lesson of Temperance is: Don’t confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Recognize the unavoidable imperfections in the process. You can still use your estimated posterior — what choice do you have? — but be cautious and humble in doing so. The more that you suspect that your estimated posterior differs from the true posterior, the more humble and cautious you should be."
  },
  {
    "objectID": "08-three-parameters.html#summary",
    "href": "08-three-parameters.html#summary",
    "title": "8  Three Parameters",
    "section": "\n8.6 Summary",
    "text": "8.6 Summary\nIn this chapter, we explored relationships between different variables in the trains data set. We built three predictive models and one causal model.\nSimilar to previous chapters, our first task is to use Wisdom . We judge how relevant our data is to the questions we ask. Is it reasonable to consider the data we have (e.g., income and age data from Boston commuters in 2012) as being drawn from the same population as the data we want to have (e.g., income and age data from today for the entire US)? Probably?\nJustice is necessary to decide the best way to represent the models we make. A little math won’t kill you. We use Courage to translate our models into code. Our goal is to understand, generate posterior distributions for the parameters, and interpret their meaning. Temperance leads us to the final stage, using our models to answer our questions.\nKey commands:\n\nCreate a model with stan_glm().\nUse posterior_epred() to estimate expected values. The e in epred stands for expected.\nUse posterior_predict() to make forecasts for individuals. The variable in predictions is always greater than the variability in expectations because predictions can’t pretend that \\(\\epsilon_i\\) is zero.\n\nOnce we have draws from a posterior distribution for our outcome variable — whether that be an expectation or a prediction — we can manipulate those draws to answer our question.\nRemember:\n\nAlways explore your data.\nPredictive models care little about causality.\nPredictive models and causal models use the same math and the same code.\n“When comparing” is a great phrase to start the summary of any non-causal model.\nDon’t confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Be humble and cautious in your use of the posterior."
  },
  {
    "objectID": "09-four-parameters.html",
    "href": "09-four-parameters.html",
    "title": "9  Four Parameters",
    "section": "",
    "text": "In our haste to make progress — to get all the way through the process of building, interpreting and using models — we have given short shrift to some of the messy details of model building and evaluation. This chapter fills in those lacunae. We will also introduce models with four parameters, including parallel slopes models."
  },
  {
    "objectID": "09-four-parameters.html#transforming-variables",
    "href": "09-four-parameters.html#transforming-variables",
    "title": "9  Four Parameters",
    "section": "\n9.1 Transforming variables",
    "text": "9.1 Transforming variables\n\nIt is often convenient to transform a predictor variable so that our model makes more sense.\n\n9.1.1 Centering\nRecall our model of income as a function of age.\n\\[ y_i = \\beta_0  + \\beta_1 age_i + \\epsilon_i\\]\nWe fit this using the trains data from primer.data. We will also be using a new package, broom.mixed, which allows us to tidy regression data for plotting.\n\n\n\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(broom.mixed)\nlibrary(patchwork)\n\n\nfit_1 <- stan_glm(formula = income ~ age, \n         data = trains, \n         refresh = 0,\n         seed = 9)\n\nprint(fit_1, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 102936.6  25293.1\nage            916.9    573.0\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74197.7  4891.4\n\n\nThere is nothing wrong with this model. Yet the interpretation of \\(\\beta_0\\), the intercept in the regression, is awkward. It represents the average income for people of age zero. That is useless! There are no people of zero age in our data. And, even if there were, it would be weird to think about such people taking the commuter train into Boston and filling out our survey forms.\nIt is easy, however, to transform age into a variable which makes the intercept more meaningful. Consider a new variable, c_age, which is age minus the average age in the sample. Using this centered version of age does not change the predictions or residuals in the model, but it does make the intercept easier to interpret.\n\ntrains_2 <- trains |> \n  mutate(c_age = age - mean(age))\n\nfit_1_c <- stan_glm(formula = income ~ c_age, \n                    data = trains_2, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_c, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 141657.1   6593.3\nc_age          871.9    616.4\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74035.8  4990.7\n\n\nThe intercept, 141,657, is the expected income for someone with c_age = 0, i.e., someone of an average age in the data, which is around 42.\n\n9.1.2 Scaling\nCentering — changing a variable via addition/subtraction — often makes the intercept easier to interpret. Scaling — changing a variable via multiplication/division — often makes it easier to interpret coefficients. The most common scaling method is to divide the variable by its standard deviation.\n\ntrains_3 <- trains |> \n  mutate(s_age = age / sd(age))\n\nfit_1_s <- stan_glm(formula = income ~ s_age, \n                    data = trains_3, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_s, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 103263.6  24917.6\ns_age        11005.1   7098.3\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74259.9  4993.8\n\n\ns_age is age scaled by its own standard deviation. A change in one unit of s_age is the same as a change in one standard deviation of the age, which is about 12. The interpretation of \\(\\beta_1\\) is now:\nWhen comparing two people, one about 1 standard deviation worth of years older than the other, we expect the older person to earn about 11,000 dollars more.\nBut, because we scaled without centering, the intercept is now back to the (nonsensical) meaning of the expected income for people of age 0.\n\n9.1.3 z-scores\nThe most common transformation applies both centering and scaling. The base R function scale() subtracts the mean and divides by the standard deviation. A variable so transformed is a “z-score,” meaning a variable with a mean of zero and a standard deviation of one. Using z-scores makes interpretation easier, especially when we seek to compare the importance of different predictors.\n\ntrains_4 <- trains |> \n  mutate(z_age = scale(age))\n\nfit_1_z <- stan_glm(formula = income ~ z_age, \n                    data = trains_4, \n                    refresh = 0,\n                    seed = 9)\n\nprint(fit_1_z, detail = FALSE)\n\n            Median   MAD_SD  \n(Intercept) 142137.6   6761.4\nz_age        10735.2   6961.9\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 74269.4  4711.7\n\n\nThe two parameters are easy to interpret after this transformation.\nThe expected income of someone of average age, which is about 42 in this study, is about 142,000 dollars.\nWhen comparing two individuals who differ in age by one standard deviation, which is about 12 years in this study, the older person is expected to earn about 11,000 dollars more than the younger.\nNote that, when using z-scores, we would often phrase this comparison in terms of “sigmas.” One person is “one sigma” older than another person means that they are one standard deviation older. This is simple enough, once you get used to it, but also confusing since we are already using the word “sigma” to mean \\(\\sigma\\), the standard deviation of \\(\\epsilon_i\\). Alas, language is something we deal with rather than control. You will hear the same word “sigma” applied to both concepts, even in the same sentence. Determine meaning by context.\n\n9.1.4 Taking logs\nIt is often helpful to take the log of predictor variables, especially in cases in which their distribution is skewed. You should generally only take the log of variables for which all the values are strictly positive. The log of a negative number is not defined. Consider the number of registered voters (rv13) at each of the polling stations in kenya.\n\nx <- kenya |> \n  filter(rv13 > 0)\n\nrv_p <- x |> \n  ggplot(aes(rv13)) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Registered Voters\",\n         y = NULL) \n\nlog_rv_p <- x |> \n  ggplot(aes(log(rv13))) + \n    geom_histogram(bins = 100) +\n    labs(x = \"Log of Registered Voters\",\n         y = NULL) +\n    expand_limits(y = c(0, 175))\n\nrv_p + log_rv_p +\n  plot_annotation(title = 'Registered Votes In Kenya Communities',\n                  subtitle = \"Taking logs helps us deal with outliers\")\n\n\n\n\nMost experienced data scientists would use the log of rv13 rather than the raw value. Comments:\n\nWe do not know the “true” model. Who is to say that a model using the raw value is right or wrong?\nCheck whether or not this choice meaningfully affects the answer to your question. Much of the time, it won’t. That is, our inferences are often fairly “robust” to small changes in the model. If you get the same answer with rv13 as from log_rv13, then no one cares which you use.\nFollow the conventions in your field. If everyone does X, then you should probably do X, unless you have a good reason not to. If you do have such a reason, explain it prominently.\nMost professionals, when presented with data distributed like rv13, would take the log. Professionals hate (irrationally?) outliers. Any transformation which makes a distribution look more normal is generally considered a good idea.\n\nMany of these suggestions apply to every aspect of the modeling process.\n\n\n9.1.5 Adding transformed terms\nInstead of simply transforming variables, we can add more terms which are transformed versions of a variable. Consider the relation of height to age in nhanes. Let’s start by dropping the missing values.\n\nno_na_nhanes <- nhanes |> \n  select(height, age) |> \n  drop_na() \n\nFit and plot a simple linear model:\n\nnhanes_1 <- stan_glm(height ~ age,\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 47)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_1)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Children are shorter, but a linear fit is poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\nThat is not a very good model, obviously.\nAdding a quadratic term makes it better. (Note the need for I() in creating the squared term within the formula argument.)\n\nnhanes_2 <- stan_glm(height ~ age + I(age^2),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 33)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_2)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Quadratic fit is much better, but still poor\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\nStill, we have not made use of our background knowledge in creating these variables. We know that people don’t get any taller after age 18 or so. Let’s create variables which capture that break.\n\nnhanes_3 <- stan_glm(height ~ I(ifelse(age > 18, 18, age)),\n                     data = no_na_nhanes,\n                     refresh = 0,\n                     seed = 23)\n\nno_na_nhanes |> \n  ggplot(aes(x = age, y = height)) +\n    geom_point(alpha = 0.1) +\n    geom_line(aes(y = fitted(nhanes_3)), \n              color = \"red\") +\n    labs(title = \"Age and Height\",\n         subtitle = \"Domain knowledge makes for better models\",\n         x = \"Age\",\n         y = \"Height (cm)\",\n         caption = \"Data source: NHANES\")\n\n\n\n\nThe point is that we should not take the variables we receive as given. We are the captains of our souls. We transform variables as needed.\n\n\n\n9.1.6 Transforming the outcome variable\nTransforming predictor variables is uncontroversial. It does not matter much. Change most continuous predictor variables to \\(z\\)-scores and you won’t go far wrong. Or keep them in their original form, and take care with your interpretations. It’s all good.\nTransforming the outcome variable is a much more difficult question. Imagine that we seek to create a model which explains rv13 from the kenya tibble. Should we transform it?\n\nMaybe? There are no right answers. A model with rv13 as the outcome variable is different from a model with log(rv13) as the outcome. The two are not directly comparable.\nMuch of the same advice with regard to taking logs of predictor variables applies here as well.\n\nSee @roas\n\n9.1.7 Interpreting coefficients\n\nWhen we interpret coefficients, it is important to know the difference between across unit and within unit comparisons. When we compare across unit, meaning comparing Joe and George, we are not looking at a causal relationship. Within unit discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying one unit under two conditions. When we talk about two potential outcomes, we are discussing the same person or unit under two conditions.\nTo put this in terms of the Preceptor tables, a within unit comparison is looking at one row of data: the data for Joe under control and the data for Joe under treatment. We are comparing one unit, or (in this case) one person, to itself under two conditions. An across unit comparison is looking at multiple rows of data, with a focus on differences across columns. We are looking at differences without making any causal claims. We are predicting, but we are not implying causation.\nThe magnitude of the coefficients in linear models are relatively easy to understand. That is not true for logistic regressions. In that case, use the Divide-by-Four rule: Take a logistic regression coefficient (other than the constant term) and divide it by 4 to get an upper bound on the predictive difference corresponding to a unit difference in that variable. All this means is that, when evaluating if a predictor is helpful in a logistic regression only, divide the coefficient by four."
  },
  {
    "objectID": "09-four-parameters.html#selecting-variables",
    "href": "09-four-parameters.html#selecting-variables",
    "title": "9  Four Parameters",
    "section": "\n9.2 Selecting variables",
    "text": "9.2 Selecting variables\nHow do we decide which variables to include in a model? There is no one right answer to this question.\n\n9.2.1 General guidelines for selecting variables\nWhen deciding which variables to keep or discard in our models, our advice is to keep a variable X if any of the following circumstances apply:\n\nThe variable has a large and well-estimated coefficient. This means, roughly, that the 95% confidence interval excludes zero. “Large” can only be defined in the context of the specific model. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model.\nUnderlying theory/observation suggests that X has a meaningfully connection to the outcome variable.\nIf the variable has a small standard error relative to the size of the coefficient, it is general practice to include it in our model to improve predictions. The rule of thumb is to keep variables for which the estimated coefficient is more than two standard errors away from zero. Some of these variables won’t “matter” much to the model. That is, their coefficients, although well-estimated, are small enough that removing the variable from the model will not affect the model’s predictions very much.\nIf the standard error is large relative to the coefficient, i.e., if the magnitude of the coefficient is more than two standard errors from zero, and we find no other reason to include it in our model, we should probably remove the variable from your model.\n\n\nThe exception to this rule is if the variable is relevant to answering a question which we have. For example, if we want to know if the ending attitude toward immigration differs between men and women, we need to include gender in the model, even if its coefficient is small and closer to zero than two standard errors.\nIt is standard in your field to include X in such regressions.\nYour boss/client/reviewer/supervisor wants to include X.\n\nLet’s use the trains dataset to evaluate how helpful certain variables are to creating an effective model, based on the guidelines above.\n\n9.2.2 Variables in the trains dataset\nTo look at our recommendations in practice, let’s focus on the trains dataset. The variables in trains include gender, liberal, party, age, income, att_start, treatment, and att_end. Which variables would be best to include in a model?\n\n9.2.3 att_end ~ treatment + att_start\nFirst, let’s look at a model with a left hand variable, att_end, and two right side variables, treatment and att_start.\n\n\n\n\nfit_1_model <- stan_glm(att_end ~ treatment + att_start, \n                    data = trains, \n                    refresh = 0)\n\nfit_1_model\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ treatment + att_start\n observations: 115\n predictors:   3\n------\n                 Median MAD_SD\n(Intercept)       2.3    0.4  \ntreatmentControl -0.9    0.3  \natt_start         0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.3    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nHow do we decide which variables are useful? First, let’s interpret our coefficients.\n\nThe variable before the tilde, att_end, is our outcome.\nThe explanatory variables are treatment, which says whether a commuter relieved treatment or control conditions, and att_start, which measures attitude at the start of the study.\nThe 95% confidence interval for att_end is equal to the coefficient– 2– plus or minus two standard errors. This shows the estimate for the att_end where the commuters were under treatment and were not in the control group.\nThe variable treatmentControl represents the offset in att_end from the estimate for our (Intercept). This offset is for the group of people that were in the Control group. To find the estimated att_end for those in this group, you must add the median for treatmentControl to the (Intercept) median.\nThe variable att_start measures the expected difference in att_end for every one unit increase in att_start.\n\nThe causal effect of the variable treatmentControl is -1. This means that, compared with the predicted att_end for groups under treatment, those in the control group have a predicted attitude that is one entire point lower. As we can see, this is a large and well estimated coefficient. Recall that this means, roughly, that the 95% confidence interval excludes zero. To calculate the 95% confidence interval, we take the coefficient plus or minus two standard errors: -1.3 and -0.5. As we can see, the 95% confidence interval does exclude zero, suggesting that treatment is a worthy variable.\nIn addition to being meaningful, which is enough to justify inclusion in our model, this variable satisfies a number of other qualifications: - The variable has a small standard error. - The variable is considered an indicator variable, which separates two groups of significance (treatment and control) that we would like to study.\nThe variable att_start, with a coefficient of 1, is also meaningful. Due to the fact that the MAD_SD value here is 0, we do not need to find the 95% confidence interval to know –intuitively– that this variable has a large and well estimated coefficient.\nConclusion: keep both variables! treatment and att_start are both significant, as well as satisfying other requirements in our guidelines. They are more than worthy of inclusion in our model.\n\n9.2.4 income ~ age + liberal\nNow, we will look at income as a function of age and liberal, a proxy for political party.\n\nfit_2 <- stan_glm(income ~ age + liberal, \n                    data = trains, \n                    refresh = 0)\n\nfit_2\n\nstan_glm\n family:       gaussian [identity]\n formula:      income ~ age + liberal\n observations: 115\n predictors:   3\n------\n            Median   MAD_SD  \n(Intercept) 111017.9  25074.4\nage           1058.1    561.8\nliberalTRUE -32925.3  13030.1\n\nAuxiliary parameter(s):\n      Median  MAD_SD \nsigma 72693.3  4925.9\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nGreat! We have an estimate for income of those who fall into the category of liberalFALSE, as well as data on our right hand side variables of age and liberalTRUE.\nFirst, let’s interpret our coefficients.\n\nThe variable before the tilde, income, is our outcome.\nThe explanatory variables are liberal, which has a resulting value of TRUE or FALSE, and age, a numeric variable.\nThe (Intercept) is estimating income where liberal == FALSE. Therefore, it is the estimated income for commuters that are not liberals and who have age = 0. The estimate for age is showing the increase in income with every additional year of age.\nThe estimate for liberalTRUE represents the offset in predicted income for commuters who are liberal. To find the estimate, we must add the coefficient to our (Intercept) value. We see that, on average, liberal commuters make less money.\n\nIt is important to note that we are not looking at a causal relationship for either of these explanatory variables. We are noting the differences between two groups, without considering causality. This is known as an across unit comparison. When we compare across unit we are not looking at a causal relationship.\nWhen comparing liberalTRUE with our (Intercept), recall that the (Intercept) is calculating income for the case where liberal == FALSE. As we can see, then, the coefficient for liberalTRUE, -32,925, shows that the liberals in our dataset make less on average than non-liberals in our dataset. The coefficient is large relative to the (Intercept) and, with rough mental math, we see that the 95% confidence interval excludes 0. Therefore, liberal is a helpful variable!\nThe variable age, however, does not appear to have a meaningful impact on our (Intercept). The coefficient of age is low and the 95% confidence interval does not exclude 0.\nConclusion: definitely keep liberal! age is less clear. It is really a matter of preference at this point.\n\n9.2.5 liberal ~ I(income/1e+05) + party\nWe will now look at liberal as a function of a transformed income and party. The reason we have transformed income here is due to the fact that, without a transformation, income makes predictions according to very small income disparities. This is not helpful. To enhance the significance of income, we have divided the variable by \\(100000\\). (Note the need for I() in creating the income term within the formula argument.)\n\nfit_3 <- stan_glm(liberal ~ I(income/1e+05) + party, \n                    data = trains, \n                    refresh = 0)\n\nfit_3\n\nstan_glm\n family:       gaussian [identity]\n formula:      liberal ~ I(income/1e+05) + party\n observations: 115\n predictors:   3\n------\n                Median MAD_SD\n(Intercept)      0.7    0.1  \nI(income/1e+05) -0.1    0.1  \npartyRepublican -0.5    0.1  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.5    0.0   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nRecall that, for logistic models, the (Intercept) is nearly impossible to interpret. To evaluate the impact of variables, we will need to use the Divide-by-Four rule. This instructs us to take a logistic regression coefficient (other than the constant term) and divide it by 4 to get an upper bound on the predictive difference corresponding to a unit difference in that variable. All this means is that, when evaluating if a predictor is helpful, in a logistic regression only, to divide the coefficient by four.\nFor partyRepublican, we take our coefficient, 0, and divide it by 4: -0.125. This is the upper bound on the predictive difference corresponding to a unit difference in this variable. Therefore, we see that partyRepublican is meaningful. We want this variable in our model.\nThe variable income, however, seems less promising. In addition to the resulting value from the Divide-by-Four rule being low (-0.023), the MAD_SD for income is also equal to the posterior median itself, indicating a standard error which is large relative to the magnitude of the coefficient. Though we may choose to include income for reasons unrelated to its impact in our model, it does not appear to be worthy of inclusion on the basis of predictive value alone.\nConclusion: the variable party is significant; we will include it in our model. Unless we have a strong reason to include income, we should exclude it.\n\n9.2.6 Final thoughts\nNow that we have looked at three cases of variables and decided whether they should be included, let’s discuss the concept of selecting variables generally.\nThe variables we decided to keep and discard are not necessarily the variables you would keep or discard, or the variables that any other data scientist would keep or discard. It is much easier to keep a variable than it is to build a case for discarding a variable. Variables are helpful even when not significant, particularly when they are indicator variables which may separate the data into two groups that we want to study.\nThe process of selecting variables – though we have guidelines – is complicated. There are many reasons to include a variable in a model. The main reasons to exclude a variable are if the variable isn’t significant or if the variable has a large standard error."
  },
  {
    "objectID": "09-four-parameters.html#comparing-models-in-theory",
    "href": "09-four-parameters.html#comparing-models-in-theory",
    "title": "9  Four Parameters",
    "section": "\n9.3 Comparing models in theory",
    "text": "9.3 Comparing models in theory\nDeciding which variables to include in a model is a subset of the larger question: How do we decide which model, out of the set of possible models, to choose?\nConsider two models which explain attitudes to immigration among Boston commuters.\n\nfit_liberal <- stan_glm(formula = att_end ~ liberal,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nprint(fit_liberal, detail = FALSE)\n\n            Median MAD_SD\n(Intercept) 10.0    0.3  \nliberalTRUE -2.0    0.5  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.7    0.2   \n\n\n\nfit_att_start <- stan_glm(formula = att_end ~ att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 85)\n\nprint(fit_att_start, detail = FALSE)\n\n            Median MAD_SD\n(Intercept) 1.6    0.4   \natt_start   0.8    0.0   \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.4    0.1   \n\n\nThey both seem like good models! The results make sense. People who are liberal have more liberal attitudes about immigration, so we would expect their att_end scores to be lower. We would also expect people to provide similar answers in two surveys administered a week or two apart. It makes sense that those with higher (more conservative) values for att_start would also have higher values for att_end.\nHow do we choose between these models?\n\n9.3.1 Better models make better predictions\nThe most obvious criteria for comparing models is the accuracy of the predictions. For example, consider the use of liberal to predict att_end.\n\ntrains |> \n  mutate(pred_liberal = fitted(fit_liberal)) |> \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Add a red circle where our predictions are most accurate (where the x and y\n  # values are the same, which is where our predictions = the true attitudes).\n  # pch = 1 makes the inside of the point translucent to show the number of\n  # correct predictions.\n  \n  geom_point(aes(x = 8, y = 8), \n             size = 20, pch = 1, \n             color = \"red\") +\n  geom_point(aes(x = 10, y = 10), \n             size = 20, pch = 1, \n             color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Liberals are less conservative\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\n\n\n\n\n\nBecause there are only two possible values for liberal — TRUE and FALSE — there are only two predictions which this model will make: about 10 for liberal == FALSE and about 8 for liberal == TRUE. (The points in the above plot are jittered.) For some individuals, these are perfect predictions. For others, they are poor predictions. the red circles on our plot illustrate the areas where our predictions are equal to true values. As we can see, the model isn’t great at predicting attitude end. (Note the two individuals who are liberal == TRUE, and who the model thinks will have att_end == 8, but who have att_end == 15. The model got them both very, very wrong.)\nConsider our second model, using att_start to forecast att_end.\n\ntrains |> \n  mutate(pred_liberal = fitted(fit_att_start)) |> \n  ggplot(aes(x = pred_liberal, y = att_end)) +\n    geom_jitter(width = 0.05, height = 0.2, alpha = 0.5) +\n  \n  # Insert red line where our predictions = the truth using geom_abline with an\n  # intercept, slope, and color.\n  \n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n    labs(title = \"Modeling Attitude Toward Immigration\",\n         subtitle = \"Survey responses are somewhat consistent\",\n         x = \"Predicted Attitude\",\n         y = \"True Attitude\")\n\n\n\n\nBecause att_end takes on 13 unique values, the model makes 13 unique predictions. Some of those predictions are perfect! But others are very wrong. The red line shows the points where our predictions match the truth. Note the individual with a predicted att_end of around 9 but with an actual value of 15. That is a big miss!\nRather than looking at individual cases, we need to look at the errors for all the predictions. Fortunately, a prediction error is the same thing as a residual, which is easy enough to calculate.\n\ntrains |> \n  select(att_end, att_start, liberal) |> \n  mutate(pred_lib = fitted(fit_liberal)) |> \n  mutate(resid_lib = fitted(fit_liberal) - att_end) |> \n  mutate(pred_as = fitted(fit_att_start)) |> \n  mutate(resid_as = fitted(fit_att_start) - att_end)\n\n# A tibble: 115 × 7\n   att_end att_start liberal pred_lib resid_lib pred_as resid_as\n     <dbl>     <dbl> <lgl>      <dbl>     <dbl>   <dbl>    <dbl>\n 1      11        11 FALSE      10.0    -0.974    10.6    -0.399\n 2      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n 3       5         3 TRUE        8.02    3.02      4.08   -0.916\n 4      11        11 FALSE      10.0    -0.974    10.6    -0.399\n 5       5         8 TRUE        8.02    3.02      8.16    3.16 \n 6      13        13 FALSE      10.0    -2.97     12.2    -0.769\n 7      13        13 FALSE      10.0    -2.97     12.2    -0.769\n 8      11        10 FALSE      10.0    -0.974     9.79   -1.21 \n 9      12        12 FALSE      10.0    -1.97     11.4    -0.584\n10      10         9 FALSE      10.0     0.0259    8.97   -1.03 \n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nLet’s look at the square root of the average squared error.\n\ntrains |> \n  select(att_end, att_start, liberal) |> \n  mutate(lib_err = (fitted(fit_liberal) - att_end)^2) |> \n  mutate(as_err = (fitted(fit_att_start) - att_end)^2) |> \n  summarize(lib_sigma = sqrt(mean(lib_err)),\n            as_sigma = sqrt(mean(as_err))) \n\n# A tibble: 1 × 2\n  lib_sigma as_sigma\n      <dbl>    <dbl>\n1      2.68     1.35\n\n\nThere are many different measures of the error which we might calculate. The squared difference is most common for historical reasons: it was the mathematically most tractable in the pre-computer age. Having calculated a squared difference for each observation, we can sum them or take their average or take the square root of their average. All produce the same relative ranking, but the last is most popular because it (more or less) corresponds to the estimated \\(\\sigma\\) for a linear model. Note how these measures are the same as the ones produced by the Bayesian models created above.\n\n\nSadly, it is not wise to simply select the model which fits the data best because doing so can be misleading. After all, you are cheating! You are using that very data to select your parameters and then, after using the data once, turning around and “checking” to see how well your model fits the data. It better fit! You used it to pick your parameters! This is the danger of overfitting.\n\n9.3.2 Beware overfitting\nOne of the biggest dangers in data science is overfitting, using a model with too many parameters which fits the data we have too well and, therefore, works poorly on data we have yet to see. Consider a simple example with 10 data points.\n\n\n\n\n\nWhat happens when we fit a model with one predictor?\n\n\n\n\n\nThat is a reasonable model. It does not fit the data particularly well, but we certainly believe that higher values of x are associated with higher values of y. A linear fit is not unreasonable.\n\nBut we can also use some of the lessons from above and try a quadratic fit by adding \\(x^2\\) as a predictor.\n\n\n\n\n\nIs this a better model? Maybe?\nBut why stop at adding \\(x^2\\) to the regression? Why not add \\(x^3\\), \\(x^4\\) and all the way to \\(x^9\\)? When we do so, the fit is much better.\n\nnine_pred <- lm(y ~ poly(x, 9),\n                       data = ovrftng)\n\nnewdata <- tibble(x = seq(1, 10, by = 0.01),\n                  y = predict(nine_pred, \n                              newdata = tibble(x = x)))\n\novrftng |> \n  ggplot(aes(x, y)) +\n    geom_point() +\n    geom_line(data = newdata, \n              aes(x, y)) +\n    labs(title = \"`y` as a 9-Degree Polynomial Function of `x`\") +\n    scale_x_continuous(breaks = seq(2, 10, 2)) +\n    scale_y_continuous(breaks = seq(2, 10, 2)) \n\n\n\n\nIf the only criteria we cared about was how well the model predicts using the data on which the parameters were estimated, then a model with more parameters will always be better. But that is not what truly matters. What matters is how well the model works on data which was not used to create the model.\n\n\n9.3.3 Better models make better predictions on new data\nThe most sensible way to test a model is to use the model to make predictions and compare those predictions to new data. After fitting the model using stan_glm, we would use posterior_predict to obtain simulations representing the predictive distribution for new cases. For instance, if we were to predict how someone’s attitude changes toward immigration among Boston commuters based on political affiliation, we would want to go out and test our theories on new Boston commuters.\nWhen thinking of generalization to new data, it is important to consider what is relevant new data in the context of the modeling problem. Some models are used to predict the future and, in those cases, we can wait and eventually observe the future and check how good our model is for making predictions. Often models are used to obtain insight to some phenomenon without immediate plan for predictions. This is the case with our Boston commuters example. In such cases, we are also interested whether learned insights from on part of the data generalizes to other parts of the data. For example, if we know how political attitudes informed future immigration stances in Boston commuters, we may want to know if those same conclusions could generalize to train commuters in different locations.\nEven if we had detected clear problems with our predictions, this would not necessarily mean that there is anything wrong with the model as fit to the original dataset. However, we would need to understand it further before generalizing to other commuters.\nOften, we would like to evaluate and compare models without waiting for new data. One can simply evaluate predictions on the observed data. But since these data have already been used to fit the model parameters, these predictions are optimistic.\nIn cross validation, part of the data is used to fit the model and the rest of the data—the hold-out set—is used as a proxy for future data. When there is no natural prediction task for future data, we can think of cross validation as a way to assess generalization from one part of the data to another part.\nIn any form of cross validation, the model is re-fit leaving out one part of the data and then the prediction for the held-out part is evaluated. In the next section, we will look at a type of cross validation called leave-one-out (LOO) cross validation."
  },
  {
    "objectID": "09-four-parameters.html#comparing-models-in-practice",
    "href": "09-four-parameters.html#comparing-models-in-practice",
    "title": "9  Four Parameters",
    "section": "\n9.4 Comparing models in practice",
    "text": "9.4 Comparing models in practice\nTo compare models without waiting for new data, we evaluate predictions on the observed data. However, due to the fact that the data has been used to fit the model parameters, our predictions are often optimistic when assessing generalization.\nIn cross validation, part of the data is used to fit the model, while the rest of the data is used as a proxy for future data. We can think of cross validation as a way to assess generalization from one part of the data to another part. How do we do this?\n\nWe can hold out individual observations, called leave-one-out (LOO) cross validation; or groups of observations, called leave-one-group-out cross validation; or use past data to predict future observations, called leave-future-out cross validation. When we perform cross validation, the model is re-fit leaving out one part of the data and then the prediction for the held-out part is evaluated.\nFor our purposes, we will be performing cross validation using leave-one-out (LOO) cross validation.\n\n9.4.1 Cross validation using loo()\n\n\nTo compare models using leave-one-out (LOO) cross validation, one piece of data is excluded from our model. The model is then re-fit and makes a prediction for the missing piece of data. The difference between the predicted value and the real value is calculated. This process is repeated for every row of data in the dataset.\nIn essence: One piece of data is excluded from our model, the model is re-fit, the model attempts to predict the value of the missing piece, we compare the true value to the predicted value, and we assess the accuracy of our model’s prediction. This process occurs for each piece of data, allowing us to assess the model’s accuracy in making predictions.\nTo perform leave-one-out(LOO) cross validation, we will be using the function loo() from an R package. This is how we will determine which model is superior for our purposes.\nFirst, we will refamiliarize ourselves with our first model, fit_liberal.\n\nfit_liberal\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ liberal\n observations: 115\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 10.0    0.3  \nliberalTRUE -2.0    0.5  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 2.7    0.2   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nNow, we will perform loo() on our model and look at the results.\n\nloo_liberal <- loo(fit_liberal)\n\nloo_liberal\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -279.4  7.5\np_loo         2.9  0.5\nlooic       558.9 15.0\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nWhat does any of this mean?\n\n\nelpd_loo is the estimated log score along with a standard error representing uncertainty due to using only 115 data points.\n\np_loo is the estimated “effective number of parameters” in the model.\n\nlooic is the LOO information criterion, −2 elpd_loo, which we compute for comparability to deviance.\n\nFor our purposes, we mostly need to focus on elpd_loo. Let’s explain, in more depth, what this information means.\nBasically, when we run loo(), we are telling R to take a piece of data out of our dataset, re-estimate all parameters, and then predict the value for the missing piece of data. The value for elpd_loo() is based off of how close our estimate was to the truth. Therefore, elpd_loo() values inform us of the effectiveness of our model in predicting data it has not seen before.\n\nThe higher our value for elpd_loo, the better our model performs. This means that, when comparing models, we want to select the model with the higher value for elpd_loo.\n\nLet’s turn our attention to our second model. To begin, let’s observe the qualities of fit_att_start once again.\n\nfit_att_start\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ att_start\n observations: 115\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 1.6    0.4   \natt_start   0.8    0.0   \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.4    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nGreat! Now, let’s perform loo() on this model.\n\nloo_att_start <- loo(fit_att_start) \n\nloo_att_start\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -201.7 12.5\np_loo         4.2  1.8\nlooic       403.4 25.1\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n\nThe elpd_loo value for this model is -201.7. This is higher than the elpd_loo for att_liberal, implying that this model is superior. However, we can’t see our estimates together. Is there a simpler way to calculate which model is better?\nActually, yes! Using the function loo_compare(), we can compare the models directly.\n\n9.4.2 Comparing models using loo_compare()\n\nTo compare the two models directly, we can use the function loo_compare with our two loo objects created above. This will calculate the difference in elpd_loo() between our models for us, making our job easier:\n\nloo_compare(loo_att_start, loo_liberal)\n\n              elpd_diff se_diff\nfit_att_start   0.0       0.0  \nfit_liberal   -77.7      12.0  \n\n\nThe value for elpd_diff is equal to the difference in elpd_loo between our two models. These_diff shows that the difference in standard error.\nTo interpret the results directly, it is important to note that the first row will be the superior model. The values of elpd_diff and att_start will be 0, as these columns show the offset in the estimates compared to the better model. To reiterate: when the better model is compared to itself, those values will be 0. The following rows show the offset in elpd and se values between the less effective model, fit_liberal, and the more effective model, fit_att_start.\nThe better model is clear: fit_att_start. Therefore, the attitude at the start of the trains study is more significant to predicting final attitude when compared with the variable liberal, which is an analog for political affiliation.\nAs we have seen, loo_compare is a shortcut for comparing two models. When you are deciding between two models, loo_compare() is a great way to simplify your decision.\nWhat do we do when the value of loo_compare() is small? As a general practice, differences smaller than four are hard to distinguish from noise. In other words: when elpd_diff is less than 4, there is no advantage to one model over the other."
  },
  {
    "objectID": "09-four-parameters.html#testing-is-nonsense",
    "href": "09-four-parameters.html#testing-is-nonsense",
    "title": "9  Four Parameters",
    "section": "\n9.5 Testing is nonsense",
    "text": "9.5 Testing is nonsense\nAs always, it is important to look at the practices of other professionals and the reasons we may choose not to follow those tactics. For instance, our continued problem with hypothesis testing. In hypothesis testing, we assert a null hypothesis \\(H_0\\) about our data and an alternative hypothesis \\(H_a\\).\nWhen performing hypothesis testing, we either reject the hypothesis or we do not reject it. The qualifications for rejecting are met if the 95% confidence interval excludes the null hypothesis. If the hypothesis is included in our 95% confidence interval, we do not reject it. In the case of “insignificant” results, with p > 0.5, we also can’t “reject” the null hypothesis. However, this does not mean that we accept it.\nThe premise of hypothesis testing is to answer a specific question – one that may not even be particularly relevant to our understanding of the world – about our data. So, what are our problems with hypothesis testing? - Rejecting or not rejecting hypotheses doesn’t helps us to answer real questions. - The fact that a difference is not “significant” has no relevance to how we use the posterior to make decisions. - Statistical significance is not equal to practical importance. - There is no reason to test when you can summarize by providing the full posterior probability distribution."
  },
  {
    "objectID": "09-four-parameters.html#parallel-lines",
    "href": "09-four-parameters.html#parallel-lines",
    "title": "9  Four Parameters",
    "section": "\n9.6 Parallel lines",
    "text": "9.6 Parallel lines\nTo conclude this chapter, we will look at a four parameter model. The model we will use will measure att_end as a function of liberal or att_start or treatment or some combination of these variables.\n\n\n\n9.6.1 Wisdom\n\n\n\n\nWisdom\n\n\n\n\nBefore making a model which seeks to explain att_end, we should plot it and the variables we think are connected to it:\n\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  geom_point() +\n  labs(title = \"Attitude End Compared with Attitude Start and Liberal\",\n       x = \"Attitude at Start of Study\",\n       y = \"Attitude at End of Study\",\n       color = \"Liberal?\")\n\n\n\n\nIs that data believable? Maybe? One could imagine that att_end would be predicted fairly well by att_start. This makes sense for most of our data points, which show not much difference between the attitudes. But what about the great disparities in attitude shown for the individual with a starting attitude of 9 and an ending attitude around 15? In a real data science project, this would require further investigation. For now, we ignore the issue and blithely press on.\nAnother component of Wisdom is the population. The concept of the “population” is subtle and important. The population is not the set of commuters for which we have data. That is the dataset. The population is the larger — potentially much larger — set of individuals about whom we want to make inferences. The parameters in our models refer to the population, not to the dataset.\nThere are many different populations, each with its own \\(\\mu\\), in which we might be interested. For instance:\n\nThe population of Boston commuters on the specific train and time included in our dataset.\nThe population of all Boston commuters.\nThe population of commuters in the United States.\n\nAll of these populations are different, so each has a different \\(\\mu\\). Which \\(\\mu\\) we are interested in depends on the problem we are trying to solve. It is a judgment call, a matter of Wisdom, as to whether or not that data we have is “close enough” to the population we are interested in to justify making a model.\nThe major part of Wisdom is deciding what questions you can’t answer because of the data you don’t have.\n\n9.6.2 Justice\n\n\n\n\nJustice\n\n\n\n\nNow that we have considered the connection between our data and the predictions we seek to make, we will need to consider our model.\nFirst: is our model causal or predictive? Recall that our model measures att_end as a function of liberal and att_start. The variables liberal and att_start do not involve a control or treatment dynamic. There is no manipulation with these variables. Given that there is no causation without manipulation, this is a predictive model.\nWe are making inferences about groups of people according to their political affiliation and starting attitude. We are not measuring causality, but we are predicting outcomes.\nWhen creating a parallel slops model, we use the basic equation of a line:\n\\[y_i = \\beta_0  + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}\\]\nIf \\(y = att\\_end\\), \\(x_1 = att\\_start\\), \\(x\\_2 = liberal\\), then the equations are as follows:\nIf liberal = FALSE:\n\\[y_i = \\beta_0  + \\beta_1 x_{1,i}\\]\nWhich equates to, in y = b + mx form:\n\\[y_i = intercept +  \\beta_1 att\\_start_i\\]\nIf liberal = TRUE:\n\\[y_i = (\\beta_0  + \\beta_2) + \\beta_1 x_{1,i}\\]\nWhich equates to, in y = b + mx form:\n\\[y_i = (intercept + liberal\\_true) + \\beta_1 att\\_start_i\\]\n\n\n9.6.3 Courage\n\n\n\n\nCourage\n\n\n\n\nThe use of stan_glm is the same as usual. Using stan_glm, we will create our model, fit_1.\n\nfit_1 <- stan_glm(formula = att_end ~ liberal + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 42)\n\nfit_1\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ liberal + att_start\n observations: 115\n predictors:   3\n------\n            Median MAD_SD\n(Intercept)  1.9    0.5  \nliberalTRUE -0.3    0.3  \natt_start    0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.4    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nTo remind ourselves, recall that the (Intercept) here is representing the att_end value for cases where liberal = FALSE and treatment does not equal Control. The next row, liberalTRUE gives a median value which represents the offset in the prediction compared with the (Intercept). In other words, the true intercept for cases where liberal = TRUE is represented by \\((Intercept) + liberalTRUE\\). The value of treatmentControl is the offset in att_end for those in the Control group.\nTo find our intercepts, we will need to tidy our regression using the tidy() function from the broom.mixed package. We tidy our data and extract values to create a parallel slopes model. The parallel slopes model allows us to visualize multi-variate Bayesian modeling (i.e. modeling with more than one explanatory variable). That is a complicated way of saying that we will visualize the fitted model created above in a way that allows us to see the intercepts and slopes for two different groups, liberalTRUE and liberalFALSE:\n\n# First, we will tidy the data from our model and select the term and estimate.\n# This allows us to create our regression lines more easily.\n\ntidy <- fit_1 |> \n  tidy() |> \n  select(term, estimate)\n\ntidy\n\n# A tibble: 3 × 2\n  term        estimate\n  <chr>          <dbl>\n1 (Intercept)    1.93 \n2 liberalTRUE   -0.300\n3 att_start      0.799\n\n# Extract and name the columns of our tidy object. By calling tidy$estimate[1],\n# we are telling R to extract the first value from the estimate column in our\n# tidy object.\n\nintercept <- tidy$estimate[1]\nliberal_true <- tidy$estimate[2]\natt_start <- tidy$estimate[3]\n\nNow, we can define the following terms—- liberal_false_intercept and liberal_false_att_slope; and liberal_true_intercept and liberal_true_att_slope:\n\n# Recall that the (Intercept) shows us the estimate for the case where liberal =\n# FALSE. We want to extract the liberal_false_intercept to indicate where the\n# intercept in our visualization should be. The slope for this case, and for the\n# liberal = TRUE case, is att_start.\n\nliberal_false_intercept <- intercept\nliberal_false_att_slope <- att_start\n\n#  When wanting the intercept for liberal = TRUE, recall that the estimate for\n#  liberalTRUE is the offset from our (Intercept). Therefore, to know the true\n#  intercept, we must add liberal_true to our intercept.\n\nliberal_true_intercept <- intercept + liberal_true\nliberal_true_att_slope <- att_start\n\nAll we’ve done here is extracted the values for our intercepts and slopes, and named them to be separated into two groups. This allows us to create a geom_abline object that takes a unique slope and intercept value, so we can separate the liberalTRUE and liberalFALSE observations.\n\n# From the dataset trains, use att_start for the x-axis and att_end for\n# the y-axis with color as liberal. This will split our data into two color\n# coordinates (one for liberal = TRUE and one for liberal = FALSE)\n\nggplot(trains, aes(x = att_start, y = att_end, color = liberal)) +\n  \n  # Use geom_point to show the datapoints. \n  \n  geom_point() +\n  \n  # Create a geom_abline object for the liberal false values. Set the intercept\n  # equal to our previously created liberal_false_intercept, while setting slope\n  # equal to our previously created liberal_false_att_slope. The color call is\n  # for coral, to match the colors used by tidyverse for geom_point().\n  \n  geom_abline(intercept = liberal_false_intercept,\n              slope = liberal_false_att_slope, \n              color = \"#F8766D\", \n              size = 1) +\n  \n  # Create a geom_abline object for the liberal TRUE values. Set the intercept\n  # equal to our previously created liberal_true_intercept, while setting slope\n  # equal to our previously created liberal_true_att_slope. The color call is\n  # for teal, to match the colors used by tidyverse for geom_point().\n\n  geom_abline(intercept = liberal_true_intercept,\n              slope = liberal_true_att_slope,\n              color = \"#00BFC4\", \n              size = 1) +\n  \n  # Add the appropriate titles and axis labels. \n  \n  labs(title = \"Parallel Slopes Model\",\n       x = \"Attitude at Start\", \n       y = \"Attitude at End\", \n       color = \"Liberal\") \n\n\n\n\nThis is our parallel slopes model. What we have done, essentially, is created a unique line for liberalTRUE and liberalFALSE to observe the differences in the groups as related to attitude start and attitude end.\nAs we can see, commuters who are not liberal tend to start with slightly higher values for att_start. Commuters who are liberal tend to have lower starting values for att_start.\nNow, what if we want to look at another model? To judge whether we can have a superior model? Let’s create another object, using stan_glm, that looks at att_end as a function of treatment and att_start.\n\nfit_2 <- stan_glm(formula = att_end ~ treatment + att_start,\n                  data = trains,\n                  refresh = 0,\n                  seed = 56)\n\nfit_2\n\nstan_glm\n family:       gaussian [identity]\n formula:      att_end ~ treatment + att_start\n observations: 115\n predictors:   3\n------\n                 Median MAD_SD\n(Intercept)       2.4    0.4  \ntreatmentControl -1.0    0.2  \natt_start         0.8    0.0  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 1.3    0.1   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nTo interpret briefly:\n\n(Intercept) here is representing the att_end value for cases where treatment does not equal Control.\ntreamtmentControl gives a median value which represents the offset in the prediction compared with the (Intercept). In other words, the true intercept for cases where treatment = Control is represented by \\((Intercept) + treatmentControl\\).\natt_start represents the slope for both groups representing a unit change.\n\nAn important point here is that these models are causal. When including the variable treatment, we have a measured causal effect of a condition. This is different from our prior parallel slopes model, where we were modeling for prediction, not causation.\n\nTo see how these models compare in performance, we will perform leave-one-out (LOO) cross validation again.\n\nL1 <- loo(fit_1)\n\nL1\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -202.3 13.1\np_loo         5.5  2.3\nlooic       404.6 26.1\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     114   99.1%   2531      \n (0.5, 0.7]   (ok)         1    0.9%   157       \n   (0.7, 1]   (bad)        0    0.0%   <NA>      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nPerform loo() on our second model:\n\nL2 <- loo(fit_2)\n\nL2\n\n\nComputed from 4000 by 115 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -195.3 12.2\np_loo         5.1  1.8\nlooic       390.5 24.5\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nPareto k diagnostic values:\n                         Count Pct.    Min. n_eff\n(-Inf, 0.5]   (good)     114   99.1%   1625      \n (0.5, 0.7]   (ok)         1    0.9%   206       \n   (0.7, 1]   (bad)        0    0.0%   <NA>      \n   (1, Inf)   (very bad)   0    0.0%   <NA>      \n\nAll Pareto k estimates are ok (k < 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nRecall that the relevant data is the data from elpd_loo. The estimates for elpd_loo vary quite a bit. Recall that the superior model will have a value of elpd_loo() that is closer to 0. The standard error (SE) for these models also differs some. To compare these directly, we will use loo_compare.\n\nloo_compare(L1, L2)\n\n      elpd_diff se_diff\nfit_2  0.0       0.0   \nfit_1 -7.0       3.8   \n\n\nRecall that, with loo_compare(), the resulting data shows the superior model first, with values of 0 for elpd_diff and se_diff, since it compares the models to the best option. The values of elpd_diff and se_diff for fit_1 show the difference in the models. As we can see, fit_2, the model which looks at treatment + att_start, is better.\nBut how certain can we be that it is better? Note that the difference between the two models is not quite two standard errors. So, there is a reasonable possibility that the difference is due to chance.\n\n\n9.6.4 Temperance\nWhat we really care about is data we haven’t seen yet, mostly data from tomorrow. But what if the world changes, as it always does? If it doesn’t change much, maybe we are OK. If it changes a lot, then what good will our model be? In general, the world changes some. That means that are forecasts are more uncertain that a naive use of our model might suggest. Having created (and checked) a model, we now use the model to answer questions. Models are made for use, not for beauty. The world confronts us. Make decisions we must. Our decisions will be better ones if we use high quality models to help make them.\nPreceptor’s Posterior is the posterior you would calculate if all the assumptions you made under Wisdom and Justice were correct. Sadly, they never are! So, you can never know Preceptor’s Posterior. Our posterior will, we hope, be a close-ish approximation of Preceptor’s Posterior."
  },
  {
    "objectID": "09-four-parameters.html#summary",
    "href": "09-four-parameters.html#summary",
    "title": "9  Four Parameters",
    "section": "\n9.7 Summary",
    "text": "9.7 Summary\nIn this chapter, we covered a number of topics important to effectively creating models.\nKey commands: - Create a model using stan_glm(). - After creating a model, we can use loo() to perform leave-one-out cross validation. This assesses how effectively our model makes predictions for data it has not seen yet. - The command loo_compare() allows us to compare two models, to see which one performs better in leave-one-out cross validation. The superior model makes better predictions.\nRemember: - We can transform variables – through centering, scaling, taking logs, etc. – to make them more sensible. Consider using a transformation if the intercept is awkward. For instance, if the intercept for age represents the estimate for people of age zero, we might consider transforming age to be easier to interpret. - When selecting variables to include in our model, follow this rule: keep it if the variable has a large and well-estimated coefficient. This means that the 95% confidence interval excludes zero. Speaking roughly, removing a variable with a large coefficient meaningfully changes the predictions of the model. - When we compare across unit, meaning comparing Joe and George, we are not looking at a causal relationship. Within unit discussions, where we are comparing Joe under treatment versus Joe under control, are causal. This means that within unit interpretation is only possible in causal models, where we are studying one unit under two conditions. - When we talk about two potential outcomes, we are discussing the same person or unit under two conditions."
  },
  {
    "objectID": "11-n-parameters.html",
    "href": "11-n-parameters.html",
    "title": "11  N Parameters",
    "section": "",
    "text": "This chapter is still a DRAFT.\nHaving created models with one parameter in Chapter @ref(one-parameter), two parameters in Chapter @ref(two-parameters), three parameters in Chapter @ref(three-parameters), four parameters in Chapter @ref(four-parameters) and five parameters in Chapter @ref(five-parameters), you are now ready to make the jump to \\(N\\) parameters.\nIn this chapter, we will consider models with many parameters and the complexities that arise therefrom. As our models grow in complexity, we need to pay extra attention to basic considerations like validity, population, and representativeness. It is easy to jump right in and start interpreting! It is harder, but necessary, to ensure that our models are really answering our questions.\nImagine you are running for Governor in Texas and want to do a better job of getting your voters to vote. How can you encourage voters to go out to the polls on election day?"
  },
  {
    "objectID": "11-n-parameters.html#wisdom",
    "href": "11-n-parameters.html#wisdom",
    "title": "11  N Parameters",
    "section": "\n11.1 Wisdom",
    "text": "11.1 Wisdom\n\n\n\n\nWisdom\n\n\n\n\nAs you research ways to increase voting, you come across a large-scale experiment showing the effect of sending out a voting reminder that “shames” citizens who do not vote. You are considering sending out a “shaming” voting reminder yourself.\nWe will be looking at the shaming dataset from the primer.data package. This is dataset is from “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment” by Gerber, Green, and Larimer (2008). Check out the paper here. You can, and should, familiarize yourself with the data by typing ?shaming.\nRecall our initial question: how can we encourage voters to go out to the polls on election day? We now need to translate this into a more precise question, one that we can answer with data.\nOur question:\nWhat is the causal effect, on the likelihood of voting, of different postcards on voters of different levels of political engagement?\n\n11.1.1 Ideal Preceptor Table\nRecall the ideal Preceptor Table. What rows and columns of data do you need such that, if you had them all, the calculation of the number of interest would be trivial? If you want to know the average height of an adult in India, then the ideal Preceptor Table would include a row for each adult in India and a column for their height.\nOne key aspect of this ideal Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. Mainly: do we need a causal model, one which estimates that attitude under both treatment and control? The Preceptor Table would require two columns for the outcome. In this case, we are trying to see the causal effect of mailed voting reminders on voting.\nAre we are modeling (just) for prediction or are we (also) modeling for causation? Predictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model. Here, we are looking at causation.\nSo, what would our ideal table look like? Assuming we are running for governor in the United States, we would ideally have data for every citizen of voting age. This means we would have approximately 200 million rows.\nBecause there is no missing data in an ideal Preceptor Table, we would also know the outcomes under both treatment (receiving a reminder) and control (not receiving a reminder). Here is a sample row from our table:\n\n\nWarning: package 'tidyverse' was built under R version 4.2.1\n\n\nWarning: package 'tibble' was built under R version 4.2.1\n\n\n\n\n\n\n\n\n\n\nID\n      \n        Outcomes\n      \n    \n\nBehavior in Treatment\n      Behavior in Control\n      Treatment effect\n    \n\n\n\nCitizen 1\n0\n1\n+1\n\n\nCitizen 2\n1\n1\n0\n\n\n\n\n\n\nIn our ideal table, we have rows for all American citizens of voting age. This is a good start! However, we may want even more information in our ideal Preceptor Table. Perhaps a column for sex would be informative. A column for age? Political affiliation? In a perfect world, we would know all of these pieces of information. In a perfect world, we could measure the exact causal effect of voting reminders for different subsets of the US population.\nWe may also want to narrow our ideal Preceptor Table. If we are running for governor in Florida, we may only want to study citizens in Florida. If we are running as a Democrat, we may only want to study citizens who are registered Democrats.\nHowever, the main point of this exercise is to see what we want to know compared with what we actually do know.\n\n11.1.2 EDA of shaming\n\nAfter loading the packages we need, let’s perform an EDA, starting off by running glimpse() on the shaming tibble from the primer.data package.\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(rstanarm)\nlibrary(ggthemes)\nlibrary(ggdist)\nlibrary(gt)\nlibrary(janitor)\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\n\nglimpse(shaming)\n\nRows: 344,084\nColumns: 15\n$ cluster       <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n$ primary_06    <int> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,…\n$ treatment     <fct> Civic Duty, Civic Duty, Hawthorne, Hawthorne, Hawthorne,…\n$ sex           <chr> \"Male\", \"Female\", \"Male\", \"Female\", \"Female\", \"Male\", \"F…\n$ age           <int> 65, 59, 55, 56, 24, 25, 47, 50, 38, 39, 65, 61, 57, 37, …\n$ primary_00    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ general_00    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ primary_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ general_02    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"…\n$ primary_04    <chr> \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"N…\n$ general_04    <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", …\n$ hh_size       <int> 2, 2, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1,…\n$ hh_primary_04 <dbl> 0.0952381, 0.0952381, 0.0476190, 0.0476190, 0.0476190, 0…\n$ hh_general_04 <dbl> 0.8571429, 0.8571429, 0.8571429, 0.8571429, 0.8571429, 0…\n$ neighbors     <int> 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, …\n\n\nglimpse() gives us a look at the raw data contained within the shaming data set. At the very top of the output, we can see the number of rows and columns, or observations and variables respectively. We see that there are 344,084 observations, with each row corresponding to a unique respondent. This summary provides an idea of some of the variables we will be working with.\nVariables of particular interest to us are sex, hh_size, and primary_06. The variable hh_size tells us the size of the respondent’s household, sex tells us the sex of the respondent, and primary_06 tells us whether or not the respondent voted in the 2006 Primary election. There are a few things to note while exploring this data set. You may – or may not – have noticed that the only response to the general_04 variable is “Yes”. In their published article, the authors note that “Only registered voters who voted in November 2004 were selected for our sample” (Gerber, Green, Larimer, 2008). After this, the authors found their history then sent out the mailings. Thus, non-registered voters are excluded from our data.\nIt is also important to identify the dependent variable and its meaning. In this shaming experiment, the dependent variable is primary_06, which is a variable coded either 0 or 1 for whether or not the respondent voted in the 2006 primary election. This is the dependent variable because the authors are trying to measure the effect that the treatments have on voting behavior in the 2006 general election.\n\nWe have not yet discussed the most important variable of them all: treatment. The treatment variable is a factor variable with 5 levels, including the control. Since we are curious as to how sending mailings affects voter turnout, the treatment variable will tell us about the impact each type of mailing can make. Let’s start off by taking a broad look at the different treatments.\n\n\nshaming |>\n  count(treatment)\n\n# A tibble: 5 × 2\n  treatment        n\n  <fct>        <int>\n1 No Postcard 191243\n2 Civic Duty   38218\n3 Hawthorne    38204\n4 Self         38218\n5 Neighbors    38201\n\n\nFour types of treatments were used in the experiment, with voters receiving one of the four types of mailing. All of the mailing treatments carried the message, “DO YOUR CIVIC DUTY - VOTE!”.\nThe first treatment, Civic Duty, also read, “Remember your rights and responsibilities as a citizen. Remember to vote.” This message acted as a baseline for the other treatments, since it carried a message very similar to the one displayed on all the mailings.\nIn the second treatment, Hawthorne, households received a mailing which told the voters that they were being studied and their voting behavior would be examined through public records. This adds a small amount of social pressure to the households receiving this mailing.\nIn the third treatment, Self, the mailing includes the recent voting record of each member of the household, placing the word “Voted” next to their name if they did in fact vote in the 2004 election or a blank space next to the name if they did not. In this mailing, the households were also told, “we intend to mail an updated chart” with the voting record of the household members after the 2006 primary. By emphasizing the public nature of voting records, this type of mailing exerts more social pressure on voting than the Hawthorne treatment.\nThe fourth treatment, Neighbors, provides the household members’ voting records, as well as the voting records of those who live nearby. This mailing also told recipients, “we intend to mail an updated chart” of who voted in the 2006 election to the entire neighborhood.\n\n11.1.3 Population\nOne of the most important components of Wisdom is the concept of the “population”. Recall the questions we asked earlier:\nAs we have discussed before, the population is not the set of people, or voters, for which we have data. This is the dataset. Nor is it the set of voters about whom we would like to have data. Those are the rows in the ideal Preceptor Table. The population is the larger — potentially much larger — set of individuals which include both the data we have and the data we want. Generally, the population will be much larger than either the data we have and the data we want.\nIn this case, we are viewing the data from the perspective of someone running for Governor this year that wants to increase voter turnout. We want to increase turnout now, not for people voting in 2006! We also may want to increase turnout in those citizens who are not registered to vote, a group that is excluded from our dataset. Is it reasonable to generate conclusions for this group? Most likely, no. However, we have limited data to work with and we have to determine how far we are willing to generalize to other groups.\nIt is a judgment call, a matter of Wisdom, as to whether or not we may assume that the data we have and the data we want to have (i.e., the ideal Preceptor Table) are drawn from the same population.\n\n\nEven though the original question is about “voters” in general, and does not specifically refer to specific states in which we might be interested, we will assume that the data we have for random voters is, uh, representative enough of the population we are interested in. If we did not believe that, then we should stop right now. The major part of Wisdom is deciding what questions you can’t answer because of the data you just don’t have."
  },
  {
    "objectID": "11-n-parameters.html#justice",
    "href": "11-n-parameters.html#justice",
    "title": "11  N Parameters",
    "section": "\n11.2 Justice",
    "text": "11.2 Justice\n\n\n\n\nJustice\n\n\n\n\nJustice emphasizes a few key concepts:\n\nThe actual Preceptor Table, a structure which includes a row for every unit in the population. We generally break the rows in the actual Preceptor Table into three categories: the data for units we want to have, the data for units which we actually have, and the data for units we do not care about.\nIs our data representative of the population?\nIs the meaning of the columns consistent, i.e., can we assume validity?\n\nWe then make an assumption about the data generating mechanism.\n\n11.2.1 Preceptor Table\nRecall that in an actual Preceptor Table, we will have a bunch of missing data! We can not use simple arithmetic to calculate the causal effect of voting reminders on voting behavior. Instead, we will be required to estimate it. This is our estimand, a variable in the real world that we are trying to measure. An estimand is not the value you calculated, but is rather the unknown variable you want to estimate.\nLet’s build a basic visualization for the actual Preceptor Table for this scenario:\n\n\n\n\n\n\n\n\nID\n      \n        Outcomes\n      \n      \n        $$\\text{Estimand}$$\n      \n    \n\nTreatment\n      Control\n      Treatment - Control\n    \n\n\n\n\nCitizen 1\n\n\nVoted\n\n\n?\n\n\n?\n\n\n\n\nCitizen 23\n\n\nDid not vote\n\n\n?\n\n\n?\n\n\n\n\nCitizen 40\n\n\n?\n\n\nVoted\n\n\n?\n\n\n\n\nCitizen 53\n\n\n?\n\n\nDid not vote\n\n\n?\n\n\n\n\nCitizen 80\n\n\nVoted\n\n\n?\n\n\n?\n\n\n\n\n\n\n\nHere, there are two possible outcomes: did vote or did not vote. What we really want to know is the Average Treatment Effect (ATE) of the treatment, the voting reminder. We want to estimate how much the voting reminder impacts the odds of someone voting.\nNote that this is a simplified version of the actual Preceptor Table. In this dataset, we have a number of other columns that we know about each of our subjects: age, sex, past voting history. In an expanded actual Preceptor Table, these columns would be included.\nNow, how can we fill in the question marks? Because of the Fundamental Problem of Causal Inference, we can never know the missing values. Because we can never know the missing values, we must make assumptions. “Assumption” just means that we need a “model,” and all models have parameters.\n\n11.2.2 The Population Table\nThe Population Table shows the data that we actually have in our desired population. It shows rows from three sources: the ideal Preceptor Table, the data, and the population outside of the data (rows which exist but for which we have no data).\nIn our ideal Preceptor rows, we have information for our covariates of sex, year, and state. However, because those rows are not included in our data, we do not have any outcome results. Since our scenario pertains to an upcoming election in Texas, the state column will read Texas and the year column will read 2021.\nThe rows from our data have everything: the covariates and the outcomes. The covariates here will be Michigan for state and 2006 for year, since these are the pieces of information that are included in our data. Of course, we still do not have values for Treatment minus Control, since we cannot observe one subject under two conditions.\nThe rows from the population have no data. These are subjects which fall under our desired population, but for which we have no data. As such, all rows are missing.\n\n\n\n\n\n\n\n\nSource\n      Sex\n      Year\n      State\n      \n        Outcomes\n      \n      \n        Causal Effect\n      \n    \n\nTreatment\n      Control\n      Treatment - Control\n    \n\n\n\n\nPopulation\n\n\n?\n\n\n1990\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n1995\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nData\n\n\nMale\n\n\n2006\n\n\nMichigan\n\n\nDid not vote\n\n\n?\n\n\n?\n\n\n\n\nData\n\n\nFemale\n\n\n2006\n\n\nMichigan\n\n\n?\n\n\nVoted\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\n?\n\n\n2010\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n2012\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPreceptor Table\n\n\nFemale\n\n\n2021\n\n\nTexas\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nFemale\n\n\n2021\n\n\nTexas\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\n?\n\n\n2026\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n2030\n\n\n?\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n\n\n\n\n11.2.3 Validity\nTo understand validity in regards to the Population Table, we must first recognize an inherent flaw in any experiment design: no two units receive exactly the same treatment.\nWe might be thinking: well, surely two postcards are the same? But, they aren’t! The postcards sent from our data were sent with information relevant to 2006 — a different candidate, different language, different syntax. A postcard sent in 2021, even if we used the exact same language, would be encouraging a new candidate with new reform and differing policies.\nThus, despite the fact that two units are in the same treatment — that is, receiving a postcard — they have very different versions of that treatment. Indeed, there are an infinite number of possible treatments. This is why it is so important to define our estimand clearly.\n\n11.2.4 Stability\nStability means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn.\nWith something like height, it is much easier to assume stability over a greater period of time. Changes in global height occur extremely slowly, so height being stable across a span of 20 years is reasonable to assume. Can we say the same for this example, where we are looking at voting behavior?\nIs data collected in 2006 on voting behavior likely to be the same in 2021? Frankly, we don’t know! We aren’t sure what would impact someone’s response to a postcard encouraging them to vote. It is possible, for instance, that a postcard informing neighbors of voting status would have more of an effect on voting behavior during a pandemic, when you are more closely interacting with neighbors.\nWhen we are confronted with this uncertainty, we can consider making our timeframe smaller. However, we would still need to assume stability from 2006 (time of data collection) to today. Stability allows us to ignore the issue of time.\n\n11.2.5 Representativeness\nThis is a good time to consider what it really means to accept that our data is representative of our population. With that in mind, let’s break down our real, current question:\n\nWe are running for governor in Texas in the year 2021. In this year and in the United States, we consider sending out a voting reminder postcard to citizens of voting age. Will this reminder encourage voting, and by how much?\n\nNow, let’s break down our data from the shaming dataset:\n\nThe data was gathered in Michigan prior to the August 2006 primary election. The population for the experiment was 180,002 households in the state of Michigan. The data only included those who had voted in the 2004 general election. Therefore, it did not include non-voters. The reminders were mailed to households at random.\n\nSo, how similar are these groups? Let’s start with some differences. * The data is from 2006. Our question is asking for answers from 2021. This is not a small gap in time. A lot changes in a decade and a half! * The data excludes all non-voters in the last election. Our question, which seeks to increase voting turnout in all citizens, would want for non-voters to be included. So, can we make any claims about those citizens? Probably not. * The data only includes voters from Michigan. We want to make inferences about Texas, or perhaps the United States as a whole. Is it within reason to do that?\nGenerally: if there was no chance that a certain type of person would have been in this experiment, we cannot make an assumption for that person.\nThe purpose of this section is to make us think critically about the assumptions we are making and whether those assumptions can be reasonably made. Though we will continue using this dataset in the remainder of the chapter, it is clear that we must make our predictions with caution.\n\n11.2.6 Functional form"
  },
  {
    "objectID": "11-n-parameters.html#courage",
    "href": "11-n-parameters.html#courage",
    "title": "11  N Parameters",
    "section": "\n11.3 Courage",
    "text": "11.3 Courage\n\n\n\n\nCourage\n\n\n\n\n\n11.3.1 Set-up\nNow, we will create an object named object_1 that includes a 3-level factor classifying voters by level of civic engagement.\n\nConvert all primary and general election variables that are not already 1/0 binary to binary format.\nCreate a new column named civ_engage that sums up each person’s voting behavior up to, but not including, the 2006 primary.\nCreate a column named voter_class that classifies voters into 3 bins: “Always Vote” for those who voted at least 5 times, “Sometimes Vote” for those who voted between 3 or 4 times, and “Rarely Vote” for those who voted 2 or fewer times. This variable should be classified as a factor.\nCreate a column called z_age which is the z-score for age.\n\n\nobject_1 <- shaming |> \n  \n  # Converting the Y/N columns to binaries with the function we made \n  # note that primary_06 is already binary and also that we don't \n  # need it to predict construct previous voter behavior status variable.\n  \n  mutate(p_00 = (primary_00 == \"Yes\"),\n         p_02 = (primary_02 == \"Yes\"),\n         p_04 = (primary_04 == \"Yes\"),\n         g_00 = (general_00 == \"Yes\"),\n         g_02 = (general_02 == \"Yes\"),\n         g_04 = (general_04 == \"Yes\")) |> \n  \n  # A sum of the voting action records across the election cycle columns gives\n  # us an idea (though not weighted for when across the elections) of the voters\n  # general level of civic involvement.\n  \n  mutate(civ_engage = p_00 + p_02 + p_04 + \n                      g_00 + g_02 + g_04) |> \n  \n  # If you look closely at the data, you will note that g_04 is always Yes, so\n  # the lowest possible value of civ_engage is 1. The reason for this is that\n  # the sample was created by starting with a list of everyone who voted in the\n  # 2004 general election. Note how that fact makes the interpretation of the\n  # relevant population somewhat subtle.\n  \n  mutate(voter_class = case_when(civ_engage %in% c(5, 6) ~ \"Always Vote\",\n                                 civ_engage %in% c(3, 4) ~ \"Sometimes Vote\",\n                                 civ_engage %in% c(1, 2) ~ \"Rarely Vote\"),\n         voter_class = factor(voter_class, levels = c(\"Rarely Vote\", \n                                                      \"Sometimes Vote\", \n                                                      \"Always Vote\"))) |> \n  \n  # Centering and scaling the age variable. Note that it would be smart to have\n  # some stopifnot() error checks at this point. For example, if civ_engage < 1\n  # or > 6, then something has gone very wrong.\n  \n  mutate(z_age = as.numeric(scale(age))) |> \n  select(primary_06, treatment, sex, civ_engage, voter_class, z_age)\n\nLet’s inspect our object:\n\nobject_1 |> \n  slice(1:3)\n\n# A tibble: 3 × 6\n  primary_06 treatment  sex    civ_engage voter_class    z_age\n       <int> <fct>      <chr>       <int> <fct>          <dbl>\n1          0 Civic Duty Male            4 Sometimes Vote 1.05 \n2          0 Civic Duty Female          4 Sometimes Vote 0.638\n3          1 Hawthorne  Male            4 Sometimes Vote 0.361\n\n\nGreat! Now, we will create our first model: the relationship between primary_06, which represents whether a citizen voted or not, against sex and treatment.\n\n11.3.2 primary_06 ~ treatment + sex\nIn this section, we will look at the relationship between primary voting and treatment + sex.\nThe math:\nWithout variable names:\n\\[ y_{i} = \\beta_{0} + \\beta_{1}x_{i, 1} + \\beta_{2}x_{i,2} ... + \\beta_{n}x_{i,n} + \\epsilon_{i} \\]\nWith variable names:\n\\[ y_{i} = \\beta_{0} + \\beta_{1}civic\\_duty_i + \\beta_{2}hawthorne_i + \\beta_{3}self_i + \\beta_{4}neighbors_i + \\beta_{5}male_i + \\epsilon_{i} \\]\nThere are two ways to formalize the model used in fit_1: with and without the variable names. The former is related to the concept of Justice as we acknowledge that the model is constructed via the linear sum of n parameters times the value for n variables, along with an error term. In other words, it is a linear model. The only other model we have learned this semester is a logistic model, but there are other kinds of models, each defined by the mathematics and the assumptions about the error term. The second type of formal notation, more associated with the virtue Courage, includes the actual variable names we are using. The trickiest part is the transformation of character/factor variables into indicator variables, meaning variables with 0/1 values. Because treatment has 5 levels, we need 4 indicator variables. The fifth level — which, by default, is the first variable alphabetically (for character variables) or the first level (for factor variables) — is incorporated in the intercept.\nLet’s translate the model into code.\n\nfit_1 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ treatment + sex,\n                  refresh = 0,\n                  seed = 987)\n\n\nprint(fit_1, digits = 3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      primary_06 ~ treatment + sex\n observations: 344084\n predictors:   6\n------\n                    Median MAD_SD\n(Intercept)         0.291  0.001 \ntreatmentCivic Duty 0.018  0.003 \ntreatmentHawthorne  0.026  0.003 \ntreatmentSelf       0.048  0.002 \ntreatmentNeighbors  0.081  0.003 \nsexMale             0.012  0.002 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.464  0.001 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nWe will now create a table that nicely formats the results of fit_1 using the tbl_regression() function from the gtsummary package. It will also display the associated 95% confidence interval for each coefficient.\n\ntbl_regression(fit_1, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |>\n  \n  # Using Beta as the name of the parameter column is weird.\n  \n  as_gt() |>\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Age Predict Likelihood of Voting\") |>\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |> \n  cols_label(estimate = md(\"**Parameter**\"))\n\n\n\n\n\n\n\nLikelihood of Voting in the Next Election\n    \n\nHow Treatment Assignment and Age Predict Likelihood of Voting\n    \n\n\nCharacteristic\n      Parameter\n      \n95% CI1\n\n    \n\n\n(Intercept)\n0.291\n0.288, 0.293\n\n\ntreatment\n\n\n\n\nNo Postcard\n—\n—\n\n\nCivic Duty\n0.018\n0.013, 0.023\n\n\nHawthorne\n0.026\n0.021, 0.031\n\n\nSelf\n0.048\n0.044, 0.054\n\n\nNeighbors\n0.081\n0.076, 0.087\n\n\nsex\n\n\n\n\nFemale\n—\n—\n\n\nMale\n0.012\n0.009, 0.015\n\n\n\nSource: Gerber, Green, and Larimer (2008)\n    \n\n\n1 CI = Credible Interval\n    \n\n\n\n\nInterpretation: * The intercept of this model is the expected value of the probability of someone voting in the 2006 primary given that they are part of the control group and are female. In this case, we estimate that women in the control group will vote ~29.1% of the time. * The coefficient for sexMale indicates the difference in likelihood of voting between a male and female. In other words, when comparing men and women, the 0.012 implies that men are ~1.2% more likely to vote than women. Note that, because this is a linear model with no interactions between sex and other variables, this difference applies to any male, regardless of the treatment he received. Because sex can not be manipulated (by assumption), we should not use a causal interpretation of the coefficient. * The coefficients of the treatments, on the other hand, do have a causal interpretation. For a single individual, of either sex, being sent the Self postcard increases your probability of voting by 4.8%. It appears that the Neighbors treatment is the most effective at ~8.1% and Civic Duty is the least effective at ~1.8%.\n\n11.3.3 primary_06 ~ z_age + sex + treatment + voter_class + voter_class*treatment\nIt is time to look at interactions! Create another model named fit_2 that estimates primary_06 as a function of z_age, sex, treatment, voter_class, and the interaction between treatment and voter classification.\nThe math:\n\\[y_{i} = \\beta_{0} + \\beta_{1}z\\_age + \\beta_{2}male_i + \\beta_{3}civic\\_duty_i + \\\\ \\beta_{4}hawthorne_i + \\beta_{5}self_i + \\beta_{6}neighbors_i + \\\\ \\beta_{7}Sometimes\\ vote_i + \\beta_{8}Always\\ vote_i + \\\\ \\beta_{9}civic\\_duty_i Sometimes\\ vote_i + \\beta_{10}hawthorne_i Sometimes\\ vote_i + \\\\ \\beta_{11}self_i Sometimes\\ vote_i + \\beta_{11}neighbors_i Sometimes\\ vote_i + \\\\ \\beta_{12}civic\\_duty_i Always\\ vote_i + \\beta_{13}hawthorne_i Always\\ vote_i + \\\\ \\beta_{14}self_i Always\\ vote_i + \\beta_{15}neighbors_i Always\\ vote_i + \\epsilon_{i}\\]\nTranslate into code:\n\nfit_2 <- stan_glm(data = object_1,\n                  formula = primary_06 ~ z_age + sex + treatment + voter_class + \n                            treatment*voter_class,\n                  family = gaussian,\n                  refresh = 0,\n                  seed = 789)\n\n\nprint(fit_2, digits = 3)\n\nstan_glm\n family:       gaussian [identity]\n formula:      primary_06 ~ z_age + sex + treatment + voter_class + treatment * \n       voter_class\n observations: 344084\n predictors:   17\n------\n                                              Median MAD_SD\n(Intercept)                                    0.153  0.003\nz_age                                          0.035  0.001\nsexMale                                        0.008  0.002\ntreatmentCivic Duty                            0.009  0.006\ntreatmentHawthorne                             0.008  0.007\ntreatmentSelf                                  0.023  0.007\ntreatmentNeighbors                             0.044  0.007\nvoter_classSometimes Vote                      0.114  0.003\nvoter_classAlways Vote                         0.294  0.004\ntreatmentCivic Duty:voter_classSometimes Vote  0.014  0.007\ntreatmentHawthorne:voter_classSometimes Vote   0.019  0.008\ntreatmentSelf:voter_classSometimes Vote        0.030  0.008\ntreatmentNeighbors:voter_classSometimes Vote   0.042  0.007\ntreatmentCivic Duty:voter_classAlways Vote    -0.001  0.008\ntreatmentHawthorne:voter_classAlways Vote      0.025  0.008\ntreatmentSelf:voter_classAlways Vote           0.025  0.008\ntreatmentNeighbors:voter_classAlways Vote      0.046  0.009\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.451  0.001 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nAs we did with our first model, create a regression table to observe our findings:\n\ntbl_regression(fit_2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 3)) |>\n  as_gt() |>\n  tab_header(title = md(\"**Likelihood of Voting in the Next Election**\"),\n             subtitle = \"How Treatment Assignment and Other Variables Predict Likelihood of Voting\") |>\n  tab_source_note(md(\"Source: Gerber, Green, and Larimer (2008)\")) |> \n  cols_label(estimate = md(\"**Parameter**\"))\n\n\n\n\n\n\n\nLikelihood of Voting in the Next Election\n    \n\nHow Treatment Assignment and Other Variables Predict Likelihood of Voting\n    \n\n\nCharacteristic\n      Parameter\n      \n95% CI1\n\n    \n\n\n(Intercept)\n0.153\n0.147, 0.159\n\n\nz_age\n0.035\n0.033, 0.037\n\n\nsex\n\n\n\n\nFemale\n—\n—\n\n\nMale\n0.008\n0.004, 0.011\n\n\ntreatment\n\n\n\n\nNo Postcard\n—\n—\n\n\nCivic Duty\n0.009\n-0.003, 0.022\n\n\nHawthorne\n0.008\n-0.006, 0.021\n\n\nSelf\n0.023\n0.010, 0.037\n\n\nNeighbors\n0.044\n0.031, 0.057\n\n\nvoter_class\n\n\n\n\nRarely Vote\n—\n—\n\n\nSometimes Vote\n0.114\n0.108, 0.120\n\n\nAlways Vote\n0.294\n0.287, 0.301\n\n\ntreatment * voter_class\n\n\n\n\nCivic Duty * Sometimes Vote\n0.014\n0.001, 0.028\n\n\nHawthorne * Sometimes Vote\n0.019\n0.004, 0.033\n\n\nSelf * Sometimes Vote\n0.030\n0.016, 0.045\n\n\nNeighbors * Sometimes Vote\n0.042\n0.028, 0.057\n\n\nCivic Duty * Always Vote\n-0.001\n-0.016, 0.015\n\n\nHawthorne * Always Vote\n0.025\n0.008, 0.042\n\n\nSelf * Always Vote\n0.025\n0.009, 0.042\n\n\nNeighbors * Always Vote\n0.046\n0.030, 0.063\n\n\n\nSource: Gerber, Green, and Larimer (2008)\n    \n\n\n1 CI = Credible Interval\n    \n\n\n\n\nNow that we have a summarized visual for our data, let’s interpret the findings: * The intercept of fit_2 is the expected probability of voting in the upcoming election for a woman of average age (~ 50 years old in this data), who is assigned to the No Postcard group, and is a Rarely Voter. The estimate is 15.3%. * The coefficient of z_age, 0, implies a change of ~3.5% in likelihood of voting for each increment of one standard deviation (~ 14.45 years). For example: when comparing someone 50 years old with someone 65, the latter is about 3.5% more likely to vote. * Exposure to the Neighbors treatment shows a ~4.4% increase in voting likelihood for someone in the Rarely Vote category. Because of random assignment of treatment, we can interpret that coefficient as an estimate of the average treatment effect. * If someone were from a different voter classification, the calculation is more complex because we need to account for the interaction term. For example, for individuals who Sometimes Vote, the treatment effect of Neighbors is 0.1%. For Always Vote Neighbors, it is 0.1%."
  },
  {
    "objectID": "11-n-parameters.html#temperance",
    "href": "11-n-parameters.html#temperance",
    "title": "11  N Parameters",
    "section": "\n11.4 Temperance",
    "text": "11.4 Temperance\n\n\n\n\nTemperance\n\n\n\n\nFinally, let’s remember the virtue of Temperance. The gist of temperance is: be humble with our inferences, as our inferences are always, certainly, and unfortunately not going to match the real world. How does this apply to our shaming scenario?\nRecall our initial question: What is the causal effect, on the likelihood of voting, of different postcards on voters of different levels of political engagement?\nTo answer the question, we want to look at different average treatment effects for each treatment and type of voting behavior. In the real world, the treatment effect for person A is almost always different than the treatment effect for person B.\nIn this section, we will create a plot that displays the posterior probability distributions of the average treatment effects for men of average age across all combinations of 4 treatments and 3 voter classifications. This means that we are making a total of 12 inferences.\nImportant note: We could look at lots of ages and both Male and Female subjects. However, that would not change our estimates of the treatment effects. The model is linear, so terms associated with z_age and sex disappear when we do the subtraction. This is one of the great advantages of linear models.\nTo begin, we will need to create our newobs object.\n\n# Because our model is linear, the terms associated with z_age and sex disappear\n# when we perform subtraction. The treatment effects calculated thereafter will\n# not only apply to males of the z-scored age of ~ 50 years. The treatment\n# effects apply to all participants, despite calling these inputs.\n\n\nsex <- \"Male\"\nz_age <- 0\ntreatment <- c(\"No Postcard\",\n               \"Civic Duty\",\n               \"Hawthorne\",\n               \"Self\",\n               \"Neighbors\")\nvoter_class <- c(\"Always Vote\",\n                 \"Sometimes Vote\",\n                 \"Rarely Vote\")\n\n# This question requires quite the complicated tibble! Speaking both\n# hypothetically and from experience, keeping track of loads of nondescript\n# column names after running posterior_epred() while doing ATE calculations\n# leaves you prone to simple, but critical, errors. expand_grid() was created\n# for cases just like this - we want all combinations of treatments and voter\n# classifications in the same way that our model displays the interaction term\n# parameters.\n\nnewobs <- expand_grid(sex, z_age, treatment, voter_class) |> \n  \n  # This is a handy setup for the following piece of code that allows us to\n  # mutate the ATE columns with self-contained variable names. This is what\n  # helps to ensure that the desired calculations are indeed being done. If you\n  # aren't familiar, check out the help page for paste() at `?paste`.\n  \n  mutate(names = paste(treatment, voter_class, sep = \"_\"))\n\npe <- posterior_epred(fit_2,\n                        newdata = newobs) |> \n  as_tibble() |> \n  \n  # Here we can stick the names that we created in newobs onto the otherwise\n  # unfortunately named posterior_epred() output. \n  \n  set_names(newobs$names)\n\nNow that we have our newobs to work with, we will need to create an object named plot_data that collects the treatment effect calculations.\nRecall that, when calculating a treatment effect, we need to subtract the estimate for each category from the control group for that category. For example, if we wanted to find the treatment effect for the Always Vote Neighbors group, we would need: Always Vote Neighbors - Always Vote No Postcard.\nTherefore, we will use mutate() twelve times, for each of the treatments and voting frequencies. After, we will pivot_longer in order for the treatment effects to be sensibly categorized for plotting. If any of this sounds confusing, read the code comments carefully.\n\nplot_data <- pe |> \n  \n  # Using our cleaned naming system, ATE calculations are simple enough. Note\n  # how much easier the code reads because we have taken the trouble to line up\n  # the columns.\n  \n  mutate(`Always Civic-Duty`    = `Civic Duty_Always Vote`     - `No Postcard_Always Vote`,\n         `Always Hawthorne`     = `Hawthorne_Always Vote`      - `No Postcard_Always Vote`,\n         `Always Self`          = `Self_Always Vote`           - `No Postcard_Always Vote`,\n         `Always Neighbors`     = `Neighbors_Always Vote`      - `No Postcard_Always Vote`,\n         `Sometimes Civic-Duty` = `Civic Duty_Sometimes Vote`  - `No Postcard_Sometimes Vote`,\n         `Sometimes Hawthorne`  = `Hawthorne_Sometimes Vote`   - `No Postcard_Sometimes Vote`,\n         `Sometimes Self`       = `Self_Sometimes Vote`        - `No Postcard_Sometimes Vote`,\n         `Sometimes Neighbors`  = `Neighbors_Sometimes Vote`   - `No Postcard_Sometimes Vote`,\n         `Rarely Civic-Duty`    = `Civic Duty_Rarely Vote`     - `No Postcard_Rarely Vote`,\n         `Rarely Hawthorne`     = `Hawthorne_Rarely Vote`      - `No Postcard_Rarely Vote`,\n         `Rarely Self`          = `Self_Rarely Vote`           - `No Postcard_Rarely Vote`,\n         `Rarely Neighbors`     = `Neighbors_Rarely Vote`      - `No Postcard_Rarely Vote`) |> \n  \n  # This is a critical step, we need to be able to reference voter\n  # classification separately from the treatment assignment, so pivoting in the\n  # following manner reconstructs the relevant columns for each of these\n  # individually. \n  \n  pivot_longer(names_to = c(\"Voter Class\", \"Group\"),\n               names_sep = \" \",\n               values_to = \"values\",\n               cols = `Always Civic-Duty`:`Rarely Neighbors`) |> \n  \n    # Reordering the factors of voter classification forces them to be displayed\n    # in a sensible order in the plot later.\n  \n    mutate(`Voter Class` = fct_relevel(factor(`Voter Class`),\n                                     c(\"Rarely\",\n                                       \"Sometimes\",\n                                       \"Always\")))\n\nFinally, we will plot our data! Read the code comments for explanations on aesthetic choices, as well as a helpful discussion on fct_reorder().\n\nplot_data  |> \n  \n  # Reordering the y axis values allows a smoother visual interpretation - \n  # you can see the treatments in sequential ATE.\n  \n  ggplot(aes(x = values, y = fct_reorder(Group, values))) +\n  \n  # position = \"dodge\" is the only sure way to see all 3 treatment distributions\n  # identity, single, or any others drop \"Sometimes\" - topic for further study\n  \n    stat_slab(aes(fill = `Voter Class`),\n              position = 'dodge') +\n    scale_fill_calc() +\n  \n    # more frequent breaks on the x-axis provides a better reader interpretation\n    # of the the shift across age groups, as opposed to intervals of 10%\n    \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1),\n                       breaks = seq(-0.05, 0.11, 0.01)) +\n    labs(title = \"Treatment Effects on The Probability of Voting\",\n         subtitle = \"Postcards work less well on those who rarely vote\",\n         y = \"Postcard Type\",\n         x = \"Average Treatment Effect\",\n         caption = \"Source: Gerber, Green, and Larimer (2008)\") +\n    theme_clean() +\n    theme(legend.position = \"bottom\")\n\n\n\n\nThis is interesting! It shows us a few valuable bits of information:\n\nWe are interested in the average treatment effect of postcards. There are 4 different postcards, each of which can be compared to what would have happened if the voter did not receive any postcard.\nThese four treatment effects, however, are heterogeneous. They vary depending on an individual’s voting history, which we organize into three categories: Rarely Vote, Sometimes Vote and Always Vote. So, we have 12 different average treatment effects, one for each possible combination of postcard and voting history.\nFor each of these combinations, the graphic shows our posterior distribution.\n\nWhat does this mean for us, as we consider which postcards to send? * Consider the highest yellow distribution, which is the posterior distribution for the average treatment effect of receiving the Neighbors postcard (compared to not getting a postcard) for Always Voters. The posterior is centered around 9% with a 95% confidence interval of, roughly, 8% to 10%. * Overall, the Civic Duty and Hawthorne postcards had small average treatment effects, across all three categories of voter. The causal effect on Rarely Voters was much smaller, regardless of treatment. It was also much less precisely estimated because there were many fewer Rarely Voters in the data. *The best way to increase turnover, assuming there are limits to how many postcards you can send, is to focus on Sometimes/Always voters and to use the Neighbors postcard.\nConclusion: If we had a limited number of postcards, we would send the Neighbors postcard to citizens who already demonstrate a tendency to vote.\nHow confident are we in these findings? If we needed to convince our boss that this is the right strategy, we need to explain how confident we are in our assumptions. To do that, we must understand the three levels of knowledge in the world of posteriors.\n\n11.4.1 The Three Levels of Knowledge\nThere exist three primary levels of knowledge possible knowledge in our scenario: the Truth (the ideal Preceptor Table), the DGM Posterior, and Our Posterior.\n\n11.4.1.1 The Truth\nIf we know the Truth (with a capital “T”), then we know the ideal Preceptor Table. With that knowledge, we can directly answer our question precisely. We can calculate each individual’s treatment effect, and any summary measure we might be interested in, like the average treatment effect.\nThis level of knowledge is possible only under an omniscient power, one who can see every outcome in every individual under every treatment. The Truth would show, for any given individual, their actions under control, their actions under treatment, and each little factor that impacted those decisions.\nThe Truth represents the highest level of knowledge one can have — with it, our questions merely require algebra. There is no need to estimate a treatment effect, or the different treatment effects for different groups of people. We would not need to predict at all — we would know.\n\n11.4.1.2 DGM posterior\nThe DGM posterior is the next level of knowledge, which lacks the omniscient quality of The Truth. This posterior is the posterior we would calculate if we had perfect knowledge of the data generating mechanism, meaning we have the correct model structure and exact parameter values. This is often falsely conflated with “Our posterior”, which is subject to error in model structure and parameter value estimations.\nWith the DGM posterior, we could not be certain about any individual’s causal effect, because of the Fundamental Problem of Causal Inference. In other words, we can never measure any one person’s causal effect because we are unable to see a person’s resulting behavior under treatment and control; we only have data on one of the two conditions.\nWhat we do with the DGM posterior is the same as Our posterior — we estimate parameters based on data and predict the future with the latest and most relevant information possible. The difference is that, when we calculate posteriors for an unknown value in the DGM posterior, we expect those posteriors to be perfect.\nIf we go to our boss with our estimates from this posterior, we would expect our 95% confidence interval to be perfectly calibrated. That is, we would expect the true value to lie within the 95% confidence interval 95% of the time. In this world, we would be surprised to see values outside of the confidence interval more than 5% of the time.\n\n11.4.1.3 Our posterior\nUnfortunately, Our posterior possesses even less certainty! In the real world, we don’t have perfect knowledge of the DGM: the model structure and the exact parameter values. What does this mean?\nWhen we go to our boss, we tell them that this is our best guess. It is an informed estimate based on the most relevant data possible. From that data, we have created a 95% confidence interval for the treatment effect of various postcards. We estimate that the treatment effect of the Neighbors postcard to be between 8% to 10%.\nDoes this mean we are certain that the treatment effect of Neighbors is between these values? Of course not! As we would tell our boss, it would not be shocking to find out that the actual treatment effect was less or more than our estimate.\nThis is because a lot of the assumptions we make during the process of building a model, the processes in Wisdom, are subject to error. Perhaps our data did not match the future as well as we had hoped. Ultimately, we try to account for our uncertainty in our estimates. Even with this safeguard, we aren’t surprised if we are a bit off.\nFor instance, would we be shocked if the treatment effect of the Neighbors postcard to be 7%? 12%? Of course not! That is only slightly off, and we know that Our posterior is subject to error. Would we be surprised if the treatment effect was found to be 20%? Yes. That is a large enough difference to suggest a real problem with our model, or some real world change that we forgot to factor into our predictions.\nBut, what amounts to a large enough difference to be a cause for concern? In other words, how wrong do we have to be in a one-off for our boss to be suspicious? When is “bad luck” a sign of stupidity? We will delve into this question in the next section of our chapter.\n\n11.4.1.4 Bad luck or bad work?\nIn any one problem, it is hard to know if we were “right,” if our posterior was similar to the DGM posterior. After all, 5% of the time the answer is outside the 95% confidence interval. But if the truth ends up very, very far away from the median of our posterior, our boss will be rightly suspicious. How many MAD SDs or standard errors away do we have to be from the truth before we are obviously a fool?\nThere are many ways to judge a forecast. Here, we’re looking at two main things: the calibration of a forecast — that is, whether events that we said would happen 30 percent of the time actually happened about 30 percent of the time — and how our forecast compared with an unskilled estimate that relies solely on historical averages. We can answer those questions using calibration plots and skill scores, respectively. These concepts are a bit too advanced for this course, but their foundations are important to understand.\nCalibration plots compare what we predicted with what actually happened. Single predictions can be difficult to judge on their own, so we often want to group many predictions together in bins and plot the averages of each bin’s forecasted increase in voting against the actual increase in voting. If our forecasts are well-calibrated, then all of the bins on the calibration plot will be close to the 45 degree line; if our forecast was poorly calibrated, the bins will be further away. Our second tool, skill scores, lets us evaluate our forecasts even further, combining accuracy and an appetite for risk into a single number.\nBrier skill scores tell us how much more valuable our forecasts are than an unskilled estimate, one that is informed by historical averages — e.g., a guess that a postcard will increase voting by 5%.\nThese are the technical ways that we can judge our own work’s accuracy. Our boss will likely judge using other methods.\nFor instance, if we answer many questions (by creating many posteriors for different problems) then, over time, our boss will get a sense of our actual skill, both because our median should be above and below the truth about the same proportion and because our confidence intervals should be correctly calibrated.\nWe know, from experience, that our posteriors are often too narrow. They assume that we know the DGM when, in fact, we know that we do not. What do we do with that knowledge? First, we prepare our boss for this fact. This is the humility in Temperance. Second, we estimate dozens of different models and combine their posteriors. The result might very well have the same median as your correct posterior, but the confidence intervals would be much wider. These concepts are more advanced than the Primer, but they are important to consider when making predictions."
  },
  {
    "objectID": "11-n-parameters.html#summary",
    "href": "11-n-parameters.html#summary",
    "title": "11  N Parameters",
    "section": "\n11.5 Summary",
    "text": "11.5 Summary\n\nUse the tidy() function from the broom.mixed package to make models with \\(N\\) parameters easier to interpret.\nA function we are familiar with, stan_glm(), is used to create models with \\(N\\) parameters.\nIt is important to remember that the data does not equal the truth.\nThe population we would like to make inferences about is not the population for which we have data. It is a matter of wisdom whether the data we do have maps closely enough to the population we are studying.\nWhen dealing with models with many parameters, double check that you know how to find the true slope and intercepts — often, this requires adding numerous values to the coefficient you are studying."
  },
  {
    "objectID": "12-case-studies.html",
    "href": "12-case-studies.html",
    "title": "12  Case Studies",
    "section": "",
    "text": "You master data science by studying the work of experienced professionals. Here are some examples."
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Tools",
    "section": "",
    "text": "The chapter will be broken into the following sections. Read whichever ones are relevant."
  },
  {
    "objectID": "tools.html#absolute-and-relative-file-paths",
    "href": "tools.html#absolute-and-relative-file-paths",
    "title": "Tools",
    "section": "Absolute and relative file paths",
    "text": "Absolute and relative file paths\n\n\nWhen you read data into R, you first need to tell R where that data lives. Most times, the data will be in a file. The file could live on your computer (local) or somewhere on the internet (remote).\nThe place where the file lives on your computer is called the “path”. You can think of the path as directions to the file. The path includes both the location of the file and the name of the file itself. There are two kinds of paths: relative and absolute. A relative path describes the location of the file relative to where you currently are on the computer. An absolute path is where the file is in respect to the base (or root) folder of the computer’s filesystem. Absolute paths always start with a forward slash, a “/”.\nConsider a file called report.csv. Read the file using a relative path:\nx <- read_csv(\"data/report.csv\")\nRead report.csv using an absolute path:\nx <- read_csv(\"/home/preceptor/desktop/projects/data/report.csv\")\nTo ensure your code can be run on a different computer, you should use relative paths. An added bonus is that it’s also less typing! This is because the absolute path of a file (the names of folders between the computer’s root / and the file) isn’t usually the same across different computers. For example, suppose Fatima and Jayden are working on a project together on the report.csv data. Fatima’s file is stored at\n/home/Fatima/files/report.csv,\nwhile Jayden’s is stored at\n/home/Jayden/files/report.csv.\nEven though Fatima and Jayden stored their files in the same place on their computers, the absolute paths are different due to their different usernames. If Jayden has code that loads the report.csv data using an absolute path, the code won’t work on Fatima’s computer. But the relative path from inside the files folder (files/report.csv) is the same on both computers; any code that uses relative paths will work on both!\n\nOne important part of using paths is recognizing when there are spaces in the file name. For example, trying to use the path\n/home/preceptor/desktop/projects/important data/report.csv\ndoes not work because it includes a space the file name, with “important data”. Instead, the path\n/home/preceptor/desktop/projects/important\\ data/report.csv\nis valid and allows you to access the report.csv data. This is because the \\ tells the computer to treat the next character only as a character instead of something special. For example, the space character is normally used to show a break in the code, so the computer treats it as a break point whenever it sees a space. However, \\ tells the computer that the space isn’t a break point and is instead part of the file path, solving the issue.\n\nSee this video for another explanation:\n\n\nSource: Udacity course “Linux Command Line Basics”"
  },
  {
    "objectID": "tools.html#working-with-the-terminal",
    "href": "tools.html#working-with-the-terminal",
    "title": "Tools",
    "section": "Working with the terminal",
    "text": "Working with the terminal\n\n\nThe Terminal is a very powerful window because it allows you to interact with your computer’s filesystem directly, and it uses file paths to find the files that you want to interact with.\nLet’s open up the Terminal tab on the left window and start learning how to use the Terminal.\n\npwd: Working directory\nThe first question you may have about working in the Terminal might be: If I can’t see the folders, how do I know where I am? Well that’s a great place to start learning the Terminal. To see what our current folder is, we type pwd (print working directory):\n\n\n\n\n\nWe are currently in a directory (or folder) called Yao, which is itself in a directory named /Users. The forward slash in front of “Users” tells is that this directory is at the lowest possible level.\n\nls: Seeing items in the directory\nTo see the items in our current folder, we use the command ls (list). Type ls in the terminal and hit return/enter. You should see something like this:\n\n\n\n\n\nNotice that this lists exactly the same items as the bottom right window in RStudio. The Terminal is just another way to interact with your computer’s filesystem. Anything you can do normally with your mouse/trackpad, like opening a folder, you can also do in the Terminal.\n\n\n\n\n\n\ncd: Changing directories\nTo move from one directory to another, we use cd (change directory). We’ll be using cd to change into the Desktop folder.\nTo change into the Desktop directory, we type cd Desktop/. A helpful hint, after you type the first few letters of a folder or file name, you can hit tab and the computer will auto complete the name. Try it! Type cd Desk and then hit tab to auto complete the name!\n\n\n\n\n\nIf you type ls again, you can see all the item on your Desktop listed.\nTo go back to the previous folder (aka the directory above), we can type cd .. The two periods represent one level above. You can see this hierarchy in this view on a Mac:\n\n\n\n\n\n\nmkdir and rmdir: Make and remove a directory\nNow that we’re in the Desktop folder, let’s get set-up to stay organized for Gov 1005. Staying organized is critical when working with many data projects. So, using mkdir Gov-1005 (make directory) we can create a folder exclusively for Gov 1005 like so:\n\n\n\n\n\nNow, when we type ls, we can see our new folder created. Note that we used a hyphen between Gov and 1005. This is because the Terminal can’t recognize spaces unless you put \\ before it, like so: mkdir Gov\\ 1005. Never use spaces or other weird characters in file or directory names.\nTo remove a folder, use rmdir (remove directory). We won’t be using this right now because we don’t need to remove anything.\n\ntouch: Creating files\nIn order to experiment with the next few commands in the Terminal, we’ll need a test file. Type touch text.txt to create a test file.\n\n\n\n\n\nAnd, of course, we can see that the test.txt file has been created using ls.\n\nmv: Moving files\nOh no! We created our test.txt file, but it should be in our Gov-1005 folder, right now it’s on the desktop. This happened because while we created the Gov-1005 folder using mkdir, we forgot to move into it by using cd Gov-1005/. But no worries, we can move the file to that folder using mv:\n\n\n\n\n\nWhen using mv the first thing you type after mv is the file you want to move. The next thing is the location where you want to move it to. In our case we want to move test.txt to Gov-1005/, so we type mv test.txt Gov-1005/. After we do this, we can use cd to enter the Gov-1005 folder and then use ls to see that our test.txt file successfully was moved into the Gov-1005 directory.\n\n\n\n\n\n\ncp: Copying files\nCopying files is very similar to moving files in the Terminal. Using the previous example, if we wanted to copy test.txt into the Gov-1005 folder but not delete the original test.txt file, we just replace mv with cp (copy paste):\n\ncp test.txt Gov-1005/\n\n\nrm: Removing files\nOk, we are at the last Terminal command that this book will be teaching you. So, we’re done with this test.txt file. Let’s remove it with rm (remove):\n\n\n\n\n\nMake sure you are the in Gov-1005 folder before you type rm test.txt! Using ls, we can see that our test file is now gone.\nCongrats! You are now able to do most basic tasks with the Terminal! If you want to learn more Terminal commands, check out Sean Kross’s The Unix Workbench."
  },
  {
    "objectID": "tools.html#git-github-and-rstudio",
    "href": "tools.html#git-github-and-rstudio",
    "title": "Tools",
    "section": "Git, GitHub, and RStudio",
    "text": "Git, GitHub, and RStudio\nThis next section focuses on connecting GitHub with RStudio using Git. Why do we care about GitHub? Think of it as a Google Drive for all your R code and projects. If your computer blows up, GitHub will save all your R work just as Google Drive saves your paper.\nInstalling Git\nThe first step to using GitHub is installing Git on your computer. But first, you may already have Git installed on your computer. To check, go to your Terminal and type git --version. If you already have Git, this command will return the Git version you have installed. If you get an error, you can download and install git here.\nGitHub accounts\n\nAfter installing Git, you’ll need a GitHub account. This is like a Google account. However, the one difference with GitHub is that your account is visible to the public. You want to pick a name carefully. It should be professional since you will be sending potential employers a link to your GitHub account in the near future. Check out some former Gov 1005 students’ GitHub profiles for inspiration:\n\nEvelyn Cai\nJessica Edwards\nBeau Meche\n\nOnce you have a GitHub account, you are ready to connect Git and RStudio to this account. Type the following two commands in your Terminal pane. Replace Your Name with your name and your@email.com with the email you used to sign up for GitHub.\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your@email.com\"\nGitHub repositories\nYou are now ready to create a GitHub repository (repo). The GitHub repo is similar to a Google Drive folder. To make your first repo, make sure you are signed in and then go to the GitHub homepage and click on the green new button on the left.\n\n\n\n\n\n\n\n\nYou will then want to choose a good name for the repo and add a brief description. Here we will use productivity. You can choose to make the repo public or private, but we recommend that you only make a repo public if it is important for the world to see. This keeps your public GitHub profile clean and professional. This repo should probably be private. Let’s also add a README file for our repo. This is a document where you can add more information.\n\n\n\n\n\nYou now have your first repo on GitHub. The next step will be to download it to your computer — a process often known as “cloning” — and start editing and syncing using Git. To do this, we’ll need to copy the link to the repo and then use it in RStudio. Once again, the green button is your friend. Click on it and then copy the link shown. You can use the clipboard button on the right to automatically copy it.\n\n\n\n\n\nConnecting GitHub to RStudio\nWe are now ready to connect your productivity repo to RStudio. With the link to the productivity repo copied, we can go back to RStudio and begin with a new project. Go to File, then New Project:\n\n\n\n\n\nNext, you’ll need to go through these steps to create the project: Version Control to Git to paste your link from GitHub and click Create Project.\n\n\n\n\n\nCongrats! You’ve linked your productivity repo to RStudio. Note that Github will ask you for a location in which to place this and other projects. We recommend creating a folder on your desktop called “projects” and placing all your RStudio projects there. Don’t just scatter them across your computer in a mess. There will be dozens of them. Be organized!\n\nUpdating .gitignore\n\nThe first thing you should always do when working with a new repo is updating the .gitignore file. You can open this file from the bottom right window under the Files tab. This file includes all the files that you don’t want to be uploaded to GitHub. This can come in handy when you are working with big datasets or files with private information. In our case, we want to add the productivityl.Rproj file to the .gitignore list.\n\n\n\n\n\nThis file is your private project file and usually you don’t want this uploaded to GitHub. So, in .gitignore, you’ll want to add *.Rproj The * tells your computer that we want to prevent all files ending in .Rproj from being uploaded. We could also just add productivity.Rproj.\n\n\n\n\n\nSave the .gitignore file and you should see the productivity.Rproj file disappear from your Git tab in the top right window. If you don’t see any changes, click on the refresh button in the upper left.\nThe symbols in the Git tab are part a “conversation” between you and Git. The “?” is Git’s way of saying: “There is a new file here. What do you want to do with it?” Adding a line to the .gitignore is your way of replying “Ignore that file.”\n\n\n\n\n\nCommit and Push\nNow that we’ve updated our .gitignore file, we want to upload this new version to GitHub. To do this, first select the .gitignore file and then click on the Commit button in the Git window:\n\n\n\n\n\nThis will open a new window where you will write a commit message. This message is a very short note on what you’re adding/changing in the repo. In our case, we’ve updated the .gitignore so let’s write just that:\n\n\n\n\n\nPress commit. This is your way of telling Git “Yes these are the files I want to upload. I’m committed.” Next, press Push. This pushes or uploads the files to GitHub. (You can probably guess what pull does, but we won’t be using that yet)\nNow, if you go to your GitHub repo and refresh the page, you can see that the .gitignore file has been uploaded with your commit message:\n\n\n\n\n\nCongrats! You just uploaded your first file to GitHub.\nOne tricky aspect is the caching of your Github ID and password. Most likely, you had to type these things in when you did your first push. Doing so was not too bad. And, after all, Github needs to know who you are, otherwise other people could mess with your repo. But you will be doing hundreds of commits/pushes. You don’t want to type in your ID/password each time! Follow these instructions. Key steps:\n\nTurn on two-factor authentication for your GitHub account under Settings -> Security.\nCreate a token:\n\n\nusethis::create_github_token()\n\nThis will, after logging in, bring you back to Github. Accept the defaults and press the Generate token button at the bottom. (You may need to change the Note if you have generated tokens before.) Copy the token which has been created. It will look something like:\n8be3e800891425f8462c4491d9a4dbb5b1c1f35c\nThen, issue this R command:\n\ngitcreds::gitcreds_set()\n\nProvide your token. After you start a new RStudio instance, Github should not ask you for your login/password again. Or it might just ask one more time. Seek help if this does not work.\n\nHappy Git and GitHub for the useR is the best source for when Git or Github problems arise."
  },
  {
    "objectID": "tools.html#pdf",
    "href": "tools.html#pdf",
    "title": "Tools",
    "section": "PDF",
    "text": "PDF\nGenerating PDF files from RStudio is both easy and hard. It is easy because R markdown is designed to produce files in a variety of output formats, including PDF. It is hard because, for RStudio to make PDF files, your computer set up must be set up with a LaTeX installation. You have four options:\n\nMaking PDF files may just “work,” especially if you are using a Mac. Give it a try!\nIf it doesn’t just work, we strongly recommend using the tinytex R package. First, install the R package.\n\n\ninstall.packages('tinytex')\n\nSecond, use the R package to install the underlying LaTeX distribution.\n\ntinytex::install_tinytex()\n\nDepending on your operating system, this may not work. But there should be an error message providing further instructions. Follow those instructions.\nRestart R and everything should just work.\n\nYou can just generate an html file, open it in Chrome, select Print . . . from the drop-down menu. You will get a pop-up window. Click the down arrow to the right of Destination and choose Save as PDF in the drop-down menu. You’ll see a preview. Choose the Save as PDF option. This is not a convenient workflow but, if disaster strikes and the problem set is due in 10 minutes, it is a reasonable option.\nYou can install a full LaTeX installation yourself. Good luck! Don’t come to us for help."
  },
  {
    "objectID": "tools.html#style-guide",
    "href": "tools.html#style-guide",
    "title": "Tools",
    "section": "Style guide",
    "text": "Style guide\nMuch of this material comes from the Tidyverse Style Guide. We will take off points on work submitted which violates these guidelines. In extremis, you may go against this advice, if you add a code comment in your work explaining your decision to do so.\nComments\nInclude comments in your code. Easy-to-understand chunks of code should not have comments. The code is the comment. But other code will merit many, many lines of comments, more lines than the code itself. In a given file, you should have about as many total lines of comments as you have lines of code.\nMake your comments meaningful. They should not be a simple description of what your code does. The best comments are descriptions about why you did what you did and which other approaches you tried or considered. (The code already tells us what you did.) Good comments often have a “Dear Diary” quality: “I did this. Then I tried that. I finally chose this other thing because of reasons X, Y and Z. If I work on this again, I should look into this other approach.” Because of this, the structure is often a paragraph of comments followed by several lines of code.\nEach line of a comment should begin with the comment symbol (a “hash”) followed by a single space: #. Code comments must be separated from code by one empty line on both sides. Format your code comments neatly. Ctrl-Shift-/ is the easiest way to do that. Name your R code chunks, without using weird characters or spaces. download_data is a good R code chunk name. Plot #1 is not.\nSpelling matters. Comments should be constructed as sentences, with appropriate capitalization and punctuation.\nGraphics\nUse captions, titles, axis labels and so on to make it clear what your tables and graphics mean.\nAnytime you make a graphic without a title (explaining what the graphic is), a subtitle (highlighting a key conclusion to draw), a caption (with some information about the source of the data) and axis labels (with information about your variables), you should justify that decision in a code comment. We (try to) always include these items but there are situations in which that makes less sense. Ultimately, these decisions are yours, but we need to understand your reasoning.\nUse your best judgment. For example, sometimes axis labels are unnecessary. Read Data Visualization: A practical introduction by Kieran Healy for guidance on making high quality graphics.\nFormating\nLong Lines\nLimit your code to 80 characters per line. This fits comfortably on a printed page with a reasonably sized font. When calling functions, you can omit the argument names for very common arguments (i.e. for arguments that are used in almost every invocation of the function). Short unnamed arguments can also go on the same line as the function name, even if the whole function call spans multiple lines.\nWhitespace\n|> should always have a space before it, and should usually be followed by a new line. After the first step in the pipe, each line should be indented by two spaces. This structure makes it easier to add new steps (or rearrange existing steps) and harder to overlook a step.\n\n# Good\n\niris |>\n  group_by(Species) |>\n  summarize_if(is.numeric, mean) |>\n  ungroup() |>\n  gather(measure, value, -Species) |>\n  arrange(value)\n\n# Bad\n\niris |> group_by(Species) |> summarize_all(mean) |>\nungroup |> gather(measure, value, -Species) |>\narrange(value)\n\nggplot2 code is handled in a similar fashion. All commands after the initial invocation of ggplot() are indented.\n\n# Good\n\ndiamonds |> \n  ggplot(aes(x = depth)) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\n# Bad\n\ndiamonds |> \nggplot(aes(x = depth)) +\ngeom_histogram(bins = 100) + labs(title = \"Distribution of Depth\",\n         x = \"Depth\",\n         y = \"Count\")\n\nCommas\nAlways put a space after a comma, never before, just like in regular English.\n\n# Good\n\nx[, 1]\n\n# Bad\n\nx[,1]\nx[ ,1]\nx[ , 1]\n\nParentheses\nDo not put spaces inside or outside parentheses for regular function calls.\n\n# Good\n\nmean(x, na.rm = TRUE)\n\n# Bad\n\nmean (x, na.rm = TRUE)\nmean( x, na.rm = TRUE )\n\nInfix operators\nMost infix operators (=, ==, +, -, <-, ~, et cetera) should be surrounded by one space.\n\n# Good\n\nheight <- (feet * 12) + inches\nmean(x, na.rm = TRUE)\ny ~ a + b\n\n\n# Bad\n\nheight<-feet*12+inches\nmean(x, na.rm=TRUE)\ny~a + b\n\nOther operators — like ::, :::, $, @, [, [[, ^, and : — should never be surrounded by spaces.\n\n# Good\n\nsqrt(x^2 + y^2)\ndf$z\nx <- 1:10\n\n# Bad\n\nsqrt(x ^ 2 + y ^ 2)\ndf $ z\nx <- 1 : 10\n\nYou may add extra spaces if it improves alignment of = or <-.\n\nlist(total = a + b + c,\n     mean = (a + b + c) / n)\n\nDo not add extra spaces to places where space is not usually allowed.\nMessages/Warnings/Errors\nR messages/warnings/errors should never appear in a submitted document. The right way to deal with these issues is to find out their cause and then fix the underlying problem. Students sometimes use “hacks” to make these messages/warnings/errors disappear. The most common hacks involve using code chunk options like message = FALSE, warning = FALSE, results = \"hide\", include = FALSE and others. Don’t do this, in general. A message/warning/error is worth understanding and then fixing. Don’t close your eyes (metaphorically) and pretend that the problem doesn’t exist. There are some situations, however, in which, no matter what you try, you can’t fix the problem. In those few cases, you can use one of these hacks, but you must make a code comment directly below it, explaining the situation. The only exception is the “setup” chunk (included by default in every new Rmd) which comes with include = FALSE. In that chunk, no explanation is necessary, by convention."
  },
  {
    "objectID": "tools.html#how-to-use-rpubs",
    "href": "tools.html#how-to-use-rpubs",
    "title": "Tools",
    "section": "How to use Rpubs",
    "text": "How to use Rpubs\nRpubs provides a free hosting service for your R work. To use it:\n\nBegin by creating a new repository on GitHub. Then clone it to your computer. We are calling the repository “rpubs_example.” As before, put *Rproj in your .gitignore file. This is to prevent your private project file from being uploaded to GitHub.\n\n\n\n\n\n\n\nStart a new R Markdown file. Go to File –> New File –> R Markdown. For simplicity, leave the name “Untitled” and hit “OK.”\n\n\n\n\n\n\n\nSave this file, again, as “Untitled” in your project directory.\n\n\n\n\n\n\n\nKnit. You should see the following.\n\n\n\n\n\n\n\nNotice a blue icon in the upper right-hand corner that reads “Publish.” Click it.\n\n\n\n\n\n\n\nYou will be asked whether you want to publish to RPubs or RStudio Connect. Choose RPubs. You will get a reminder that all documents you publish on RPubs are publicly visible. Click “Publish.”\n\n\n\n\n\n\n\nThis will take you to the RPubs website. You will need to create an account. Follow the steps as prompted.\n\n\n\n\n\n\n\nAdd document details. Name your document. Add a meaningful slug – otherwise you will end up with an ugly, long address you didn’t choose and can’t remember. You can leave the Description blank for simplicity of the exercise.\n\n\n\n\n\n\n\nHit “Continue”, et voilá! You have published your first document to Rpubs!\n\n\n\n\n\n\n\nThere is one more important step. “rsconnect” contains files specific to your computer that you do not want to push to GitHub. Therefore, as with .Rproj files before, we want to add the rsconnect folder to the .gitignore file. Click on .gitignore, add it there and hit “Save.” You will see it disappear from your GitHub in the top right window. If you don’t see any changes, hit the Refresh button in the top right corner. Since you’ve updated your .gitignore file, now is a good time to commit and push your changes to your GitHub repository."
  },
  {
    "objectID": "tools.html#how-to-make-a-table",
    "href": "tools.html#how-to-make-a-table",
    "title": "Tools",
    "section": "How to make a table",
    "text": "How to make a table\ngt is an R package for creating elegant tables. First, we’ll create a gt summary table of some observations from the data. Second, we’ll run a regression and display the outcome using gtsummary, a companion package to gt which specializes in presenting the results of statistical models.\nIf you want to learn more about gt check this fantastic guide. Go here for the official gt package website. See here for an extensive guide to gtsummary.\nLoad the necessary libraries.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.1\n\n\nWarning: package 'tibble' was built under R version 4.2.1\n\nlibrary(primer.data)\nlibrary(gt)\n\nWe set message=FALSE in the above code chunk to avoid showing all the ugly notes when these libraries are loaded.\nLet’s pull some data which we will use in our table:\n\nx <- trains |>\n  select(gender, income, att_end) |>\n  slice(1:5)\nx\n\n# A tibble: 5 × 3\n  gender income att_end\n  <chr>   <dbl>   <dbl>\n1 Female 135000      11\n2 Female 105000      10\n3 Male   135000       5\n4 Male   300000      11\n5 Male   135000       5\n\n\nCreate the simplest table with gt(), the key command:\n\nx |> \n  gt()\n\n\n\n\n\n\ngender\n      income\n      att_end\n    \n\n\nFemale\n135000\n11\n\n\nFemale\n105000\n10\n\n\nMale\n135000\n5\n\n\nMale\n300000\n11\n\n\nMale\n135000\n5\n\n\n\n\n\n\n \nNow let’s make this more professional. gt offers a variety of functions to add features like these1:\n\n\n\n\n\nYou can add a title and subtitle using tab_header():\n\nx |> \n  gt() |>\n   tab_header(title = \"Enos Data Observations\", \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\n\n\n\n\n\n\n\nEnos Data Observations\n    \n\nGender, Income, and End Attitude from the Trains Data\n    \n\n\ngender\n      income\n      att_end\n    \n\n\nFemale\n135000\n11\n\n\nFemale\n105000\n10\n\n\nMale\n135000\n5\n\n\nMale\n300000\n11\n\n\nMale\n135000\n5\n\n\n\n\n\n\n \nBy default, titles and other text can not be formatted. If you want formatting, you must wrap the character string in a call to md(), where md stands for (M)ark(d)own. For example, here is a bolded title.\n\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\")\n\n\n\n\n\n\n\nEnos Data Observations\n    \n\nGender, Income, and End Attitude from the Trains Data\n    \n\n\ngender\n      income\n      att_end\n    \n\n\nFemale\n135000\n11\n\n\nFemale\n105000\n10\n\n\nMale\n135000\n5\n\n\nMale\n300000\n11\n\n\nMale\n135000\n5\n\n\n\n\n\n\n \nWe can use tab_spanner() to add spanner columns. The c() argument takes in the variables that the spanner column will cover.\n\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |> \n   tab_spanner(label = \"Name of Spanner Column Here\", c(gender, income))\n\n\n\n\n\n\n\nEnos Data Observations\n    \n\nGender, Income, and End Attitude from the Trains Data\n    \n\n\n\n\n        Name of Spanner Column Here\n      \n      att_end\n    \n\ngender\n      income\n    \n\n\n\nFemale\n135000\n11\n\n\nFemale\n105000\n10\n\n\nMale\n135000\n5\n\n\nMale\n300000\n11\n\n\nMale\n135000\n5\n\n\n\n\n\n\nFrom here on, our current table will not include a spanner column. If you wish to see more examples of spanner columns, go to Chapter 4.\n \nYou can change the column names using cols_label():\n\nx |> \n  gt()|>\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |>\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\")\n\n\n\n\n\n\n\nEnos Data Observations\n    \n\nGender, Income, and End Attitude from the Trains Data\n    \n\n\nGender\n      Income\n      End Attitude\n    \n\n\nFemale\n135000\n11\n\n\nFemale\n105000\n10\n\n\nMale\n135000\n5\n\n\nMale\n300000\n11\n\n\nMale\n135000\n5\n\n\n\n\n\n\n \nUse tab_source_note() to cite the source of the data or to create a caption. This function is not exclusively for providing a source — though it’s a handy way to do so — and can be used to display any text you’d like:\n\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |>\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") |> \n  tab_source_note(\"Source: Ryan Enos\")\n\n\n\n\n\n\n\nEnos Data Observations\n    \n\nGender, Income, and End Attitude from the Trains Data\n    \n\n\nGender\n      Income\n      End Attitude\n    \n\n\nFemale\n135000\n11\n\n\nFemale\n105000\n10\n\n\nMale\n135000\n5\n\n\nMale\n300000\n11\n\n\nMale\n135000\n5\n\n\n\nSource: Ryan Enos\n    \n\n\n\n\n \nUsing md() again, we can italicize the name of the Enos study in the caption:\n\nx |> \n  gt()|>\n   tab_header(title = md(\"**Enos Data Observations**\"), \n              subtitle = \"Gender, Income, and End Attitude from the Trains Data\") |>\n  cols_label(gender = \"Gender\",\n             income = \"Income\", \n             att_end = \"End Attitude\") |> \n  tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup Contact on Exclusionary Attitudes*\"))\n\n\n\n\n\n\n\nEnos Data Observations\n    \n\nGender, Income, and End Attitude from the Trains Data\n    \n\n\nGender\n      Income\n      End Attitude\n    \n\n\nFemale\n135000\n11\n\n\nFemale\n105000\n10\n\n\nMale\n135000\n5\n\n\nMale\n300000\n11\n\n\nMale\n135000\n5\n\n\n\nSource: Ryan Enos, Causal Effect of Intergroup Contact on Exclusionary Attitudes\n\n    \n\n\n\n\n \nNow that the table structure looks good, we want to format the numbers themselves. Let’s add some dollar signs to the income column using fmt_currency(). This function also adds commas (if you want commas without dollar signs use fmt_number()). The c() within fmt_currency() denotes the variable being formatted as a currency:\n\nx |> \n  gt() |>\n    tab_header(title = md(\"**Enos Data Observations**\"), \n               subtitle = \"Gender, Income, and End Attitude from the Trains Data\")|>\n    cols_label(gender = \"Gender\",\n               income = \"Income\", \n               att_end = \"End Attitude\") |> \n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                       Contact on Exclusionary Attitudes*\")) |>\n    fmt_currency(columns = c(income), \n                 decimals = 0) \n\n\n\n\n\n\n\nEnos Data Observations\n    \n\nGender, Income, and End Attitude from the Trains Data\n    \n\n\nGender\n      Income\n      End Attitude\n    \n\n\nFemale\n$135,000\n11\n\n\nFemale\n$105,000\n10\n\n\nMale\n$135,000\n5\n\n\nMale\n$300,000\n11\n\n\nMale\n$135,000\n5\n\n\n\nSource: Ryan Enos, Causal Effect of Intergroup\nContact on Exclusionary Attitudes\n\n    \n\n\n\n\n \nNote that the line return in the title between “Intergroup” and “Contact” does not effect or break up the title displayed by md().\nRegression tables\nWe can making a gt table with a stan_glm() regression object. Key to this is the gtsummary package and its tbl_regression() function.\n\nlibrary(rstanarm)\n\nWarning: package 'Rcpp' was built under R version 4.2.1\n\nlibrary(broom.mixed)\nlibrary(gtsummary)\n\nWarning: package 'gtsummary' was built under R version 4.2.1\n\nfit2 <- stan_glm(att_end ~ party, data = trains, refresh = 0)\n\ntbl_regression(fit2, \n               intercept = TRUE, \n               estimate_fun = function(x) style_sigfig(x, digits = 2)) |>\n  as_gt() |>\n    tab_header(title = \"Regression of Attitudes about Immigration\", \n               subtitle = \"The Effect of Party on End Attitude\") |>\n    tab_source_note(md(\"Source: Ryan Enos, *Causal Effect of Intergroup \n                        Contact on Exclusionary Attitudes*\"))\n\n\n\n\n\n\n\nRegression of Attitudes about Immigration\n    \n\nThe Effect of Party on End Attitude\n    \n\n\nCharacteristic\n      Beta\n      \n95% CI1\n\n    \n\n\n(Intercept)\n8.8\n8.2, 9.4\n\n\nparty\n\n\n\n\nDemocrat\n—\n—\n\n\nRepublican\n2.2\n0.76, 3.6\n\n\n\nSource: Ryan Enos, Causal Effect of Intergroup\nContact on Exclusionary Attitudes\n\n    \n\n\n1 CI = Credible Interval"
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "Functions",
    "section": "",
    "text": "A function is a piece of code that is packaged in a way that makes it easy to reuse. Functions make it easy for you to filter(), arrange(), select(), and create a tibble(), as you have seen in Chapters @ref(visualization) and @ref(wrangling). Functions also allow you to transform variables and perform mathematical calculations. We use functions like rnorm() and runif() to generate random draws from a distribution.\nEvery time we reference a function in this Primer, we include the parentheses. You call a function by including its parentheses and any necessary arguments within those parentheses. This is a correct call of rnorm():\n\nrnorm(n = 1)\n\n[1] 1.192546\n\n\nIf you run the function name without its parentheses, R will return the code that makes up the function.\n\nrnorm\n\nfunction (n, mean = 0, sd = 1) \n.Call(C_rnorm, n, mean, sd)\n<bytecode: 0x0000017fc58572e8>\n<environment: namespace:stats>\n\n\nFunctions can do all sorts of things. sample() takes a vector of values and returns a number of values randomly selected from that vector. You can specify the number of random values with the argument size. This call is the equivalent of rolling a die.\n\nsample(x = 1:6, size = 1)\n\n[1] 2\n\n\nFunctions can also take in other functions as arguments. For example, replicate() takes an expression and repeats it n times. What if we replicated the rolling of a die ten times?\n\nreplicate(10, sample(1:6, 1))\n\n [1] 4 5 2 5 1 4 6 4 6 2\n\n\nAn especially useful type of function is the family of map_* functions, from the purrr package, which is automatically loaded with library(tidyverse). These functions apply some other function to every row in a tibble.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.1\n\n\nWarning: package 'tibble' was built under R version 4.2.1\n\n\nLet’s create a tibble with one variable x which takes on three values: 3, 7, and 2.\n\ntibble(x = c(4, 16, 9))\n\n# A tibble: 3 × 1\n      x\n  <dbl>\n1     4\n2    16\n3     9\n\n\nIt is easy to use mutate to create a new variable, sq_root, which is the square root of each value of x.\n\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = sqrt((x)))\n\n# A tibble: 3 × 2\n      x sq_root\n  <dbl>   <dbl>\n1     4       2\n2    16       4\n3     9       3\n\n\nmap_* functions provide another approach. A map_* function takes two required arguments. First is the object over which you want to iterate. This will generally be a column in the tibble in which you are working. Second is the function which you want to run for each row in the tibble.\n\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = map_dbl(x, ~ sqrt(.)))\n\n# A tibble: 3 × 2\n      x sq_root\n  <dbl>   <dbl>\n1     4       2\n2    16       4\n3     9       3\n\n\nmap_dbl() (pronounced “map-double”) took the function sqrt() and applied it to each element of x. There are two tricky parts to the use of map_* functions. First, you need to put the tilde symbol — the “~” — before the name of the function which you want to call. Without the ~, you will get an error:\n\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = map_dbl(x, sqrt(.)))\n\nError in `mutate()`:\n! Problem while computing `sq_root = map_dbl(x, sqrt(.))`.\nCaused by error in `as_mapper()`:\n! object '.' not found\n\n\nSecond, you need to include a period — the “.” — in the spot where the variable goes. Using the name of the variable — x in this case — will generate an error.\n\ntibble(x = c(4, 16, 9)) |> \n  mutate(sq_root = map_dbl(x, ~ sqrt(x)))\n\nError in `mutate()`:\n! Problem while computing `sq_root = map_dbl(x, ~sqrt(x))`.\nCaused by error in `stop_bad_type()`:\n! Result 1 must be a single double, not a double vector of length 3\n\n\nTilde and dot (~ and .) are easy to forget.\nIf you know the expected output of your function, you can specify that kind of vector:\n\n\nmap(): list\n\n\nmap_lgl(): logical\n\nmap_int(): integer\n\nmap_dbl(): double (numeric)\n\nmap_chr(): character\n\nmap_df(): data frame\n\nSince our example returns numeric output, we use map_dbl() instead of map().\nThe key difference between using mutate() and map_* functions is that map_* functions are designed to work well with lists, both as inputs and as outputs. mutate() is designed for atomic vectors, meaning vectors in which element is a single value."
  },
  {
    "objectID": "functions.html#list-columns-and-map-functions",
    "href": "functions.html#list-columns-and-map-functions",
    "title": "Functions",
    "section": "List-columns and map functions",
    "text": "List-columns and map functions\nRecall that a list is different from an atomic vector. In atomic vectors, each element of the vector has one value. Lists, however, can contain vectors, and even more complex objects, as elements.\n\nx <- list(c(4, 16, 9), c(\"A\", \"Z\"))\nx\n\n[[1]]\n[1]  4 16  9\n\n[[2]]\n[1] \"A\" \"Z\"\n\n\nx is a list with two elements. That element is a numeric vector of length 3. The second element is a character vector of length 2. We use [[]] to extract specific elements. Example:\n\nx[[1]][3]\n\n[1] 9\n\n\nThe first [[]] extracts the first element form the list x. The second `[[]]`` extracts the 3rd element from the vector which is that first element.\nThere are a number of built-in R functions that output lists. For example, the ggplot objects you have been making store all of the plot information in lists. Any function that returns multiple values can be used to create a list output by wrapping that returned object with list().\n\nx <- rnorm(10)\n\n# range() returns the min and max of the argument \n\nrange(x)\n\n[1] -1.3547992  0.4059626\n\ntibble(col_1 = list(range(x))) \n\n# A tibble: 1 × 1\n  col_1    \n  <list>   \n1 <dbl [2]>\n\n\nNotice this is a 1x1 tibble with one observation, which is a list of one element. Voila! You have just created a list-column.\nIf a function returns multiple values as a vector, like range() does, you must use list() as a wrapper if you want to create a list-column.\nA list column is a column of your data which is a list rather than an atomic vector. Like with lists, you can pipe to str() to examine the column.\n\ntibble(col_1 = list(range(x))) |>\n  str()\n\ntibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n $ col_1:List of 1\n  ..$ : num [1:2] -1.355 0.406\n\n\nWe can use map_* functions to both create a list-column and then, much more importantly, work with that list-column afterwards. Example:\n\n\n# This simple example demonstrates the workflow which we will often follow.\n# Start by creating a tibble which will be used to store the results. (Or start\n# with a tibble which already exists and to which you will be adding more\n# columns.) It is often convenient to get all the code working with just a few\n# rows. Once it is working, we increase the number of rows to a thousand or\n# million or whatever we need.\n\ntibble(ID = 1:3) |> \n  \n  # The big convenience is being able to store a list in each row of the tibble.\n  # Note that we are not using the value of ID in the call to rnorm(). (That is\n  # why we don't have a \".\" anywhere.) But we are still using ID as a way of\n  # iterating through each row; ID is keeping count for us, in a sense.\n  \n  mutate(draws = map(ID, ~ rnorm(10))) |> \n  \n  # Each succeeding step of the pipe works with columns already in the tibble\n  # while, in general, adding more columns. The next step calculates the max\n  # value in each of the draw vectors. We use map_dbl() because we know that\n  # max() will returns a single number.\n  \n  mutate(max = map_dbl(draws, ~ max(.))) |> \n  \n  # We will often need to calculate more than one item from a given column like\n  # draws. For example, in addition to knowing the max value, we would like to\n  # know the range. Because the range is a vector, we need to store the result\n  # in a list column. map() does that for us automatically.\n  \n  mutate(min_max = map(draws, ~ range(.)))\n\n# A tibble: 3 × 4\n     ID draws        max min_max  \n  <int> <list>     <dbl> <list>   \n1     1 <dbl [10]> 1.14  <dbl [2]>\n2     2 <dbl [10]> 1.72  <dbl [2]>\n3     3 <dbl [10]> 0.977 <dbl [2]>\n\n\nThis flexibility is only possible via the use of list-columns and map_* functions. This workflow is extremely common. We start with an empty tibble, using ID to specify the number of rows. With that skeleton, each step of the pipe adds a new column, working off a column which already exists.\n\nLet’s practice with the nhanes dataset from the primer.data package. How could we add a column to the dataset that included the quantiles of the height variable for each gender?\n\nlibrary(primer.data)\n\nSelect the relevant variables, and group by gender. We are grouping because we are curious as to how height is distributed in between gender. We drop any rows with missing data.\n\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender)\n\n# A tibble: 9,647 × 2\n# Groups:   gender [2]\n   gender height\n   <chr>   <dbl>\n 1 Male     165.\n 2 Male     165.\n 3 Male     165.\n 4 Male     105.\n 5 Female   168.\n 6 Male     133.\n 7 Male     131.\n 8 Female   167.\n 9 Female   167.\n10 Female   167.\n# … with 9,637 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThere are two approaches. In the first, we are happy to have an output tibble with just two rows:\n\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender) |> \n  summarise(q_height = list(quantile(height)),\n            .groups = \"drop\")\n\n# A tibble: 2 × 2\n  gender q_height \n  <chr>  <list>   \n1 Female <dbl [5]>\n2 Male   <dbl [5]>\n\n\nNote that there was no need to use map_* functions in this case. The simple dplyr approach works fine. The only “trick” is the use of list() to wrap the output of quantile(). Use str() to examine the exact values.\n\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender) |> \n  summarise(q_height = list(quantile(height)),\n            .groups = \"drop\") |> \n  str()\n\ntibble [2 × 2] (S3: tbl_df/tbl/data.frame)\n $ gender  : chr [1:2] \"Female\" \"Male\"\n $ q_height:List of 2\n  ..$ : Named num [1:5] 83.8 154.3 160.6 165.9 184.5\n  .. ..- attr(*, \"names\")= chr [1:5] \"0%\" \"25%\" \"50%\" \"75%\" ...\n  ..$ : Named num [1:5] 83.6 166.2 173.8 179.4 200.4\n  .. ..- attr(*, \"names\")= chr [1:5] \"0%\" \"25%\" \"50%\" \"75%\" ...\n\n\nMen are taller than women throughout the distribution, but the smallest individual (child) in the data, see the 0% quantile, happens to be male.\nThe second case involves a scenario in which we do not want to “lose” any rows in our tibble. We want a q_height column for all the rows, even if the values included are repetitive. A common scenario is that we want to use q_height to perform a calculation for each individual. To do this, we need a map_* function.\n\nnhanes |>\n  select(gender, height) |>\n  drop_na() |> \n  group_by(gender) |> \n  summarize(q_height = map(height, ~ quantile(.)),\n            .groups = \"drop\")\n\n# A tibble: 9,647 × 2\n   gender q_height \n   <chr>  <list>   \n 1 Female <dbl [5]>\n 2 Female <dbl [5]>\n 3 Female <dbl [5]>\n 4 Female <dbl [5]>\n 5 Female <dbl [5]>\n 6 Female <dbl [5]>\n 7 Female <dbl [5]>\n 8 Female <dbl [5]>\n 9 Female <dbl [5]>\n10 Female <dbl [5]>\n# … with 9,637 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe first four lines of the pipe are the same in both cases. The only difference is the use of list(quantile(height)) in the first and map(height, ~ quantile(.) in the second.\nUntil now, we have practiced using map_* functions with built-in R functions. Sometimes, however, there is not an R function which does what we want. When that happens, we need to create our own function."
  },
  {
    "objectID": "functions.html#custom-functions",
    "href": "functions.html#custom-functions",
    "title": "Functions",
    "section": "Custom Functions",
    "text": "Custom Functions\nThere are many built-in functions in R. A function is composed of a name, a list of arguments and a body. We create our own functions with the function() function.\nCreating your own functions\nAssume we want to create a function which adds 1 and 1 together. The first step is to write some R code which does that.\n\n1 + 1\n\n[1] 2\n\n\nThis code will become the “body” of the function, the part in between the curly braces. We also need to a function definition, which is composed of the name of the function, a call to the function() function, and a pair of curly braces.\n\nadd_one_and_one <- function(){}\n\nCombining the function definition and the body of the function completes the process.\n\nadd_one_and_one <- function(){\n  1 + 1\n}\n\nadd_one_and_one()\n\n[1] 2\n\n\nYou just created a function! This function will return 1 + 1 whenever called.\nConsider a function which adds the number 6 to a value x, a value which we want to allow the user provide.\n\nadd_six_to_something <- function(x){\n  x + 6\n}\n\nadd_six_to_something(x = 1)\n\n[1] 7\n\n\nYou have incorporated your first formal argument. Formal arguments in functions are additional parameters that allow the user to customize the use of the function. Instead of adding 1 + 1 over and over again, your function takes in a number x that the user defines and adds 6. Consider a function with two formal arguments.\n\nadd_x_to_y <- function(x, y) {\n  x + y\n}\n\nadd_x_to_y(1, 2)\n\n[1] 3\n\nadd_x_to_y(4, 3)\n\n[1] 7\n\n\nAnonymous functions with map_* functions\nWe can create functions that perform operations “on the fly,” without bothering to give them a name. These nameless functions are called anonymous functions.\nYou can use anonymous functions in conjunction with the map_* family of functions. This is probably the most common use of anonymous functions, at least in this Primer.\nYou can call an anonymous function using a ~ operator and then using a . to represent the current element. Consider these three approaches:\n\ntibble(old = c(4, 16, 9)) |> \n  mutate(new_1 = old + 6) |> \n  mutate(new_2 = map_dbl(old, ~ add_six_to_something(.))) |> \n  mutate(new_3 = map_dbl(old, ~ (. + 6)))\n\n# A tibble: 3 × 4\n    old new_1 new_2 new_3\n  <dbl> <dbl> <dbl> <dbl>\n1     4    10    10    10\n2    16    22    22    22\n3     9    15    15    15\n\n\n\nAll three produce the same answer, as we would expect. Just using mutate() is best, as long as it accomplishes your goal. In complex situations, especially those involving simulation, it often won’t. Example:\n\n\n\n\ntibble(ID = 1:3) |> \n  mutate(x = rnorm(1))\n\n# A tibble: 3 × 2\n     ID      x\n  <int>  <dbl>\n1     1 -0.282\n2     2 -0.282\n3     3 -0.282\n\n\nCalling rnorm(), or any function with a random component, does not have the effect which you probably want if you do it in the context of a simple mutate(). Instead, R runs rnorm(1) once, and then copies the value generated to the remaining two rows of the tibble. To get a different value in each row, you need to explicitly tell R to do that by using a map_* function:\n\n\n\n\ntibble(ID = 1:3) |> \n  mutate(x = rnorm(1)) |> \n  mutate(y = map_dbl(ID, ~ rnorm(1)))\n\n# A tibble: 3 × 3\n     ID      x      y\n  <int>  <dbl>  <dbl>\n1     1 -0.282 -1.31 \n2     2 -0.282  0.795\n3     3 -0.282  0.270\n\n\nNote that the parentheses in the anonymous function are not necessary. As long as everything after the ~ works as R code, the anonymous function should work, each time replacing the . with the value in the relevant row from the .x variable — which is old in this case.\n\ntibble(old = c(4, 16, 9)) |> \n  mutate(new = map_dbl(old, ~ . + 1))\n\n# A tibble: 3 × 2\n    old   new\n  <dbl> <dbl>\n1     4     5\n2    16    17\n3     9    10\n\n\nSkateboard >> perfectly formed rear-view mirror\n\nThis image — widely attributed to the Spotify development team — conveys an important point.\n\n\n\n\nFrom Your ultimate guide to Minimum Viable Product (+great examples)\n\n\n\n\nBuild that skateboard before you build the car or some fancy car part. A limited-but-functioning thing is very useful. It also keeps spirits high.\nThis is related to the Telescope Rule:\n\nIt is faster to make a four-inch mirror and then a six-inch mirror than it is to make a six-inch mirror."
  },
  {
    "objectID": "functions.html#no_na_sampler",
    "href": "functions.html#no_na_sampler",
    "title": "Functions",
    "section": "no_NA_sampler()",
    "text": "no_NA_sampler()\nAssume that we want to sample 10 observations for height from the nhanes tibble from the primer.data package. That is easy to do with the built in function sample().\n\nsample(nhanes$height, size = 10)\n\n [1] 159.4 119.3 170.5 171.1 180.9 135.7  99.3 168.7 152.6 175.0\n\n\nOne problem with this approach is that it will sample missing values of height. We can avoid that by manipulating the vector inside of the call to sample().\n\nsample(nhanes$height[! is.na(nhanes$height)], size = 10)\n\n [1] 163.4 141.0 158.0 160.5 168.8 182.6 116.6 157.9  86.0 175.7\n\n\nThat works, but, first, it is ugly code. And, second, it is hard to extend when we have more constraints. For example, assume we only want to sample from individuals who have no missing values for any variables, not just height. To do that, we really ought to make a custom function. Call that function no_NA_sampler().\nThe first step in function creation is to write code in a normal pipe which does what you want the function to do. In this case, that code would look like:\n\nnhanes |> \n  drop_na() |>\n  slice_sample(n = 10) |> \n  pull(height)\n\n [1] 156.8 166.5 170.9 160.3 155.5 151.7 165.5 150.0 175.3 159.6\n\n\nWe start with nhanes, use drop_na() to remove rows with missing values for any variable, sample 10 rows at random and then pull out height. To turn this into a function, we just need to copy/paste this pipe within the body of our function definition:\n\nno_NA_sampler <- function(){\n  nhanes |> \n    drop_na() |>\n    slice_sample(n = 10) |> \n    pull(height)\n}\n\nno_NA_sampler()\n\n [1] 162.4 165.2 159.0 155.5 159.0 160.3 157.2 170.7 164.4 155.9\n\n\nVoila! A function just executes the code within its body. The first step in building a function is not to write the function. It is to write the code which you want the function to execute.\nThe first version, however, “hard codes” a lot of options which we might want to change. What if we want to sample 5 values of height or 500? In that case, we could hard code a new number in place of “10”. A better option would be to add an argument so that we can pass in whatever value we want.\n\nno_NA_sampler <- function(n){\n  nhanes |> \n    drop_na() |>\n    slice_sample(n = n) |> \n    pull(height)\n}\n\nno_NA_sampler(n = 2)\n\n[1] 163.7 170.0\n\nno_NA_sampler(n = 25)\n\n [1] 156.0 171.6 166.5 168.4 166.0 163.7 163.6 161.0 159.0 156.5 163.4 163.7\n[13] 168.1 170.3 154.8 176.6 173.2 166.1 169.4 173.4 174.2 156.8 159.3 157.0\n[25] 159.0\n\n\nWhat if we want to sample from a different variable than height or from a different tibble than nhanes? Again, the trick is to turn hard coded values into arguments. The argument tbl is a placeholder for a data set, n for the number of samples you want extracted from your data set, and var for the variable in the samples that we are studying.\n\nno_NA_sampler <- function(tbl, var, n){\n  tbl |> \n    drop_na() |>\n    slice_sample(n = n) |> \n    pull({{var}})\n}\n\nno_NA_sampler(tbl = nhanes, var = height, n = 2)\n\n[1] 163.6 163.4\n\n\nR does not know how to interpret something like age when it is passed in an argument. The double curly braces around var tell R, in essence, that var is a variable in the tibble created from sampling from our input tibble tbl. We can use the order of the arguments, without naming them, with no_NA_sampler(), just as with any other R function:\n\nno_NA_sampler(trains, age, 5)\n\n[1] 45 42 41 31 44\n\n\n\nNow that we have the function doing what we want, we should add some comments and some error checking.\n\nno_NA_sampler <- function(tbl, var, n){\n  \n  # Function for grabbing `n` samples from a variable `var` which lives in a\n  # tibble `tbl`. \n  \n  # I could not figure out how to check to see if `var` actually lives in the\n  # tibble in my error checking. Also, I don't like that I need to use\n  # is_double() as the check on `n` even though I want `n` to be an integer.\n  \n  stopifnot(is_tibble(tbl))\n  stopifnot(is_double(n))\n\n  tbl |> \n    drop_na() |>\n    \n    # What happens if n is \"too large\"? That is, I need to think harder about a)\n    # whether or not I am sampling with or without replacement and b) which I\n    # should be doing.\n    \n    slice_sample(n = n) |> \n    pull({{var}})\n}\n\nDo the comments in the above code seem weird? Perhaps. But they are good comments! First, there about as many lines of comments as there are lines of code. That is a good rule of thumb. Second, the comments do not simple report what the code is doing. That is redundant! The code itself tells us what the code is doing. The comments, instead, are a discussion of issues related to the code, to things we don’t understand, to topics which we should revisit. They are like a diary. Good programmers keep good diaries."
  },
  {
    "objectID": "functions.html#prediction-game",
    "href": "functions.html#prediction-game",
    "title": "Functions",
    "section": "Prediction Game",
    "text": "Prediction Game\nLet’s play a prediction game. Consider the kenya tibble from primer.data.\n\nkenya\n\n# A tibble: 1,672 × 9\n   block    poll_station treatment poverty dista…¹ pop_d…² mean_…³ reg_b…⁴  rv13\n   <chr>    <chr>        <fct>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n 1 KWALE/42 007/001      control     0.247    22.0 2.96e-3    39.6 0.00358  1116\n 2 KWALE/35 007/004      local + …   0.329    25.1 8.88e-4    43.8 0.0742    364\n 3 KWALE/40 007/009      local       0.263    27.8 1.84e-3    34.7 0.00691  4632\n 4 KWALE/18 007/011      local + …   0.429    27.2 2.70e-4    44.6 0.26      150\n 5 KWALE/12 007/017      local + …   0.341    19.3 5.44e-4    39.0 0.0228    833\n 6 KWALE/42 007/018      local       0.204    24.0 7.98e-3    37.1 0.00243  1646\n 7 KWALE/40 007/019      SMS         0.272    25.1 1.67e-3    39.2 0.00487   616\n 8 KWALE/12 007/020      control     0.316    23.8 5.38e-4    36.3 0         775\n 9 KWALE/30 007/022      canvass     0.396    20.5 2.16e-4    42.9 0.00575   696\n10 KWALE/16 007/023      local + …   0.398    14.5 1.48e-4    39.8 0.0360    722\n# … with 1,662 more rows, and abbreviated variable names ¹​distance,\n#   ²​pop_density, ³​mean_age, ⁴​reg_byrv13\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe game is that we will pick a random value of rv13, which is the number of people who live in the vicinity of a polling station. You guess a number. I guess a number. The winner of the Prediction Game is the person whose guess is closest to the random value selected. Example:\n\nyour_guess <- 500\nmy_guess <- 600\n\nsampled_value <- no_NA_sampler(kenya, rv13, n = 1) \n\nyour_error <- abs(your_guess - sampled_value)\nmy_error <- abs(my_guess - sampled_value)\n\nif(your_error < my_error) cat(\"You win!\")\n\nYou win!\n\nif(your_error > my_error) cat(\"I win!\")\n\nRun this code in your R Console to try it out. It works! It is also sloppy and disorganized. The first step in writing good code is to write bad code.\nWe don’t want to play the Prediction Game just once. We want to play it thousands of times. Copy/pasting this code a thousand times would be stupid. Instead, we need a function. Just place the working code within a function definition, and Voila!\n\nprediction_game <- function(){\n  your_guess <- 500\n  my_guess <- 600\n  \n  sampled_value <- no_NA_sampler(kenya, rv13, n = 1) \n  \n  your_error <- abs(your_guess - sampled_value)\n  my_error <- abs(my_guess - sampled_value)\n  \n  if(your_error < my_error) cat(\"You win!\")\n  if(your_error > my_error) cat(\"I win!\")\n}\n\nOther than the function definition itself, there are no changes. Yet, by creating a function, we can now easily run this many times.\n\nreplicate(3, prediction_game())\n\nYou win!You win!You win!\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n\nThe problem with this version is that we want prediction_game() to return a message about the winner. Right now, it returns nothing. It just prints the winner. Let’s change that, and also allow for guesses to be passed in as an argument, along with the tibble and variable. We can leave n hard coded as 1 since, by definition, the Prediction Game is an attempt to guess one number, at least for now. To do this, we need to use the return() function which, when executed, causes the function to finish and return whatever value is within the paratheses.\n\n\nprediction_game <- function(guesses, tbl, var){\n  \n  # Check to make sure that guesses is a vector of doubles of length 2.\n  \n  stopifnot(all(is_double(guesses)))\n  stopifnot(length(guesses) == 2)\n  \n  # This tells the function that the \"guess\" inputted first in the \n  # guesses is \"your\" guess, whereas the second input is \"my\" guess.\n  \n  your_guess <- guesses[1]\n  my_guess <- guesses[2]\n  \n  # Use the function no_NA_sampler to draw a sample from a data set\n  # of our choosing, with a {{var}} and n.\n  \n  sampled_value <- no_NA_sampler(tbl, {{var}}, n = 1) \n  \n  # Subtract the sampled value obtained from no_NA_sampler from \n  # both of our guesses. \n  \n  your_error <- abs(your_guess - sampled_value)\n  my_error <- abs(my_guess - sampled_value)\n  \n  # If the difference between your guess and the sampled value is \n  # less than the difference between my guess and the sampled value\n  # (meaning that your guess was closer to the truth), the function\n  # returns the message \"Guess, your_guess, wins!\".\n  \n  if(your_error < my_error){ \n    return(paste(\"Guess\", your_guess, \"wins!\"))\n  }\n  \n  # If your error exceeds my error (meaning that your guess was\n  # further than the truth than mine), the function prints the \n  # message \"Guess, my_guess, wins!\" \n  \n  if(your_error > my_error){ \n    return(paste(\"Guess\", my_guess, \"wins!\"))\n  }\n  \n  # If we guess the same number, and our error rates are therefore\n  # identical, we return the message \"A tie!\". \n  \n  if(your_error == my_error){ \n    return(\"A tie!\")\n  }\n\n}\n\n\nreplicate(5, prediction_game(guesses = c(500, 600), kenya, rv13))\n\n[1] \"Guess 500 wins!\" \"Guess 500 wins!\" \"Guess 600 wins!\" \"Guess 500 wins!\"\n[5] \"Guess 500 wins!\"\n\n\nIn general, we will want to store the results in a tibble, which makes later analysis and plotting easier.\n\ntibble(ID = 1:3) |> \n  mutate(result = map_chr(ID, ~ \n                            prediction_game(guesses = c(500, 600),\n                                            kenya, \n                                            rv13)))\n\n# A tibble: 3 × 2\n     ID result         \n  <int> <chr>          \n1     1 Guess 500 wins!\n2     2 Guess 500 wins!\n3     3 Guess 500 wins!\n\n\nWho wins the game the most if we play 1,000 times?\n\n\n\n\ntibble(ID = 1:1000) |> \n  mutate(result = map_chr(ID, ~ \n                            prediction_game(guesses = c(500, 600),\n                                            kenya, \n                                            rv13))) |> \n  ggplot(aes(result)) +\n    geom_bar()\n\n\n\n\nIt is hardly surprising that 500 wins more often than 600 since the mean of rv13 is 539.2332536. The mean seems like a pretty good guess! But it is not the best guess.\nTo test whether the mean or the median is a better guess, we will use our created prediction_game function with the guesses of 442 (the median) and 539 (the mean) and plot the results.\n\ntibble(ID = 1:1000) |> \n  mutate(result = map_chr(ID, \n                          ~ prediction_game(c(442, 539),\n                                            kenya,\n                                            rv13))) |> \n  ggplot(aes(result)) +\n    geom_bar()\n\n\n\n\nThe mean is not a bad prediction. But the best prediction is (surprisingly?) the median, which is 442.\nPlaying within a tibble\nIn other cases, it is more convenient to play portions of the Prediction Game within a tibble. Imagine that we are trying to guess the biggest value out of 10 random samples.\n\ntibble(ID = 1:3, guess_1 = 800, guess_2 = 900) |> \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10)))\n\n# A tibble: 3 × 4\n     ID guess_1 guess_2 result    \n  <int>   <dbl>   <dbl> <list>    \n1     1     800     900 <dbl [10]>\n2     2     800     900 <dbl [10]>\n3     3     800     900 <dbl [10]>\n\n\nWe can now manipulate the result column and then see which prediction did better. Using the same structure as before, we subtract our guesses from the variable we were guessing; in this case, the biggest value in 10 random samples.\n\ntibble(ID = 1:3, guess_1 = 800, guess_2 = 900) |> \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) |> \n  mutate(biggest = map_dbl(result, ~ max(.))) |> \n  mutate(error_1 = abs(guess_1 - biggest)) |> \n  mutate(error_2 = abs(guess_2 - biggest)) |> \n  mutate(winner = case_when(error_1 < error_2 ~ \"Guess one wins!\",\n                            error_1 > error_2 ~ \"Guess two wins!\",\n                            TRUE ~ \"A tie!\"))\n\n# A tibble: 3 × 8\n     ID guess_1 guess_2 result     biggest error_1 error_2 winner         \n  <int>   <dbl>   <dbl> <list>       <dbl>   <dbl>   <dbl> <chr>          \n1     1     800     900 <dbl [10]>    1382     582     482 Guess two wins!\n2     2     800     900 <dbl [10]>    2246    1446    1346 Guess two wins!\n3     3     800     900 <dbl [10]>    1422     622     522 Guess two wins!\n\n\nRun the test 1,000 times.\n\ntibble(ID = 1:1000, guess_1 = 800, guess_2 = 900) |> \n  mutate(result = map(ID, ~ no_NA_sampler(kenya, rv13, 10))) |> \n  mutate(biggest = map_dbl(result, ~ max(.))) |> \n  mutate(error_1 = abs(guess_1 - biggest)) |> \n  mutate(error_2 = abs(guess_2 - biggest)) |> \n  mutate(winner = case_when(error_1 < error_2 ~ \"Guess one wins!\",\n                            error_1 > error_2 ~ \"Guess two wins!\",\n                            TRUE ~ \"A tie!\")) |> \n  ggplot(aes(winner)) +\n    geom_bar()\n\n\n\n\nEmpirically, we see than 900 is a much better guess than 800. Instead of calling a function to be run 1,000 times, we just performed each step within each row of a tibble with 1,000 rows. Both approaches work. The best choice depends on the context of your problem."
  },
  {
    "objectID": "functions.html#summary",
    "href": "functions.html#summary",
    "title": "Functions",
    "section": "Summary",
    "text": "Summary\nThe first step in writing good code is to write bad code.\nTilde and dot (~ and .) are easy to forget.\nThe first step in building a function is not to write the function. It is to write the code which you want the function to execute.\nLists and list-columns\n\nA list is different from an atomic vector. Atomic vectors are familiar to us: each element of the vector has one value, and thus if an atomic vector is a column in your data set, each observation gets a single value. Lists, however, can contain vectors, and other more complex objects, as elements.\nThere are various ways to create lists. The most common is to use the list() function to “wrap” some object. map() always returns a list.\nWe can take a list column and, by applying an anonymous function to it with map(), create another list column. This is similar to taking a tibble and piping it into a function, like mutate(), which returns a new tibble to work with.\nYou can also use map_* functions to take a list column as an input and return an atomic vector – a column with a single value per observation – as an output.\n\nIf a function returns multiple values as a vector, you must use list() as a wrapper if you want to create a list-column.\nWriting functions\n\nOptimize usefulness by adding more formal arguments when needed. A function that only gives an option for n may not be as helpful as a function that allows us to enter options for a data set, variable, and n value.\nGive your arguments sensible names.\n\nBy default, a function returns the result of the last line of the body. Use return() to override this default.\nWhen starting a function, remember that smaller steps are easier than trying to build everything in one motion. In general: start by writing the body, test the body in a basic function, and then add formal arguments.\n\nUse double curly braces around vars, since R does not know how to interpret variable names when they are passed in an argument. The double curly braces tell R that var is a variable in a tibble.\nDistributions\n\nThe word “distribution” can mean two things. First, it is an object — a mathematical formula, an imaginary urn — from which you can draw values. Second, it is a list of such values.\n\nThe two most important aspects of a distribution are its center and its variability.\nThe median is often a more stable measure of the center than the mean. The mad (scaled median absolute deviation) is often a more stable measure of variation than the standard deviation.\nOutliers cause a lack a stability. In a distribution without outliers, the mean/median and mad/sd are so close in value that it does not matter much which ones you use."
  },
  {
    "objectID": "set-up.html",
    "href": "set-up.html",
    "title": "Set Up for Working on The Primer",
    "section": "",
    "text": "This document provides a guide in setting up R/RStudio to work on The Primer, both the book itself, PPDBS/primer, and the associated tutorial and data packages: PPDBS/primer.tutorials and PPDBS/primer.data. There are three steps:\nThe test which ensures that you have successfully completed this set up is to submit a PR for the TODO.txt file in the primer package which adds your name at the very top of the file. The PR should only change that file."
  },
  {
    "objectID": "set-up.html#computer-set-up",
    "href": "set-up.html#computer-set-up",
    "title": "Set Up for Working on The Primer",
    "section": "Computer Set Up",
    "text": "Computer Set Up\n\n\nInstall the latest released versions of R and RStudio. Install the usethis package.\n\nRead the Getting Started and Tools sections of The Primer and make sure your Git/Github is working. Read (and watch the videos from) Getting Used to R, RStudio, and R Markdown by Chester Ismay and Patrick C. Kennedy. Check out RStudio Essentials Videos. Most relevant for us are “Writing code in RStudio”, “Projects in RStudio” and “Github and RStudio”. The best reference for R/RStudio/Git/Github issues is always Happy Git and GitHub for the useR.\n\nMake sure that your Git/Github connections are good. If you have gone through the key chapters in Happy Git with R — as you should have — then these may already be OK. If not (or, even if you have), then you need to run usethis::git_sitrep().\n\n\n> library(usethis)   \n> git_sitrep()    \nGit config (global)   \n● Name: 'David Kane'   \n● Email: 'dave.kane@gmail.com'   \n● Vaccinated: FALSE   \nℹ See `?git_vaccinate` to learn more   \nℹ Defaulting to https Git protocol   \n● Default Git protocol: 'https'   \nGitHub   \n● Default GitHub host: 'https://github.com'   \n● Personal access token for 'https://github.com': '<discovered>'   \n● GitHub user: 'davidkane9'   \n● Token scopes: 'delete_repo, gist, notifications, repo, user, workflow'   \n● Email(s): 'dave.kane@gmail.com (primary)', 'dkane@fas.harvard.edu'   \n...   \nI left out the end of the output.\nIf the first part — Git config — seems messed up, execute:\n\nuse_git_config(user.name = \"David Kane\", user.email = \"dave.kane@gmail.com\")\n\nIf the second part seems messed up, try:\n\nusethis::create_github_token()\n\nand read about Github credentials. After you do, restart R and then run git_sitrep() again to make sure that things look like mine, more or less.\n\nInstall the renv package. You can read about the renv package here.\n\nIt is not critical to understand all the details of how renv works. The big picture is that it creates a set of libraries which will be used just for this project and whose versions are kept in sync between you and me.\n\nAt this point, you should have all the tools you need to contribute. If you have never done a pull request, however, you will need to learn more. Start by reading the help page. Read the whole thing! Don’t just skim it. These are important concepts for professional-level workflow. The usethis package is mostly providing wrappers around the underlying git commands. If you want to understand what is happening at a lower level, read this, but doing so is optional.\n\nAgain, with luck, you will only have to do these steps once.\n\nProve to yourself (and to me) that your set up is working by submittimg a pull request to me which simply adds your name to the top of one of the TODO.txt files. (See below for how to do this.)"
  },
  {
    "objectID": "set-up.html#project-set-up",
    "href": "set-up.html#project-set-up",
    "title": "Set Up for Working on The Primer",
    "section": "Project Set Up",
    "text": "Project Set Up\nYou will need to do the below steps at least one time. It is more likely, however, that you will do them dozens of times. If things are working, great! If they start not working, you can try to diagnose the problem. But, if you can’t, then you are in a nuke it from orbit scenario, which means that you start by deleting the current version of the package from two places: your computer, and your Github account. To delete the primer from your computer, put the R Studio project directory in the Trash. Make sure to also close out of the R Studio session after you delete it. If for some reason you cannot completely remove it, consider using the command $sudo rm -r dirname where you replace “dirname” with the path to primer on your computer! sudo and rm can be extremely dangerous when used together, so make sure to double check the command and/or do additional research. After you successfully remove it from your computer, go to your Github account and then go to Settings to delete the repo.\nKey steps:\n\nFork/download the target repo:\n\n\nlibrary(usethis)  \ncreate_from_github(\"PPBDS/primer\",   \n                    fork = TRUE,   \n                    destdir = \"/Users/davidkane/Desktop/projects/\",   \n                    protocol = \"https\")  \n\nThat is the repo for working on the book. If you are working on PPBDS/primer.data or PPBDS/primer.tutorials, you need to create_from_github() using those repos. You must change destdir to be a location on your computer. Indeed, professionals will generally have several different RStudio sessions open, each working on a different R project/package, each of which is connected to its own Github repo.\nFor your education, it is worth reading the help page for create_from_github(). The fork and protocal arguments may not really be necessary and, obviously, you should place the project in the location on your computer in which your other projects live. The command first forks a copy of PPBDS/primer to your Github account and then clone/downloads that fork to your computer.\nThis may seem like overkill, but, as Pro Git explains, it is how (essentially) all large projects are organized. With luck, you only have to issue this command once. After that, you are always connected, both to your fork and to the true repos, which live at github/com/PPBDS. Also, note that, if something ever gets totally messed up on your computer, you can just delete the project folder on your computer and the repo on your Github account and then start again. (If you have made changes that you don’t want to lose, just save the files with those changes to one side and then move them back after you have recreated the project.)\nNote that this command should automatically put you in a new RStudio session with the primer (or primer.tutorials or primer.data) RStudio project which resides on your computer\n\nThe next step is to get renv setup so that you are running the same package versions as everyone else. Run this once:\n\n\nlibrary(renv)\nrenv::restore()\n\nThis will install all the packages you need in the directory of this project. (This has no effect on your main library of R packages.) Restart your R session. Again, this means that you now have two separate installations of, for example, ggplot2. One is in the default place which your R sessions is by default pointed to. (In a different project without a renv directory, you can run .libPaths() to see where that is.) The second place that ggplot2 is installed is in the renv directory which lives in this project.\nNote that, for the most part, you won’t do anything with renv after this initial use. If you use error = TRUE in any code chunk, you will also need renv.ignore = TRUE in that code chunk, or you will get an annoying warning because renv can’t parse the code in that chunk.\nHowever, there are three other renv commands you might issue:\nrenv::status() just reports if anything is messed up. It won’t hurt anything.\nrenv::restore() looks at the renv.lock file and installs the packages it specifies. You will need to do this when I make a change to renv.lock, e.g., if I upgrade our version of ggplot2 or add a new package.\nrenv::snapshot() should only be issued if you know what you are doing. This changes the renv.lock file, which is something that, usually, only I do. Most common case for use would be if you need to add a new package to the project.\n\nCreate a branch to work from:\n\n\npr_init(branch = \"chapter-9\")\n\nMake sure the branch name is sensible. Again, this is a command that you only need to issue once, at least for our current workflow. You should always be “on” this branch, never on the default (master) branch. You can check this in the upper right corner of the git panel on R Studio.\nIn more professional settings, you will often work on several different branches at once. So, if you are comfortable, you should feel free to create more than one branch, use it, delete it and so on. Never work on the default branch, however. And, if you use multiple branches, be careful where you are and what you are doing."
  },
  {
    "objectID": "set-up.html#daily-work",
    "href": "set-up.html#daily-work",
    "title": "Set Up for Working on The Primer",
    "section": "Daily Work",
    "text": "Daily Work\n\nPull regularly:\n\n\npr_merge_main()\n\nIssue this command all the time. This is how you make sure that your repo and your computer is updated with the latest changes that have been made in the book. The word “upstream” is associated with the repos at PPBDS. The word “origin” is associated with the fork at your Github account. But, in general, you don’t need to worry about this. Just pull every time you sit down. (Just clicking the pull button is not enough. That only pulls from your repo, to which no changes have been made. It does not pull from PPBDS/primer, et al.) You issue this command multiple times a day.\n\nMake changes in the file you are editing. Knit to make sure the changes work. Commit with a message. Push to the repo on your Github account. And so on.\n\nAt some point, you will be ready to push to the PPBDS organization. However, you can’t do this directly. Instead, you must submit a pull request (PR). Because you are part of a larger project, these commands are slightly different than what you have done before, which has usually just been clicking on the pull (blue) and push (green) arrows in the Git pane in RStudio.\n\nIssue pull requests every few days, depending on how much work you have done and/or whether other people are waiting for something you have done.\n\n\npr_push()\n\nThis command bundles up a bunch of git commands (which you could do by hand) into one handy step. This command does everything needed to create a “pull request” — a request from you to me that I accept the changes you are proposing into the repo at PPBDS/primer — and then opens up the web page to show you. But your are not done! You must PRESS the green button on that web page, sometimes twice. Until then, the PR has not actually been created. pr_push() just does everything before that. The “pr” in pr_push() stands for pull request.\n\nI will leave aside for now issues associated with the back-and-forth discussions we might have around your pull request. I will probably just accept it. Your changes will go into the repos at PPBDS and then be distributed to everyone else when they run pr_merge_main().\nYou can now continue on. There is no need to wait for me to deal with your pull request. There is no need to fork/clone/download again. You don’t need to create a new branch, although many people do, with a branch name which describes what they are working on now. You just keep editing your files, knitting, and committing then pushing to your forked repo. When you feel you have completed another chunk of work, just run pr_push() again.\nRead the usethis setup help page at least once, perhaps after a week or two of working within this framework. It has lots of good stuff!"
  },
  {
    "objectID": "set-up.html#common-problems",
    "href": "set-up.html#common-problems",
    "title": "Set Up for Working on The Primer",
    "section": "Common Problems",
    "text": "Common Problems\n\n\n\nIn the immediate aftermath of this creation process, the blue/green arrows (in the Git panel) for pulling/pushing may be grayed out. This is a sign that the connection between your computer and your forked repo has not “settled in.” (I am not sure of the cause or even if this is the right terminology.) I think that just issuing your first pr_merge_main() fixes it. If not, it always goes away. Until it does, however, you can’t pull/push to your repo. That doesn’t really matter, however, since the key commands you need are pr_merge_main() and pr_push(), both of which always work immediately.\nAfter running pr_merge_main(), you will often see a bunch of files in your Git tab in the top right corner of Rstudio marked with an M (for Modified), including files which you know you did not edit. These are the files that have been updated on the “truth” — on PPBDS/primer — since your last pr_merge_main(). Since you pulled them directly from the PPBDS/primer repo, your forked repo sees all the changes other people have made and thinks that you made them. This is easily fixed, however — just commit all the changes to your forked repo. (Strangely, this seems to not always happen. If you don’t see this effect, don’t worry.)\nAlways run pr_merge_main() before committing a file. Otherwise, you may create lots of merge conflicts. If this happens, save a copy of the file(s) you personally were editing off to the side. Then, nuke it from orbit, following the instructions above. Repeat the Project Set Up process. Then move in your file(s) by hand into the new repo, and commit/push them as normal.\nWhen you submit a pull request to merge your work with the PPBDS repo, it won’t always be smiles and sunshine — every once in a while, you’ll run into merge conflicts. When these arise, it is because two parties work on a file separately and submit conflicting changes. This makes it hard for GitHub to “merge” your version with the other version. When this happens, find multiple adjacent “>”, “<”, and “=” signs in your document — these will show you where the conflicts occur. For more background on merge conflicts, read this.\n\nIf you see the above-mentioned conflicts in your document, do not submit a pull request. This will mess things up. Instead, first, go through your document, and make sure all the weird conflict indicators (<, >, and =) are removed. Second, decide what goes in that space. It might be the stuff you wrote. It might be the other stuff. It might be some combination of the two which you decide on. Whatever happens, you are making an affirmative choice about what should appear in the file at that location. Once all the merge conflicts are fixed, run pr_push() again.\n\n\npr_push() can be tricky. First, note that, if I have not accepted a (p)ull (r)equest which you have submitted, then your PR is still open. You can see it on Github. In fact, you can see all the closed/completed pull requests as well. If, while one PR is still open, you submit another pr_push(), then this will just be added to your current PR. And that is OK! We don’t need it to be separate.\n\nBut even if there is not an open PR, pr_push() can be tricky. The key thing to remember is that you must press a green button on Github for a new PR to be created. Normally, this is easy. Running pr_push() automatically (or perhaps after you run pr_view()) puts you in a browser and brings you to the correct Github page. Press the button and – presto! – you have created a PR. But, sometimes, the web page is different. It actually sends you back to an old pull request. When this happens, you need to click on the “Pull Request” tab above. This will take you to a new page, with a green button labeled “Compare & Pull Request”. Press that button.\n\nIf you end up needing to install a new package — which should be rare — just install it and then type renv::status() to confirm than renv is aware of the change. Then, type renv::snapshot(). This will update the renv.lock file to include the new package. You just commit/push the new version of renv.lock, and that shares the information with everyone else on the project. Never commit/push a modified renv.lock unless you know why it has changed.\nBe careful of committing garbage files like “.DS_Store”, which is a file created sometimes. Only commit changes which you understand. In the vast majority of cases your PRs will only involve one or two files."
  },
  {
    "objectID": "set-up.html#style-guide",
    "href": "set-up.html#style-guide",
    "title": "Set Up for Working on The Primer",
    "section": "Style Guide",
    "text": "Style Guide\n\nNever use just a single # after using it for the chapter title. The first subpart uses a ##. There should be 5 to 8 subparts for each chapter. Within each subpart, you may have sub-subparts, indicated with ###. There should be 3 to 10 of those. You may use #### if you like.\nSection headings (other than Chapter titles) are in sentence case (with only the first word capitalized, unless it is something always capitalized) rather than title case (in which all words except small words like “the” and “of” are capitalized). Chapter titles are in title case. Headings do not end with a period.\nNever hard code stuff like “A tibble with 336,776 rows and 19 columns.” What happens when you update the data? Instead, calculate all numbers on the fly, with “r scales::comma(x)” whenever x is a number in the thousands or greater. Example: “A tibble with ‘r scales::comma(nrow(x))’ rows and ‘r ncol(x)’ columns.”\n“We” are writing this book.\nPackage names are in bold: ggplot2 is a package for doing graphics. In general, we reserve bolding for package names. Use italics for emphasis in other contexts.\nR code, anything you might type in the console, is always within backticks. Example: mtcars is a built-in dataset.\nFunction names always include the parentheses: we write pivot_wider(), not pivot_wider.\nAdd lots of memes and videos and cartoons.\nDo not use code chunk names because it messes up building the book because of limits in bookdown.\nMake ample use of comments, placed with the handy CMD-Shift-/ shortcut. These are notes for everyone else working on the chapter, and for future you.\nAll tables should be created with the gt package.\nAll images and gifs are loaded with knitr::include_graphics().\nOnly code chunk options allowed are include = FALSE, echo = FALSE, fig.cap = “This is my cap” and message = FALSE when loading packages like ggplot2 since it prevents all the messages from printing out.\nInterim data sets should be called x or something sensible to the situation, like ch7 for a data set you are working with in Chapter 7. Do not use names like data and df, both of which are R commands.\nStudents are sometimes tentative. Don’t be! Edit aggressively. If you don’t like what is there, delete it. (If I disagree with your decision, I can always get the text back from Github.) Move things around. Make the chapter yours, while keeping to the style of the other chapters. Note that 90% of the prose here was not written by me. Cut anything you don’t like.\nIf you make an mp4, you can convert it to .gif using https://convertio.co/mp4-gif.\nEverything is Bayesian. The confidence interval for a regression means that there is a 95% chance that the true value lies within that interval. Use Rubin Causal Model and potential outcomes to define precisely what “true value” you are talking about. And so on.\n\nStray thoughts\nEvery chapter 5+ begins with a problem, and the decision we must make. These are often toy, highly stylized problems. The decisions are not realistic. But, in structure, these problems parallel the real problems that people face, the actual decisions which they must make.\nThe problem is specified at the end of the “preamble,” the untitled part of the chapter after the title and before the first subpart. Example from Chapter 8:\n\nA person arrives at a Boston commuter station. The only thing you know is their political party. How old are they? Two people arrive: A Democrat and a Republican. What are the odds that the Democrat is 10% older than the Republican?\n\n\nA different person arrives at the station. You know nothing about them. What will their attitude toward immigration be if they are exposed to Spanish-speakers on the platform? What will it be if they are not? How certain are you?\n\nIs this an actual problem that someone might face? No! But it is like such problems. The first requires the creation of a predictive model. The second necessitates a causal model. The rest of the chapter teaches the reader how to create such models. The end of the chapter harkens back to the questions from the beginning.\nMight it be nice to put more meat on the story than that? Perhaps. In an ideal world, the “decision” you faced would be more complex than just playing the prediction game. Begin with a decision. What real world problem are you trying to solve? What are the costs and benefits of different approaches? What unknown thing are you trying to estimate? With Sampling, it might be: How many people should I call? With estimating one parameter — like vote share as the ballots come in — it might be: How much should I bet on the election outcome?\nThe data we have might not be directly connected to our problem. For example, we might be running a Senate campaign and trying to decide what to spend money on. The Spanish-speakers-on-a-train-platform data set is not directly related to that problem, but it isn’t unrelated. Indeed, the first theme of “validity” is directly related to this issue: Is the data we have relevant to the problem we want to solve?"
  },
  {
    "objectID": "00-getting-started.html",
    "href": "00-getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "The world confronts us. Make decisions we must."
  },
  {
    "objectID": "00-getting-started.html#summary",
    "href": "00-getting-started.html#summary",
    "title": "Getting Started",
    "section": "Summary",
    "text": "Summary\nYou should have done the following:\n\nInstalled the latest versions of R and RStudio.\nInstalled, from CRAN, the remotes package:\n\n\ninstall.packages(\"remotes\")\n\n\nInstalled, from Github, the primer.tutorials package:\n\n\nremotes::install_github(\"PPBDS/primer.tutorials\")\n\n\nLearned some basic terminology that will help you when you read the rest of the Primer.\n\nLet’s get started."
  },
  {
    "objectID": "03-data.html",
    "href": "03-data.html",
    "title": "3  Data",
    "section": "",
    "text": "You can never look at the data too much. – Mark Engerman"
  },
  {
    "objectID": "03-data.html#introduction",
    "href": "03-data.html#introduction",
    "title": "3  Data",
    "section": "\n3.1 Introduction",
    "text": "3.1 Introduction\n\n\n\n\n\n\n\n\n\n\n\nGetting data into and out of R is a major part of any real world data science project. There are multiple data formats that you use to transport this data, each with their own positives and negatives.\nStart by loading the packages which we will need in this chapter.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.1\n\n\nWarning: package 'tibble' was built under R version 4.2.1\n\nlibrary(primer.data)\nlibrary(dbplyr)\n\nWarning: package 'dbplyr' was built under R version 4.2.1\n\nlibrary(janitor)\n\nYou can find most of the files that we use for this chapter here. We can also access these files by saving the URL to R and then using it so that we can download and access the files located on the internet.\n\ngithub_url <- \"https://raw.githubusercontent.com/PPBDS/primer.tutorials/master/inst/tutorials/033-data-files/data/\""
  },
  {
    "objectID": "03-data.html#reading-and-writing-files",
    "href": "03-data.html#reading-and-writing-files",
    "title": "3  Data",
    "section": "\n3.2 Reading and writing files",
    "text": "3.2 Reading and writing files\n\n\n\n\nChoose your file formats wisely.\n\n\n\n\n\nThe first method that we can use to import data is by using a file. You’ve likely downloaded files before, whether it’s a game’s EXE file, an image’s JPG file, or an essay’s PDF file. At their core, those files are just data. The JPG file has a bunch of data about the colors in the image and the PDF file has a bunch of data about the text. So we can use these files to store data from experiments or surveys so that we can then analyze that data later or share that data with other people.\nIn this section, we’ll be going over the common file formats and how we can pull data from these files into R or create new files so that we can share them with other people.\n\n3.2.1 Text files\nThe most common type of data text file is “CSV,” which stands for comma separated value. In other words, CSV files are files whose values are separated by commas. Each comma from the csv file corresponds to a column, and the column names are, by default, taken from the first line of the file.\nCSV files (and their counterparts) are easily transferable between computers and programming languages and are extremely simple because they’re effectively just text files that have a special format. Additionally, they’re very easy to parse through small amounts of data because they’re easily readable by humans and computers. However, due to their simple nature, CSV files are only able to move basic data and only as text values. They also have poor support for special characters like commas, which can make the dataset harder to organize and understand by adding a new column for a few specific entries. This make CSV files good for sharing data between computers and languages, but not efficient for transporting or saving large amounts of data.\n\n3.2.1.1 Reading and writing from a CSV file\nHere’s an example of a CSV file and what it looks like. Use read_csv() from the readr package — which is one of the main packages within the tidyverse collection of packages — to load the data into R. The file argument is the file path for the CSV file.\n\n# We can access files either by using their URL or their file path. In this\n# case, we're using the GitHub URL to access the database. This is done by\n# pasting the file name into our URL using the paste0() command.\n\nfile_1 <- paste0(github_url, \"test_1.csv\")\n\nread_csv(file = file_1)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n\n\nUse write_csv() to save a tibble to a csv file. write_csv() has two main arguments: x and file. The x argument is the data set that you want to save. The file argument is the file path to which you want to save the file. The end of the file argument is the name that you want to use for the file.\n\ncsv_data <- tribble(\n  ~ `a`, ~ `b`, ~ `c`,\n      1,     2,     3,\n      4,     5,     6)\n\nwrite_csv(x = csv_data, file = \"my_csv.csv\")\n\nWhen we read the csv file again, the data shows up. This is useful for saving information both to share and for your own projects.\n\nread_csv(\"my_csv.csv\")\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n\nWhen you want to remove a file from your system, use file.remove().\n\nfile.remove(\"my_csv.csv\")\n\n[1] TRUE\n\n\nSometimes, CSV files will not be what you want. Maybe they have the wrong column names, have information at the top of the file, or have comments interspersed within the file.\n\n3.2.1.2 skip\n\nConsider the following csv file: test_2.csv. Here’s what it looks like as a text file:\n\n\n[1] \"Top two rows consist of junk which\"        \n[2] \"we don't care about. Data starts on row 3.\"\n[3] \"a,b,c\"                                     \n[4] \"9,8,7\"                                     \n[5] \"4,5,6\"                                     \n\n\nAs you can see, there is text at the top of this file. Often times information about how data was collected, or other relevant information, is included at the top of the data file. However, read_csv() can’t differentiate between this text and the data that we want to read, causing it to fail and output gibberish.\n\n# You can also get a csv file by using the URL of the file. This won't work for\n# all file types though.\n\nfile_2 <- paste0(github_url, \"test_2.csv\")\n\nread_csv(file_2)\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 4 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Top two rows consist of junk which\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 4 × 1\n  `Top two rows consist of junk which`      \n  <chr>                                     \n1 we don't care about. Data starts on row 3.\n2 a,b,c                                     \n3 9,8,7                                     \n4 4,5,6                                     \n\n\nWe can use the skip argument to skip the first 2 text lines and allow read_csv() to work.\n\nread_csv(file = file_2,\n         skip = 2)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     9     8     7\n2     4     5     6\n\n\nNow that we’ve gotten rid of the warnings, let’s look at the other message R sends: the column specification message.\n\n3.2.1.3 col_types\n\nThe column specification message is a message that R sends to tell you what data types it is using for the column.\nData types are the types discussed in Chapter 2 Wrangling, such as characters, factors, integers, and dates. When we use a tibble, each column has to have a specific type of data. In this example, all of the columns have numbers in them. If there are characters in the column, the columns are going to have a character data type.\nTo get rid of the column specification message, use the col_types() argument and specify the data types. You can do this by just copying the column specification message and putting it as the col_types() argument.\n\nread_csv(file = file_2,\n         skip = 2,\n         col_types = cols(a = col_double(),\n                          b = col_double(),\n                          c = col_double()))\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     9     8     7\n2     4     5     6\n\n\nYou can also change the column arguments so that you get the data type that you want. Take test_7.csv.\n\ntest_7 <- paste0(github_url, \"test_7.csv\")\n\nread_csv(test_7)\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): student\ndbl (1): grade\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n  grade student\n  <dbl> <chr>  \n1     1 Sam    \n2     5 Becca  \n\n\nLet’s try parsing this so that the student column is a factor and the grade column is an integer.\n\nread_csv(test_7,\n         col_types = cols(grade = col_integer(),\n                          student = col_factor()))\n\n# A tibble: 2 × 2\n  grade student\n  <int> <fct>  \n1     1 Sam    \n2     5 Becca  \n\n\nBy being clever with the columns, we can make our lives easier down the line when we graph the data.\nWe can also manipulate other arguments for CSV files.\n\n3.2.1.4 col_names and clean_names()\n\nLet’s try changing the column names in the test_3.csv file.\nAs you can see below, the file doesn’t have any column names, resulting in the first row being considered as the names for the rest of the file.\n\nfile_3 <- paste0(github_url, \"test_3.csv\")\n\nread_csv(file_3)\n\nRows: 1 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): 11, 21, 33\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 1 × 3\n   `11`  `21`  `33`\n  <dbl> <dbl> <dbl>\n1     4     5     6\n\n\nWe can fix this by changing the col_names argument.\n\nread_csv(file_3, col_names = c(\"a\", \"b\", \"c\"))\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1    11    21    33\n2     4     5     6\n\n\nYou can also create names automatically by setting col_names to FALSE\n\nread_csv(file_3, col_names = FALSE)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): X1, X2, X3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n     X1    X2    X3\n  <dbl> <dbl> <dbl>\n1    11    21    33\n2     4     5     6\n\n\nChanging the names of the columns allows for you to call on the columns later when the data is actually a tibble. Setting the column names to something that you can understand makes it much easier to understand your code later on.\nBut what if we have good column names, they just aren’t formatted correctly? Let’s look at test_4.csv for an example.\n\nfile_4 <- paste0(github_url, \"test_4.csv\")\n\nfile_4_tibble <- read_csv(file_4)\n\nNew names:\nRows: 3 Columns: 4\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" dbl\n(4): one powers, Two_Powers...2, 3_Powers, Two_Powers...4\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `Two_Powers` -> `Two_Powers...2`\n• `Two_Powers` -> `Two_Powers...4`\n\nfile_4_tibble\n\n# A tibble: 3 × 4\n  `one powers` Two_Powers...2 `3_Powers` Two_Powers...4\n         <dbl>          <dbl>      <dbl>          <dbl>\n1            1              2          3              2\n2            1              4         81              5\n3            1              8         27              8\n\n\nAs you can see, while the function does compile the names aren’t easy to access. It’s possible to access the column by using the ` tickmark like below:\n\nfile_4_tibble$`one powers`\n\n[1] 1 1 1\n\n\nBut that can still cause problems down the line and it’s just annoying to use the backticks every time you want a column name. This is when we can use the clean_names() function from the janitor package. It essentially formats the column names so that they follow the underscore separated naming convention and are all unique.\n\nfile_4_tibble |> \n  clean_names()\n\n# A tibble: 3 × 4\n  one_powers two_powers_2 x3_powers two_powers_4\n       <dbl>        <dbl>     <dbl>        <dbl>\n1          1            2         3            2\n2          1            4        81            5\n3          1            8        27            8\n\n\nThis cleans the column names, saving you time when your file has a large amount of columns or you need to type a lot of column names. These issues are more common when you’re pulling data off of the internet as that’s normally dirty data and can have a lot of columns formatted in weird ways.\n\n\n3.2.1.5 na\n\nAnother feature of read_csv() is the na argument.\n\nfile_5 <- paste0(github_url, \"test_5.csv\")\n\nread_csv(file_5)\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): b\ndbl (2): a, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a b         c\n  <dbl> <chr> <dbl>\n1     1 .         3\n2     4 5         6\n\n\ntest_5.csv is missing a value, and it uses a . as a substitute. This makes the computer think that the period is the actual value of the data point, which is obviously not true (we want numbers instead). By default, read_csv() treats white space like spaces or tabs as a missing value, but you can set this argument directly as well.\n\nread_csv(file_5,\n         na = \".\")\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1    NA     3\n2     4     5     6\n\n\n\n3.2.1.6 comment\n\nYou can also tell the code to ignore comment lines, which may be common in a csv file that was written by a human and as such has comments in it. test_6.csv is a perfect example of this. Here’s what it looks like:\n\n\n[1] \"a,b,c\"                          \"# This is a comment line\"      \n[3] \"98,99,100\"                      \"# Here is another comment line\"\n[5] \"4,5,6\"                         \n\n\nBy setting the comment argument, we’re able to skip lines that have a certain starting point. In this case, the comment is the # sign, so we just need to include that in.\n\nread_csv(file_6, comment = \"#\")\n\nRows: 2 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (3): a, b, c\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1    98    99   100\n2     4     5     6\n\n\nIf you ever want to skip certain lines, just use the comment argument in order to designate it as something to be skipped. This is best used with cases where the read_csv() command will not compile without it.\n\n3.2.1.7 read_delim()\n\nSo far, we’ve covered how we can organize CSV data, but at it’s core we’re working with comma separated values. But what happens when we want to read data from something that doesn’t use commas to separate the values?\nWhen our tabular data comes in a different format, we can use the read_delim() function instead. For example, a different version of test_6.csv could exist that has no column names and uses pipes (|) as the delimiter instead of commas.\nHere’s another file named delim_1, which uses the | to separate the lines instead of a comma like a normal CSV file.\n\n\n[1] \"population|town\"   \"150|Cambridge, MA\" \"92|Newton, MA\"    \n\n\nWith read_delim(), we specify the first argument as the path to the file, as done with read_csv(). Then we provide values to the delim argument to have the code use | as the separator instead of the comma.\n\n# Because delim_1 uses pipes to separate values, we can just use that as our\n# delim value. However, for more complex symbols like tab, we use something\n# different like \"\\\\t\". This varies for every symbol, but you can find most\n# delim values on the internet.\n\ndelim_1 <- paste0(github_url, \"delim_1.txt\")\n\nread_delim(delim_1, delim = \"|\")\n\nRows: 2 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"|\"\nchr (1): town\ndbl (1): population\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 2 × 2\n  population town         \n       <dbl> <chr>        \n1        150 Cambridge, MA\n2         92 Newton, MA   \n\n\nYou can often find CSV files on websites like kaggle as well as by exporting from an Excel spreadsheet. Keep in mind that data imported off of the internet is often very messy, so try to use some of the functions listed here to clean it up. For a full list of arguments and in-depth documentation about the read_csv() function, please visit this website.\n\n\n3.2.2 Excel files\nExcel is a spreadsheet program that use tables to analyze, store, or manipulate data. The tables are composed of cells which include text, numbers, or formulas. Excel files have the filename extensions .xls or .xlsx, and they’re capable of storing additional things that you cannot store in a .csv file such as fonts, text formatting, graphics, etc.\nIn order to write excel files you have to install complex packages, and they are hard to create. Writing excel files is beyond the scope of this Primer.\nThis makes Excel files valuable because they’re commonly accepted and usable (Microsoft Excel is a very common program), but they’re also hard to use because you can’t write new data into them. As such, Excel files are common for data that was originally in Excel, like accounting data or other spreadsheet applications.\nReading Excel files is easy. To do so, we use the read_excel() function from the readxl package.\n\nlibrary(readxl)\n\n# Unfortunately, it is not possible to read Excel files directly from the web.\n# So we download the file by hand and then read it in from the current working\n# directory. Note that the \"proper\" way of handling this would be to create a\n# temp directory with tempdir(), download the file into that directory, read it,\n# and then delete the temp directory. That way, you would not have random\n# downloaded files hanging around.\n\n# The mode = \"wb\" is a necessary addition for Windows users because Windows is\n# weird. It's not necessary on MacOS and may cause an error as well.\n\ndownload.file(url = paste0(github_url, \"excel_1.xlsx\"), \n              destfile = \"example_excel.xlsx\", mode = \"wb\")\n\n\nread_excel(path = \"example_excel.xlsx\")\n\n# A tibble: 2 × 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2     3\n2     4     5     6\n\n\nIf the .xlsx file has multiple sheets, you have to use the sheet argument to specify the sheet number or name. The read_excel() function also has arguments similar to the read_csv() function such as col_names, col_types, and na.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 RDS files\nOne very important aspect of R is saving objects into RDS files, which store a single R object into a file. These files allow us to save R objects such as plots and tibbles into R and then reload the object that it contains later without re-running the code that made it. This is especially useful for when you’re dealing with bulk data and want to save the plot that comes with it for later so that you don’t have to do all of the data wrangling and plotting again. With a RDS file, you save the entire object, allowing you to do more things with it later without having to go through all of the code. However, RDS files are limited to R projects only, and they’re incomprehensible both to human eyes and to other programming languages. This makes RDS files ideal for saving objects temporarily in your own project or for sharing objects to other R users.\nTake the following R object, a graph of the iris data set.\n\niris_p <- iris |> \n  ggplot(aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_jitter() +\n  labs(title = \"Sepal Dimensions of Various Species of Iris\",\n       x = \"Sepal Length\",\n       y = \"Sepal Width\")\niris_p\n\n\n\n\nWhen we save to a RDS file, we use the function write_rds(). Just like write_csv(), this function has two main arguments: x and file. The x argument is the object that you want to save. The file argument is the file path where you want to save the file. This determines the name that you use for the file.\n\nwrite_rds(x = iris_p, file = \"iris_p.rds\")\n\nread_rds() reads the file back into R. Just like read_csv() read_rds() has one main argument, which is the path to the file that you are wanting to read into R.\n\nread_rds(file = \"iris_p.rds\")\n\n\n\n\nWe can then use that R object in more operations, such as adding a trend line.\n\nrds_p <- read_rds(file = \"iris_p.rds\")\nrds_p + \n  geom_smooth(method = \"loess\",\n              formula = y ~ x,\n              se = FALSE)\n\n\n\n\nBy saving the iris_p plot in a RDS file, we eliminate the time needed to calculate and generate that plot again because we can use the saved information. We can then use the object by reading the file back into r and using it like any normal plot, adding new layers and doing new operations.\nThere are also .Rdata files which can store multiple objects, but RDS files can accomplish a similar task. This makes it much easier to use RDS files for anything that you want to keep in R.\n\n3.2.4 JSON\nAn increasingly common format for sharing data is JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and from which you can obtain data.\nJSON files are a minimal readable format that structures data, and they’re commonly used to transmit data between a server and a web application like a website. For people familiar with the Javascript coding language, you’ll likely see the similarities between the JSON file format and the Javascript syntax. This makes JSON files ideal for internet transport, but they don’t see much use within a project like RDS files do.\nThe functions fromJSON() and toJSON() allow you to convert between R objects and JSON. Both functions come from the jsonlite package.\n\nlibrary(jsonlite)\n\nThe function toJSON() converts a tibble to JSON format. Consider the example_1 tibble:\n\nexample_1 <- tibble(name= c(\"Miguel\", \"Sofia\", \"Aya\", \"Cheng\"), \n                    student_id = 1:4, exam_1 = c(85, 94, 87, 90), \n                    exam_2 = c(86, 93, 88, 91))\n\nexample_1\n\n# A tibble: 4 × 4\n  name   student_id exam_1 exam_2\n  <chr>       <int>  <dbl>  <dbl>\n1 Miguel          1     85     86\n2 Sofia           2     94     93\n3 Aya             3     87     88\n4 Cheng           4     90     91\n\n\n\n# The pretty argument adds indentation and whitespace when TRUE.\n\ntoJSON(example_1, pretty = TRUE) \n\n[\n  {\n    \"name\": \"Miguel\",\n    \"student_id\": 1,\n    \"exam_1\": 85,\n    \"exam_2\": 86\n  },\n  {\n    \"name\": \"Sofia\",\n    \"student_id\": 2,\n    \"exam_1\": 94,\n    \"exam_2\": 93\n  },\n  {\n    \"name\": \"Aya\",\n    \"student_id\": 3,\n    \"exam_1\": 87,\n    \"exam_2\": 88\n  },\n  {\n    \"name\": \"Cheng\",\n    \"student_id\": 4,\n    \"exam_1\": 90,\n    \"exam_2\": 91\n  }\n] \n\n\nThe function fromJSON() converts JSON format to a tibble.\n\njson_format_ex <-\n'[\n  {\"Name\" : \"Mario\", \"Age\" : 32, \"Occupation\" : \"Plumber\"}, \n  {\"Name\" : \"Peach\", \"Age\" : 21, \"Occupation\" : \"Princess\"},\n  {},\n  {\"Name\" : \"Bowser\", \"Occupation\" : \"Koopa\"}\n]'\n\nfromJSON(json_format_ex) \n\n    Name Age Occupation\n1  Mario  32    Plumber\n2  Peach  21   Princess\n3   <NA>  NA       <NA>\n4 Bowser  NA      Koopa\n\n\nMake sure to follow the JSON format exactly when you’re writing JSON files, as the format is what makes them special and what allows them to work."
  },
  {
    "objectID": "03-data.html#databases",
    "href": "03-data.html#databases",
    "title": "3  Data",
    "section": "\n3.3 Databases",
    "text": "3.3 Databases\n\n\n\n\nDROP TABLE is a particularly infamous SQL command\n\n\n\n\nDatabases are one of the most common methods of storing data, as they are capable of storing large amounts of data while also allowing for multiple people to access and change them at the same time. Think of these like a giant book with a bunch of tables that hold data. These are useful by themselves, but with the advent of computers we are now able to use relational databases.\nA relational database is a database that has the tables interact with one another based on common data, allowing you to create custom tables from an existing set of records. For example, a relational database may hold multiple tables that use the same ID to keep track of different information, like one table having an ID and a movie name while another has the ID and the rating. You can then combine those two in your code, creating a table with the ID, name, and rating. This allows the person who made the database to easily put in new data as well as allow you to pull out some data at a time without loading the entire database.\nIt’s common to use and interact with these databases in the real world due to most businesses using relational databases to keep track of their data and to update it at their leisure. It’s not uncommon for databases to hold thousands or even millions of rows due to the need to keep track of data.\nIn this section, we’ll be going over how to pull data from these databases and how to interact with the data without using the entire database.\n\n3.3.1 Reading data from a SQLite database\nSQLite is probably the simplest relational database that one can use in combination with R. SQLite databases are self-contained and usually stored and accessed locally on one computer, instead of on the internet or through the cloud. Data is usually stored in a file with a .db extension. Similar to Excel files, these are not plain text files and cannot be read in a plain text editor.\nThe first thing you need to do to read data into R from a database is to connect to the database. We do that using dbConnect() function from the DBI (database interface) package. This does not read in the data, but simply tells R where the database is and opens up a communication channel.\n\nlibrary(DBI)\nlibrary(RSQLite)\n\nWarning: package 'RSQLite' was built under R version 4.2.1\n\n# This example uses a different github URL, so you can't use the paste0() trick\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/can_lang.db?raw=true\",\n              destfile = \"example_db.db\", mode = \"wb\")\n\ncon_lang_data <- dbConnect(RSQLite::SQLite(), \"example_db.db\")\n\nOften relational databases have many tables, and their power comes from the useful ways they can be joined. Thus anytime you want to access data from a relational database, you need to know the table names. You can get the names of all the tables in the database using dbListTables().\n\ntables <- dbListTables(con_lang_data)\ntables\n\n[1] \"lang\"\n\n\nWe only get one table name returned, which tells us that there is only one table in this database. To reference a table in the database to do things like select columns and filter rows, we use the tbl() function from the dbplyr package. The package dbplyr allows us to work with data stored in databases as if they were local data frames, which is useful because we can do a lot with big datasets without actually having to bring these vast amounts of data into your computer!\n\nlang_db <- tbl(con_lang_data, \"lang\")\nlang_db\n\n# Source:   table<lang> [?? x 6]\n# Database: sqlite 3.38.5 [C:\\Users\\tejas\\OneDrive\\Documents\\RProjects\\primer_quarto\\example_db.db]\n   category                              langu…¹ mothe…² most_…³ most_…⁴ lang_…⁵\n   <chr>                                 <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 Aboriginal languages                  Aborig…     590     235      30     665\n 2 Non-Official & Non-Aboriginal langua… Afrika…   10260    4785      85   23415\n 3 Non-Official & Non-Aboriginal langua… Afro-A…    1150     445      10    2775\n 4 Non-Official & Non-Aboriginal langua… Akan (…   13460    5985      25   22150\n 5 Non-Official & Non-Aboriginal langua… Albani…   26895   13135     345   31930\n 6 Aboriginal languages                  Algonq…      45      10       0     120\n 7 Aboriginal languages                  Algonq…    1260     370      40    2480\n 8 Non-Official & Non-Aboriginal langua… Americ…    2685    3020    1145   21930\n 9 Non-Official & Non-Aboriginal langua… Amharic   22465   12785     200   33670\n10 Non-Official & Non-Aboriginal langua… Arabic   419890  223535    5585  629055\n# … with more rows, and abbreviated variable names ¹​language, ²​mother_tongue,\n#   ³​most_at_home, ⁴​most_at_work, ⁵​lang_known\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAlthough it looks like we just got a data frame from the database, we didn’t! It’s a reference, showing us data that is still in the SQLite database (note the first two lines of the output). It does this because databases are often more efficient at selecting, filtering and joining large data sets than R. And typically, the database will not even be stored on your computer, but rather a more powerful machine somewhere on the web. So R is lazy and waits to bring this data into memory until you explicitly tell it. To do so, we use the collect() function.\nHere we will filter for only rows in the Aboriginal languages category according to the 2016 Canada Census, and then use collect() to finally bring this data into R as a data frame.\n\naboriginal_lang_db <- filter(lang_db, category == \"Aboriginal languages\")\naboriginal_lang_db\n\n# Source:   SQL [?? x 6]\n# Database: sqlite 3.38.5 [C:\\Users\\tejas\\OneDrive\\Documents\\RProjects\\primer_quarto\\example_db.db]\n   category             language                 mothe…¹ most_…² most_…³ lang_…⁴\n   <chr>                <chr>                      <dbl>   <dbl>   <dbl>   <dbl>\n 1 Aboriginal languages Aboriginal languages, n…     590     235      30     665\n 2 Aboriginal languages Algonquian languages, n…      45      10       0     120\n 3 Aboriginal languages Algonquin                   1260     370      40    2480\n 4 Aboriginal languages Athabaskan languages, n…      50      10       0      85\n 5 Aboriginal languages Atikamekw                   6150    5465    1100    6645\n 6 Aboriginal languages Babine (Wetsuwet'en)         110      20      10     210\n 7 Aboriginal languages Beaver                       190      50       0     340\n 8 Aboriginal languages Blackfoot                   2815    1110      85    5645\n 9 Aboriginal languages Carrier                     1025     250      15    2100\n10 Aboriginal languages Cayuga                        45      10      10     125\n# … with more rows, and abbreviated variable names ¹​mother_tongue,\n#   ²​most_at_home, ³​most_at_work, ⁴​lang_known\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\naboriginal_lang_data <- collect(aboriginal_lang_db)\naboriginal_lang_data\n\n# A tibble: 67 × 6\n   category             language                 mothe…¹ most_…² most_…³ lang_…⁴\n   <chr>                <chr>                      <dbl>   <dbl>   <dbl>   <dbl>\n 1 Aboriginal languages Aboriginal languages, n…     590     235      30     665\n 2 Aboriginal languages Algonquian languages, n…      45      10       0     120\n 3 Aboriginal languages Algonquin                   1260     370      40    2480\n 4 Aboriginal languages Athabaskan languages, n…      50      10       0      85\n 5 Aboriginal languages Atikamekw                   6150    5465    1100    6645\n 6 Aboriginal languages Babine (Wetsuwet'en)         110      20      10     210\n 7 Aboriginal languages Beaver                       190      50       0     340\n 8 Aboriginal languages Blackfoot                   2815    1110      85    5645\n 9 Aboriginal languages Carrier                     1025     250      15    2100\n10 Aboriginal languages Cayuga                        45      10      10     125\n# … with 57 more rows, and abbreviated variable names ¹​mother_tongue,\n#   ²​most_at_home, ³​most_at_work, ⁴​lang_known\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWhy bother to use the collect() function? The data looks pretty similar in both outputs shown above. And dbplyr provides lots of functions similar to filter() that you can use to directly feed the database reference (i.e. what tbl() gives you) into downstream analysis functions (e.g., ggplot2 for data visualization and lm for linear regression modeling). However, this does not work in every case; look what happens when we try to use nrow to count rows in a data frame:\n\nnrow(aboriginal_lang_db)\n\n[1] NA\n\n\nor tail to preview the last 6 rows of a data frame:\ntail(aboriginal_lang_db)\n## Error: tail() is not supported by sql sources\nThese functions only work when we use the version that we used collect() on:\n\nnrow(aboriginal_lang_data)\n\n[1] 67\n\n\n\ntail(aboriginal_lang_data)\n\n# A tibble: 6 × 6\n  category             language                  mothe…¹ most_…² most_…³ lang_…⁴\n  <chr>                <chr>                       <dbl>   <dbl>   <dbl>   <dbl>\n1 Aboriginal languages Tahltan                        95       5       0     265\n2 Aboriginal languages Thompson (Ntlakapamux)        335      20       0     450\n3 Aboriginal languages Tlingit                        95       0      10     260\n4 Aboriginal languages Tsimshian                     200      30      10     410\n5 Aboriginal languages Wakashan languages, n.i.…      10       0       0      25\n6 Aboriginal languages Woods Cree                   1840     800      75    2665\n# … with abbreviated variable names ¹​mother_tongue, ²​most_at_home,\n#   ³​most_at_work, ⁴​lang_known\n\n\nIn order to delete and stop using an SQLite server, you need to first disconnect the file from the connection be using dbDisconnect() and passing in the connection object as an argument. You can then safely delete the database file from your computer by using file.remove().\n\ndbDisconnect(con_lang_data)\nfile.remove(\"example_db.db\")\n\nAdditionally, some operations will not work to extract columns or single values from the reference given by the tbl function. Thus, once you have finished your data wrangling of the tbl() database reference object, it is advisable to bring it into your local machine’s memory using collect() as a data frame.\n\nWarning: Usually, databases are very big! Reading the object into your local machine may give an error or take a lot of time to run so be careful if you plan to do this!\n\n\n3.3.2 Interacting with SQLite databases\nNow that we’ve figured out how to get data from a database, let’s look at how to wrangle data within that database.\nBecause databases normally contain large amounts of data, it’s advisable to do your wrangling before you use collect() to transform the database table into a tibble. This stops you from pulling large amounts of data onto your computer and just ignoring it.\nSo let’s try pulling in a database and see how we can manipulate it.\n\nlibrary(DBI)\nlibrary(RSQLite)\n\n# Again, use mode = \"wb\" if you're using a Windows operating system.\n\ndownload.file(url = \"https://github.com/PPBDS/primer/blob/master/03-data/data/chinook.db?raw=true\",\n             dest = \"chinook.db\", mode = \"wb\")\n\ncon <- dbConnect(RSQLite::SQLite(), \"chinook.db\")\n\nFirst off, look at what data this database holds.\n\ndbListTables(con)\n\n [1] \"albums\"          \"artists\"         \"customers\"       \"employees\"      \n [5] \"genres\"          \"invoice_items\"   \"invoices\"        \"media_types\"    \n [9] \"playlist_track\"  \"playlists\"       \"sqlite_sequence\" \"sqlite_stat1\"   \n[13] \"tracks\"         \n\n\nThere are 13 tables in this database. Let’s access the first couple of tables, just so that we can get a good look at them and see how they’re structured.\n\nalbums <- tbl(con, \"albums\")\nalbums\n\n# Source:   table<albums> [?? x 3]\n# Database: sqlite 3.38.5 [C:\\Users\\tejas\\OneDrive\\Documents\\RProjects\\primer_quarto\\chinook.db]\n   AlbumId Title                                 ArtistId\n     <int> <chr>                                    <int>\n 1       1 For Those About To Rock We Salute You        1\n 2       2 Balls to the Wall                            2\n 3       3 Restless and Wild                            2\n 4       4 Let There Be Rock                            1\n 5       5 Big Ones                                     3\n 6       6 Jagged Little Pill                           4\n 7       7 Facelift                                     5\n 8       8 Warner 25 Anos                               6\n 9       9 Plays Metallica By Four Cellos               7\n10      10 Audioslave                                   8\n# … with more rows\n# ℹ Use `print(n = ...)` to see more rows\n\nartists <- tbl(con, \"artists\")\nartists\n\n# Source:   table<artists> [?? x 2]\n# Database: sqlite 3.38.5 [C:\\Users\\tejas\\OneDrive\\Documents\\RProjects\\primer_quarto\\chinook.db]\n   ArtistId Name                \n      <int> <chr>               \n 1        1 AC/DC               \n 2        2 Accept              \n 3        3 Aerosmith           \n 4        4 Alanis Morissette   \n 5        5 Alice In Chains     \n 6        6 Antônio Carlos Jobim\n 7        7 Apocalyptica        \n 8        8 Audioslave          \n 9        9 BackBeat            \n10       10 Billy Cobham        \n# … with more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs you can see, these tables both have a common column: the ArtistId column. However, they all have different information linked to that ID, such as “albums” having the albums that artist produced and “artists” having the name of that artist.\nHere is the full relationship diagram of the database that we’re using.\nLet’s go over some of the operations that you can do on a SQLite database.\n\n3.3.2.1 Using dbplyr\nWe’ve already seen how you can use the dbpylr package to get a table from a database, but let’s look at how you can also use it to do operations on databases as well.\nThe dbpylr package allows you to use many of the same functions from the tidyverse like select(), filter(), mutate() without any issues, making it the easiest to do operations with.\nHere’s an example of using the dbpylr package to get the number of albums each artist has created.\n\n# You can keep a column during the summarize() if you just put the name equal to\n# itself. This code is deliberately long in order to show the common functions\n# that we use.\n\nband_albums <- tbl(con, \"albums\") |>\n                 inner_join(tbl(con, \"artists\"), by = \"ArtistId\") |>\n                 select(\"AlbumId\", \"ArtistId\", \"Title\", \"Name\") |>\n                 group_by(ArtistId) |>\n                 summarize(Name = Name, num_albums = n()) |>\n                 mutate(artist_name = Name) |>\n                 mutate(artist_id = ArtistId) |>\n                 filter(num_albums > 3) |>\n                 arrange(desc(num_albums)) |>\n                 select(artist_id, artist_name, num_albums)\nband_albums\n\n# Source:     SQL [?? x 3]\n# Database:   sqlite 3.38.5 [C:\\Users\\tejas\\OneDrive\\Documents\\RProjects\\primer_quarto\\chinook.db]\n# Ordered by: desc(num_albums)\n   artist_id artist_name     num_albums\n       <int> <chr>                <int>\n 1        90 Iron Maiden             21\n 2        22 Led Zeppelin            14\n 3        58 Deep Purple             11\n 4        50 Metallica               10\n 5       150 U2                      10\n 6       114 Ozzy Osbourne            6\n 7       118 Pearl Jam                5\n 8        21 Various Artists          4\n 9        82 Faith No More            4\n10        84 Foo Fighters             4\n# … with more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThese are the functions that we’re already familiar with using, and they work just as well on database tables as well.\nHowever, there are cases where you will have to use SQL code to accomplish certain tasks."
  },
  {
    "objectID": "03-data.html#webscraping",
    "href": "03-data.html#webscraping",
    "title": "3  Data",
    "section": "\n3.4 Webscraping",
    "text": "3.4 Webscraping\n\n\n\n\n\nIn the first part of this chapter, we learned how to read in data from plain text files that are usually “rectangular” in shape using the tidyverse read_* functions. Sadly, not all data comes in this simple format, but we can happily use many other tools to read in more messy/wild data formats. The formal name for gathering non-rectangular data from the web and transforming it into a more useful format for data analysis is web scraping.\nWe can do web scraping through r by using the rvest library, which is a library that allows us to look at HTML and CSS selectors, then pick them apart to get the data we want. Let’s look at what this means.\n\n3.4.1 HTML and CSS selectors\nBefore we jump into scraping, let’s learn a little bit about what the “source code” of a website looks like.\n\n3.4.1.1 Website structure\n\n\n\n\nA little bit of HTML editing can bring you a long way.\n\n\n\n\nWebsites are coded in a language called HTML, or HyperText Markup Language. This is the code that puts all of the information on the website, such as the numbers and text. These are the files created when we “knit” our .Rmd files and it’s what makes them put information on the screen. When we’re webscraping, we can look at the information on the website by looking at the HTML code. You can try this out by yourself! Just go to any website and right-click, then press “Inspect”. You’ll be able to see the HTML code behind the website and even edit it to your liking!\n\nknitr::include_graphics(\"03-data/images/html_inspect.gif\")\n\n\n\nYou can see the HTML code of a website and even edit it temporarily by inspecting the HTML code of the in\n\n\n\n\nHTML uses many nested “tags” to organize the structure. For example, an HTML file like this:\n<!-- This is using the HTML language. You don't actually put this in a chunk if \nyou want to put this in an Rmarkdown file, you can just type it straight in.-->\n<html>\n  <p>\n    Hello World\n  </p>\n</html>\nlooks like this in the result:\n\n\n\n\n\n\n\nHello World\n\n\n\nEach tag defines an element, or a part of a website. For example, we used the <p> tag to define a paragraph element in HTML. We then close the paragraph by using the </p> tag, ending the paragraph. You can find a full list of HTML tags and elements here.\nNow, we’re doing data science. And let’s say you find some really cool website off of the internet that has a lot of really useful data, but there’s no download file (and it’s legal to scrape from that website). That means that you have to pull the information from the HTML code.\nWe can do this by using the rvest package like I talked about earlier. This is essentially a package that allows you to scrape information from HTML code, allowing you to scrape the information off of the complete website.\n\nlibrary(rvest)\n\nLet’s try scraping the Hello World from the earlier HTML code.\nFirst off, we need to use the minimal_html() function so that we can get an R object that we can then work on. This is a temporary use because we’re trying to Think about it like changing the information from HTML code into something that R can understand.\n\nraw_html_1 <- \"<html>\n                 <p>\n                   Hello World\n                 </p>\n               </html>\"\nraw_html_1 |>\n  minimal_html()\n\n{html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<p>\\n                   Hello World\\n                 </p>\\n      ...\n\n\nThen we need to filter by the elements that the HTML file contains. In this case, we’re looking for the paragraph tag, or <p>. That means that we can just use html_element() to get the first paragraph element.\n\n# Only use the letter part of the tag, not the <, >, or /\n\nraw_html_1 |>\n  minimal_html() |>\n  html_element(\"p\")\n\n{html_node}\n<p>\n\n\nThis returns an HTML node, or the specific element that we chose. If we had more paragraphs in our HTML code, this would return more HTML nodes. Now, the interesting thing about these nodes is that they contain information that we can access using html_text2(). This lets us parse through the code without any problems.\n\n# html_text2() is different from the normal html_text() because it returns an\n# actual string. This is normally what we want.\n\nraw_html_1 |>\n  minimal_html() |>\n  html_element(\"p\") |>\n  html_text2()\n\n[1] \"Hello World\"\n\n\nBut websites, especially ones with useful data, don’t just exist to state “Hello World”. They’re much more complex than that. By layering different elements, you can create a website that contains a lot more information.\nFor example, doing this in HTML:\n<html>\n  <table>\n    <tr>\n      <td>\n        This is some important info.\n      </td>\n      <td>\n        This is some unimportant info.\n      </td>\n    </tr>\n    <tr>\n      <td>\n        This is really important info.\n      </td>\n      <td>\n        This is some other unimportant information.\n      </td>\n    </tr>\n  </table>\n  <p>\n    This is so useless, you shouldn't even be reading it.\n  </p>\n</html>\nCreates a table and a paragraph that looks like this:\n\n\n\n\n\n\nThis is some important info.\n\n\nThis is some unimportant info.\n\n\n\n\nThis is really important info.\n\n\nThis is some other unimportant information.\n\n\n\n\nThis is so useless, you shouldn’t even be reading it.\n\n\n\nNow, let’s say we want to get all of the information in the table but we don’t want to get the useless description at the end.\nWe can do this by only looking at the table (the <table> tag) and then getting the cells (the <td> tag). We’ll also save this to a variable for later.\n\nraw_html_2 <- \"<html>\n                 <table>\n                   <tr>\n                     <td>\n                       This is some important info.\n                     </td>\n                     <td>\n                       This is some unimportant info.\n                     </td>\n                   </tr>\n                   <tr>\n                     <td>\n                       This is really important info.\n                     </td>\n                     <td>\n                       This is some other unimportant information.\n                     </td>\n                   </tr>\n                 </table>\n                 <p>\n                   This is so useless, you shouldn't even be reading it.\n                 </p>\n               </html>\"\nraw_html_2\n\n[1] \"<html>\\n                 <table>\\n                   <tr>\\n                     <td>\\n                       This is some important info.\\n                     </td>\\n                     <td>\\n                       This is some unimportant info.\\n                     </td>\\n                   </tr>\\n                   <tr>\\n                     <td>\\n                       This is really important info.\\n                     </td>\\n                     <td>\\n                       This is some other unimportant information.\\n                     </td>\\n                   </tr>\\n                 </table>\\n                 <p>\\n                   This is so useless, you shouldn't even be reading it.\\n                 </p>\\n               </html>\"\n\n# We use html_elements() to get all of the elements in the HTML file.\n\ntd_tags <- raw_html_2 |>\n             minimal_html() |>\n             html_element(\"table\") |>\n             html_elements(\"td\")\ntd_tags\n\n{xml_nodeset (4)}\n[1] <td>\\n                       This is some important info.\\n               ...\n[2] <td>\\n                       This is some unimportant info.\\n             ...\n[3] <td>\\n                       This is really important info.\\n             ...\n[4] <td>\\n                       This is some other unimportant information.\\ ...\n\n\nNotice that this actually outputs a list of the elements and their text. That means that we can use the this output like a list in order to get the information that we want. Just use the [[]] syntax.\n\ntd_tags[[2]] |>\n  html_text2()\n\n[1] \"This is some unimportant info.\"\n\n\nThis method is what you’d use if you just wanted to get all of the information inside of a specific table without filtering any data out. This is because pulling directly from the HTML makes a lot more sense in this scenario than fiddling around with other junk. Just pull the data and go, no fancy things.\nHowever, what if we want only the important information in that table? That’s what CSS selectors are for.\n\n3.4.1.2 CSS selectors\nCSS, or Cascading Style Sheets, is a coding language that defines the style of webpages. You may have noticed earlier that the HTML code we output earlier was just black and white. But websites aren’t black and white, they have colors and other cool things. They do this by using CSS to add special rules to specific elements.\nNow, there are a number of ways to do this. The first one is by using a class to tell the webpage that you want all of these elements to have the same style. CSS classes are a way for multiple elements to have the same style, rather than being limited to a unique ID. We can see an example of this below, where we set the elements with the class good-info to be green and the class amazing-info to have a pale red background.\n\n/*This uses the `css` language instead of the R language. You can use this\n  code in an Rmarkdown file by substituting the \"r\" for \"css\" in the top \n  brackets*/\n  \n.good-info {\n  color: green\n}\n  \n.amazing-info {\n  background-color: lightpink\n}\n\n\nWe can then use that class in the HTML code.\n<html>\n  <table>\n    <tr>\n      <td class = 'good-info'>\n        This is some important info.\n      </td>\n      <td>\n        This is some unimportant info.\n      </td>\n    </tr>\n    <tr>\n      <td class = 'good-info amazing-info'>\n        This is really important info.\n      </td>\n      <td>\n        This is some other unimportant information.\n      </td>\n    </tr>\n  </table>\n  <p>\n    This is so useless, you shouldn't even be reading it.\n  </p>\n</html>\n\n\n\n\n\n\nThis is some important info.\n\n\nThis is some unimportant info.\n\n\n\n\nThis is really important info.\n\n\nThis is some other unimportant information.\n\n\n\n\nThis is so useless, you shouldn’t even be reading it.\n\n\n\n\nNow, there’s a really clever trick that we can use here. Let’s say that we only want the table cells that are important, but we don’t know exactly where they are (we only know their class). Well in most cases, the data you want will be a different color, like what we see in the data above. Think about it like making important information have an italic font to make it stand out. CSS makes all of the information italic by assigning it a specific class, so we can just look for everything that has that class and pull it out so that we only get the important information. We can do this by using html_elements(). Just plug in the class of the element that you want with a . in front to tell R that it’s an class.\n\nraw_html_3 <- \"<html>\n                 <table>\n                   <tr>\n                     <td class = 'good-info'>\n                       This is some important info.\n                     </td>\n                     <td>\n                       This is some unimportant info.\n                     </td>\n                   </tr>\n                   <tr>\n                     <td class = 'good-info amazing-info'>\n                       This is really important info.\n                     </td>\n                     <td>\n                       This is some other unimportant information.\n                     </td>\n                   </tr>\n                 </table>\n                 <p>\n                   This is so useless, you shouldn't even be reading it.\n                 </p>\n               </html>\"\n\nraw_html_3 |>\n  minimal_html() |>\n  html_elements(\".good-info\") |>\n  html_text2()\n\n[1] \"This is some important info.\"   \"This is really important info.\"\n\n\nYou can sort by multiple classes by just chaining them together.\n\nraw_html_3 |>\n  minimal_html() |>\n  html_elements(\".good-info.amazing-info\") |>\n  html_text2()\n\n[1] \"This is really important info.\"\n\n\nWebpages will also use something called an ID to change the color of a specific element. Do the same thing but this time with a “#” at the beginning to signify that it’s a symbol.\n\n.red-id {\n  color: red\n}\n\n\n<html>\n<p id = \"red-id\"> This is red! </p>\n<p> This is not red. </p>\n</html>\n\n\n\nThis is red!\n\n\nThis is not red.\n\n\n\n\nSo far, we’ve covered 3 ways to find the parts of a website through it’s HTML code: the element (<p>), the ID (#red-id), and the class (.red-class). However, we can also mix-and-match these tags. Take the following HTML element:\n<html>\n<a href=\"https://ppbds.github.io/primer/index.html\" id=\"abcd1234\" class=\"hello hi\">\nlink to primer\n</a>\n</html>\n\n\n link to primer \n\n\n\nWe can try accessing it through the normal methods, like so:\n\nraw_html_5 <- '<a href = \"https://ppbds.github.io/primer/index.htm\" id=\"abcd1234\" class=\"hello hi\">link to primer</a>'\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"#abcd1234\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\".hello\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\n\nOr we can mix and match them to make an even more narrow search by chaining them together.\n\n# Keep in mind that IDs (the # part) are usually unique to that element only, so\n# there's not really a point in filtering it even more when it's already unique.\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a#abcd1234\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello#abcd1234\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\".hello#abcd1234\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello.hi#abcd1234\") |>\n  html_text2()\n\n[1] \"link to primer\"\n\n\nWe can also get the link that the node refers to rather than the text. In this case, the link is stored in the href attribute, so we use html_attr() to access it.\n\nraw_html_5 |>\n  minimal_html() |>\n  html_element(\"a.hello.hi#abcd1234\") |>\n  html_attr(\"href\")\n\n[1] \"https://ppbds.github.io/primer/index.htm\"\n\n\nThis accesses the href attribute of the element without accessing the link, allowing you to find the links to outside websites.\nThese chains are known as CSS Selectors and they’re an important part of data science because they allow us to find data and information on a website without having to download every file or copying everything down. Instead, we can just use the website’s inbuilt code to get the information.\n\n3.4.2 Application\nNow let’s look at this in a real-world context. Say we are interested in knowing the average rental price (per square footage) of the most recently available one-bedroom apartments in Vancouver according to this. When we visit the Vancouver Craigslist website and search for one-bedroom apartments, this is what we are shown:\n\n\n\n\n\nFrom that page, it’s pretty easy for us to find the apartment price and square footage (Craigslist would be utterly incomprehensible otherwise). But the computer can’t deal with that because human eyes and computer eyes are very different. It can’t understand anything on the screen. So instead of looking at the screen, we can just parse through the HTML and find the correct information. Here’s part of the HTML code for Vancouver’s Craigslist:\n        <span class=\"result-meta\">\n                <span class=\"result-price\">$800</span>\n\n                <span class=\"housing\">\n                    1br -\n                </span>\n\n                <span class=\"result-hood\"> (13768 108th Avenue)</span>\n\n                <span class=\"result-tags\">\n                    <span class=\"maptag\" data-pid=\"6786042973\">map</span>\n                </span>\n\n                <span class=\"banish icon icon-trash\" role=\"button\">\n                    <span class=\"screen-reader-text\">hide this posting</span>\n                </span>\n\n            <span class=\"unbanish icon icon-trash red\" role=\"button\" aria-hidden=\"true\"></span>\n            <a href=\"#\" class=\"restore-link\">\n                <span class=\"restore-narrow-text\">restore</span>\n                <span class=\"restore-wide-text\">restore this posting</span>\n            </a>\n\n        </span>\n    </p>\n</li>\n         <li class=\"result-row\" data-pid=\"6788463837\">\n\n        <a href=\"https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html\" class=\"result-image gallery\" data-ids=\"1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV\">\n                <span class=\"result-price\">$2285</span>\n        </a>\n\n    <p class=\"result-info\">\n        <span class=\"icon icon-star\" role=\"button\">\n            <span class=\"screen-reader-text\">favorite this post</span>\n        </span>\n\n            <time class=\"result-date\" datetime=\"2019-01-06 12:06\" title=\"Sun 06 Jan 12:06:01 PM\">Jan  6</time>\n\n\n        <a href=\"https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html\" data-id=\"6788463837\" class=\"result-title hdrlnk\">Luxury 1 Bedroom CentreView with View - Lonsdale</a>\n\n\nThis genuinely sucks to read. There’s links, random numbers, and elements that have 4 classes on them at the same time, all while being stupidly long to the point where your eyes fall out. While we can pick out the CSS selectors that we need, (“result-price” and “housing”), this is a hassle and a pain and there’s a better solution.\nWe will be using the SelectorGadget tool in order to find CSS selectors. It’s an open source tool that simplifies generating and finding CSS selectors. We recommend that you use the Chrome web browser to use this tool, and install the selector gadget tool from the Chrome Web Store. Here is a short video on how to install and use the SelectorGadget tool to get a CSS selector for use in web scraping:\n\n\nFrom installing and using the selectorgadget as shown in the video above, we get the two CSS selectors .housing and .result-price that we can use to scrape information about the square footage and the rental price, respectively. The selector gadget returns them to us as a comma separated list (here .housing , .result-price), which is exactly the format we need to provide to R.\nHowever, when we’re scraping from actual websites we need to take more than just the size of the code into consideration. We also need to take the legal aspects into account,\n\n3.4.2.1 Are you allowed to scrape that website?\nBEFORE scraping data from the web, you should always check whether or not you are ALLOWED to scrape it! There are two documents that are important for this: the robots.txt file (found by adding /robots.txt to the end of a URL like so) and reading the website’s Terms of Service document. The website’s Terms of Service document is far and away the more important of the two because it’s actually legally binding, so you should look there first. What happens when we look at Craigslist’s Terms of Service document? Well we read this:\n“You agree not to copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g., by hand).”\nsource: https://www.craigslist.org/about/terms.of.use\n\nWant to learn more about the legalities of web scraping and crawling? Read this interesting blog post titled “Web Scraping and Crawling Are Perfectly Legal, Right?” by Benoit Bernard (this is optional, not required reading).\n\nSo what to do now? Well, we can’t scrape Craigslist, so we should find something else that allows you to scrape. Let’s use a database from Open Secrets about foreign-connected PACs within our country.\n\n\n3.4.2.2 Scraping from actual websites\nNow, earlier we only scraped from pre-written HTML code, but you can do this from websites as well. We can do this just by plugging in the URL to the website and using read_html() instead of minimal_html().\n\nweb_url <- \"https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020\"\n\nweb_url |>\n  read_html()\n\n{html_document}\n<html class=\"no-js\" lang=\"en\" dir=\"ltr\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n\\n    <!-- Google Adsense Script -->\\n<!--    <script async src=\" ...\n\n\nIn this case, we can use Selector Gadget to find the correct CSS selector “table.DataTable-Partial”. We then just proceed as normal, but we need to use html_table() because we’re trying to bring in a table and use it as a tibble.\n\nweb_url |>\n  read_html() |>\n  html_element(\"table.DataTable-Partial\") |>\n  html_table()\n\n# A tibble: 225 × 5\n   `PAC Name (Affiliate)`                Country of Origin/…¹ Total Dems  Repubs\n   <chr>                                 <chr>                <chr> <chr> <chr> \n 1 7-Eleven                              Japan/Seven & I Hol… $20,… $1,0… $19,0…\n 2 ABB Group (ABB Group)                 Switzerland/Asea Br… $16,… $6,8… $10,1…\n 3 Accenture (Accenture)                 Ireland/Accenture p… $83,… $50,… $33,0…\n 4 Air Liquide America                   France/L'Air Liquid… $37,… $15,… $22,0…\n 5 Airbus Group                          Netherlands/Airbus … $182… $79,… $103,…\n 6 Alkermes Inc                          Ireland/Alkermes Plc $94,… $30,… $64,0…\n 7 Allianz of America (Allianz)          Germany/Allianz AG … $71,… $36,… $35,1…\n 8 AMG Vanadium                          Netherlands/AMG Adv… $2,0… $0    $2,000\n 9 Anheuser-Busch (Anheuser-Busch InBev) Belgium/Anheuser-Bu… $336… $174… $162,…\n10 AON Corp (AON plc)                    UK/AON PLC           $80,… $44,… $36,5…\n# … with 215 more rows, and abbreviated variable name\n#   ¹​`Country of Origin/Parent Company`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThis allows us to scrape tibbles from websites. We can then use those tibbles to create new graphs.\nLet’s try this by using a Wikipedia page about gun violence in the United States.\nFirst, we need to save the URL and read the HTML code from it into R.\n\nwiki_url <- \"https://en.wikipedia.org/w/index.php?title=%22,%22Gun_violence_in_the_United_States_by_state%22,%22&direction=prev&oldid=810166167\"\n\nwiki_url |>\n  read_html()\n\n{html_document}\n<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject  ...\n\n\nWe can then use SelectorGadget to find the correct CSS selector. If you can’t easily click on the table, you can also find a selector by reading the HTML code itself. You can do this by highlighting the table, right clicking, and pressing “Inspect”. While you’ll have to dig through the HTML in order to find the correct selector, this is a perfectly viable way to find selectors.\nWe can then save the table provided into a variable known as raw_data_wiki.\n\nraw_data_wiki <- wiki_url |>\n                   read_html() |>\n                   html_element(\"table.wikitable.sortable\") |>\n                   html_table()\nraw_data_wiki\n\n# A tibble: 51 × 4\n   State                Population (total inhabitants) (2015) …¹ Murde…² Murde…³\n   <chr>                <chr>                                    <chr>     <dbl>\n 1 Alabama              4,853,875                                348         7.2\n 2 Alaska               737,709                                  59          8  \n 3 Arizona              6,817,565                                309         4.5\n 4 Arkansas             2,977,853                                181         6.1\n 5 California           38,993,940                               1,861       4.8\n 6 Colorado             5,448,819                                176         3.2\n 7 Connecticut          3,584,730                                117         3.3\n 8 Delaware             944,076                                  63          6.7\n 9 District of Columbia 670,377                                  162        24.2\n10 Florida              20,244,914                               1,041       5.1\n# … with 41 more rows, and abbreviated variable names\n#   ¹​`Population (total inhabitants) (2015) [1]`,\n#   ²​`Murders and Nonnegligent\\nManslaughter(total deaths) (2015) [2]`,\n#   ³​`Murder and Nonnegligent\\nManslaughter Rate(per 100,000 inhabitants) (2015)`\n# ℹ Use `print(n = ...)` to see more rows\n\n\nPast this point, you can work with the data just like you would normal data. Here’s an example graph of how states that have a denser population experience more consistent gun deaths than states with a less dense population.\n\nclean_data <- raw_data_wiki |>\n                rename(\"population\" = \"Population (total inhabitants) (2015) [1]\",\n                       \"death_rate\" = \"Murder and Nonnegligent\\nManslaughter Rate(per 100,000 inhabitants) (2015)\",\n                       \"total_deaths\" = \"Murders and Nonnegligent\\nManslaughter(total deaths) (2015) [2]\") |>\n                        select(population, death_rate) |>\n                        mutate(population = parse_number(population)) |>\n                        mutate(pop_tile = ntile(population, 20)) |>\n                        group_by(pop_tile) |>\n                        summarize(num_states = n(), sum_rate = sum(death_rate)) |>\n                        mutate(avg_rate = sum_rate/num_states) |>\n                        mutate(percent_rate = avg_rate / sum(avg_rate))\n\nggplot(clean_data, aes(x = pop_tile, y = percent_rate)) + \n  geom_col() +\n  geom_smooth(method = \"loess\", formula = \"y ~ x\", se = FALSE) +\n  scale_x_continuous(breaks = 1:20) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  theme_classic() +\n  labs(title = \"Distribution of US Death Rate by Guns in Quantile of Population in 2015\",\n       subtitle = \"Death rate of less populated states fluctuate more than states with a denser population\",\n       x = \"Quantile by Population (least to most)\",\n       y = \"Percent of Average Death Rate\",\n       caption = \"Wikipedia: Gun Violence in the United States (2017)\")\n\n\n\n\nWebscraping is a powerful tool to gain data directly from the internet, but make sure that you’re following the proper protocols so that you don’t break the law and actually get the information that you need."
  },
  {
    "objectID": "03-data.html#working-with-apis",
    "href": "03-data.html#working-with-apis",
    "title": "3  Data",
    "section": "\n3.5 Working with APIs",
    "text": "3.5 Working with APIs\n\n\n\n\n\n\n\n\n\n“API” stands for Application Program Interface. They allow us to access open data from government agencies, companies, and other organizations. API provides the rules for software applications to interact with one another. Open data APIs provide the rules you need to know to write R code to request and pull data from the organization’s web server into R. Usually, some of the computational burden of querying and subsetting the data is taken on by the source’s server, to create the subset of requested data to pass to your computer. In practice, this means you can often pull the subset of data you want from a very large available dataset without having to download the full dataset and load it locally into your R session.\nAs an overview, the basic steps for accessing and using data from a web API when working in R are:\n\nFigure out the API rules for HTTP requests\nWrite R code to create a request in the proper format\nSend the request using GET or POST HTTP methods\nOnce you get back data from the request, parse it into an easier-to-use format if necessary\n\nTo get the data from an API, you should first read the organization’s API documentation. An organization will post details on what data is available through their API(s), as well as how to set up HTTP requests to get that data. To request the data through the API, you will typically need to send the organization’s web server an HTTP request using a GET or POST method. The API documentation details will typically show an example GET or POST request for the API, including the base URL to use and the possible query parameters that can be used to customize the dataset request.\nHere is an example:\nThe National Aeronautics and Space Administration (NASA) has an API for pulling the Astronomy Picture of the Day. In their API documentation, they specify that the base URL for the API request should be https://api.nasa.gov/planetary/apod and that you can include parameters to specify the date of the daily picture you want, whether to pull a high-resolution version of the picture, and a NOAA API key you have requested from NOAA.\nMany organizations will require you to get an API key and use this key in each of your API requests. This key allows the organization to control API access, including enforcing rate limits per user. API rate limits restrict how often you can request data (such as an hourly limit of 1,000 requests per user for NASA APIs).\nAPI keys should be kept private, so if you are writing code that includes an API key, be very careful not to include the actual key in any code that is public (even any code in public GitHub repositories). To ensure privacy, save the value of your key in a file named .Renviron in your home directory. This file should be a plain text file and must end in a blank line. Once you’ve saved your API key to a global variable in that file (e.g., with a line added to the .Renviron file like NOAA_API_KEY = “abdafjsiopnab038”), you can assign the key value to an R object in an R session using the Sys.getenv function (e.g., noaa_api_key <- Sys.getenv(“NOAA_API_KEY”)), and then use the object noaa_api_key anywhere you would otherwise have used the character string with your API key.\nTo find more R packages for accessing and exploring open data, check out the Open Data CRAN task view. You can also browse through the ROpenSci packages, all of which have GitHub repositories where you can further explore how each package works! ROpenSci is an organization with the mission to create open software tools for science. If you create your own package to access data relevant to scientific research through an API, consider submitting it for peer-review through ROpenSci.\nThe riem package, developed by Maelle Salmon and an ROpenSci package, is an excellent and straightforward example of how you can use R to pull open data through a web API. This package allows you to pull weather data from airports around the world directly from the Iowa Environmental Mesonet. To show you how to pull data into R through an API, in this section we will walk you through code in the riem package or code based closely on code in the package.\nTo get a certain set of weather data from the Iowa Environmental Mesonet, you can send an HTTP request specifying a base URL, https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/, as well as some parameters describing the subset of dataset you want (e.g., date ranges, weather variables, output format). Once you know the rules for the names and possible values of these parameters (more on that below), you can submit an HTTP GET request using the functionGET() from the httr package.\nWhen you are making an HTTP request using the GET() or POST() functions from the httr package, you can include the key-value pairs for any query parameters as a list object in the query argument of the function. For example, suppose you want to get wind speed in miles per hour (data = “sped”) for Denver, CO, (station = “DEN”) for the month of June 2016 (year1 = “2016”, month1 = “6”, etc.) in Denver’s local time zone (tz = “America/Denver”) and in a comma-separated file (format = “comma”). To get this weather dataset, you can run:\n\nlibrary(httr)\n\n\nlibrary(httr)\nmeso_url <- \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py/\"\ndenver <- GET(url = meso_url,\n                    query = list(station = \"DEN\",\n                                 data = \"sped\",\n                                 year1 = \"2016\",\n                                 month1 = \"6\",\n                                 day1 = \"1\",\n                                 year2 = \"2016\",\n                                 month2 = \"6\",\n                                 day2 = \"30\",\n                                 tz = \"America/Denver\",\n                                 format = \"comma\")) |>\n  content() |> \n  read_csv(skip = 5, na = \"M\")\n\nRows: 9108 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): station\ndbl  (1): sped\ndttm (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# There are 9,106 rows of data to look at! Let's just look at subset for our\n# purposes.\n\ndenver |> \n  slice(1:3)\n\n# A tibble: 3 × 3\n  station valid                sped\n  <chr>   <dttm>              <dbl>\n1 DEN     2016-06-01 00:00:00   9.2\n2 DEN     2016-06-01 00:05:00   9.2\n3 DEN     2016-06-01 00:10:00   6.9\n\n\nThe content() call extracts the content from the response to the HTTP request sent by the GET() function. The Iowa Environmental Mesonet API offers the option to return the requested data in a comma-separated file (format = “comma” in the GET request), so here content and read_csv() are used to extract and read in that csv file. Usually, data will be returned in a JSON format instead.\nThe only tricky part of this process is figuring out the available parameter names (e.g., station) and possible values for each (e.g., “DEN” for Denver). Currently, the details you can send in an HTTP request through Iowa Environmental Mesonet’s API include:\n\nA four-character weather station identifier (station)\nThe weather variables (e.g., temperature, wind speed) to include (data)\nStarting and ending dates describing the range for which you’d like to pull data (year1, month1, day1, year2, month2, day2)\nThe time zone to use for date-times for the weather observations (tz)\nDifferent formatting options (e.g., delimiter to use in the resulting data file [format], whether to include longitude and latitude)\n\nTypically, these parameter names and possible values are explained in the API documentation. In some cases, however, the documentation will be limited. In that case, you may be able to figure out possible values, especially if the API specifies a GET rather than POST method, by playing around with the website’s point-and-click interface and then looking at the url for the resulting data pages. For example, if you look at the Iowa Environmental Mesonet’s page for accessing this data, you’ll notice that the point-and-click web interface allows you the options in the list above, and if you click through to access a dataset using this interface, the web address of the data page includes these parameter names and values.\nThe riem package implements all these ideas in three very clean and straightforward functions. You can explore the code behind this package and see how these ideas can be incorporated into a small R package, in the /R directory of the package’s GitHub page.\nR packages already exist for many open data APIs. If an R package already exists for an API, you can use functions from that package directly, rather than writing your own code using the API protocols and httr functions. Other examples of existing R packages to interact with open data APIs include:\n\ntwitteR: Twitter\nrnoaa: National Oceanic and Atmospheric Administration\nQuandl: Quandl (financial data)\nRGoogleAnalytics: Google Analytics\ncensusr, acs: United States Census\nWDI, wbstats: World Bank\nGuardianR, rdian: The Guardian Media Group\nblsAPI: Bureau of Labor Statistics\nrtimes: New York Times\ndataRetrieval, waterData: United States Geological Survey If an R package doesn’t exist for an open API and you’d like to write your own package, find out more about writing API packages with this vignette for the httr package. This document includes advice on error handling within R code that accesses data through an open API.\n\nInformation for this section on API’s was taken from Mastering Software Development in R textbook, authored by Roger D. Peng, Sean Kross, and Brooke Anderson."
  },
  {
    "objectID": "03-data.html#distill",
    "href": "03-data.html#distill",
    "title": "3  Data",
    "section": "\n3.6 Distill",
    "text": "3.6 Distill\n\n\n\n\n\n\nNow, you may have noticed that whenever we knit our .Rmd files, we produce an HTML file. And just a little bit earlier in the webscraping section, we talked about how websites using HTML to make up the basic structure. So by that logic, can’t we make our own websites by using the HTML files that are created when we knit the .Rmd?\nThat’s exactly what the distill package does. Essentially, distill allows you to make websites using the HTML files that are output when you knit them. It also organizes the pages nicely to make the website navigable and look professional.\nHere’s an example of a distill website. As you can see, this is a clear, easy website with a few pages and easily distinguishable information.\nWe cover how to use the distill package in the associated primer tutorials, where we’ll walk you through creating a distill page and developing your own citations.\n\n\n3.6.1 Common Errors and Functions\nAs you forge onwards into the world of distill, you will likely encounter a variety of errors that’ll make you want to throw your computer away. As such, we’ll go through a few of the common functions and errors encountered while making a distill page.\n\n3.6.1.1 create_website()\n\nThe first step to creating a website is by using the create_website() function. This creates the entire distill website and should be the first function that you run whenever you want to make one. Just use the following syntax:\n\nlibrary(distill)\ncreate_website(dir = \".\", title = \"title-of-your-website\", gh_pages = TRUE)\n\nThis essentially creates a distill website in your current R project, titles it “title-of-your-website” and formats it so that we can publish it on Github Pages (Github’s website publishing service). After you run this, you should see an index.Rmd and an about.Rmd file in your project, as well as some other miscellaneous files. These are the “Home” and “About” pages that you can use on your website.\n\n3.6.1.2 create_article()\n\nBut what if I want more pages than that? Only being restricted to a Home and About page is pretty bad, especially when you want to have a bibliography. This is when we use create_article() to create a new page within our website. Just run the function with the title of the file and it’ll automatically create the .Rmd file for you to code in.\n\ncreate_article(\"sources.Rmd\")\n\nThen, you need to put it into your website. Just go to the _site.yml file and add the page in using text: \"Sources\" and \"href: sources.html\". This is what the _site.yml file should look like after you add the page.\nname: \".\"\ntitle: \"title-of-your-website\"\ndescription: |\n  Welcome to the website. I hope you enjoy it!\noutput_dir: \"docs\"\nnavbar:\n  right:\n    # These two pages were automatically put in.\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    # This page was added by us.\n    - text: \"Sources\"\n      href: sources.html\noutput: distill::distill_article\nThe _site.yml is the file that keeps everything in line and organizes your distill website. However, it’s very specific as to the syntax, so you should take care to only add in specific things. Additionally, keep in mind that you should always use the create_article() function whenever you need to create a new page or a .Rmd file. Do not add in a .Rmd file manually. These both lead to errors that can break your project.\n\n3.6.1.3 Formatting the article\nYou can format the article just like you did before because it’s still an RMarkdown file. However, you can also put more advanced features like block quotes here. Just make sure not to edit the headers at the top of the file without knowing exactly what you’re doing in order to prevent any errors.\n\n3.6.1.4 Error: $ operator is invalid for atomic vectors\n\nThis is the error caused by creating a .Rmd file without using it in your _site.yml or using create_article() to make it. Essentially, distill doesn’t know how to deal with the extra .Rmd file and crashes because of it. This can normally be solved just by deleting the extra .Rmd, but it may be necessary to just nuke the project from orbit and restart.\nAs such, make sure that you never create an extra .Rmd, as it’s just going to be more work on your end when you start encountering this error. If you want to write R code for a graph so that you can put it into your website, just use a normal R script file by going to File -> New File -> R Script in RStudio.\n\n3.6.1.5 YAML Errors\nThese are errors that are created by problems in your _site.yml file or in the headers (the — part) at the top of your file. These parts essentially tell distill what the title and the description of your page is, as well as some other information. However, missing parts can cause pretty significant problems because distill just doesn’t have the information that it needs. For example, deleting the site: part of the header will not allow the website to work or not including description: | will stop the description from showing up.\nHere’s what the YAML header at the top of your .Rmd file should look like. You can make some edits to it or add more information, but make sure to keep this basic structure in order to prevent any errors.\n---\ntitle: \"Home\"\ndescription: |\n  Description of the page\nsite: distill::distill_website\n---\nKeep in mind that you can modify this when you need to make a new page. However, whenever you have an error beginning with Error in yaml::yaml.load(..., eval.expr = TRUE) :, that normally means that you have a YAML error and you need to fix this part of the code."
  },
  {
    "objectID": "03-data.html#summary",
    "href": "03-data.html#summary",
    "title": "3  Data",
    "section": "\n3.7 Summary",
    "text": "3.7 Summary\n\nPulling data into and out of R is a key factor in the data science process.\nUse files like CSV, Excel, RDS, JSON, and SQL files/databases to organize and share your data.\nUse SelectorGadget to pull data off of websites, but make sure it’s legal to.\nUse an API when to get data from government agencies or companies.\nPublish your results on a website by using Distill.\nThis chapter is all about pulling data into R. It’s not about graphing that data, cleaning it, wrangling it, or anything like that, it’s just about pulling that data in without losing your mind.\nIn this chapter, we looked at a few common file formats and used the readr package to read them and pull data from them so that we can use that data in our plots. This allowed us to download data from websites like kaggle.com and use it in our R session.\nWe also looked at databases and how we can pull in a few select pieces of data from them so that we aren’t overloading our computers with thousands or millions of data rows. We went over how we can write SQL queries so that we can acheive some special effects without causing errors or detonating our computers.\nAnd finally, we looked at how we can pull data from websites both through webscraping and through APIs, letting us pull data from the internet quickly and easily. This lets us find data for our projects and load it into our R session without having to create it ourselves or download a file from the internet.\nIn the end, this chapter is about getting data from other people and using it inside your own projects.\n\n\n\n\nMake sure to use up to date data."
  },
  {
    "objectID": "01-visualization.html",
    "href": "01-visualization.html",
    "title": "\n1  Visualization\n",
    "section": "",
    "text": "Everyone loves visualizations.\nOnce you have read this chapter, and completed the associated tutorials, you will be able to create graphics like this one with your data. Join us on the journey."
  },
  {
    "objectID": "01-visualization.html#looking-at-data",
    "href": "01-visualization.html#looking-at-data",
    "title": "\n1  Visualization\n",
    "section": "\n1.1 Looking at data",
    "text": "1.1 Looking at data\nThis chapter focuses on ggplot2, one of the core packages in the tidyverse. The tidyverse is a package that contains 8 individual packages, most importantly ggplot2, but also other packages that help us view data within R. To access the datasets, help pages, and functions that we will use in this chapter, load the tidyverse:\n\nlibrary(tidyverse)\n\nThat one line of code loads all the packages associated with the tidyverse, packages which you will use in almost every data analysis. The first time you load the tidyverse, R will report which functions from the tidyverse conflict with functions in base R or with other packages you may have loaded. (We hide these and other messages in this book because they are ugly.)\nYou might get this error message:\nError in library(tidyverse) : there is no package called ‘tidyverse’\nIf that happens, you need to install the package:\n\ninstall.packages(\"tidyverse\")\n\nThen, run library(tidyverse) again.\n\n1.1.1 Examining trains\n\nMost data comes to us in “spreadsheet”-type format. These datasets are called data frames or tibbles in R. Let’s explore the trains tibble from the primer.data package. This data comes from @enos2014, which investigated attitudes toward immigration among Boston commuters.\n\nlibrary(primer.data)\ntrains\n\n# A tibble: 115 × 14\n   treat…¹ att_s…² att_end gender race  liberal party   age income line  station\n   <fct>     <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n 1 Treated      11      11 Female White FALSE   Demo…    31 135000 Fram… Grafton\n 2 Treated       9      10 Female White FALSE   Repu…    34 105000 Fram… Southb…\n 3 Treated       3       5 Male   White TRUE    Demo…    63 135000 Fram… Grafton\n 4 Treated      11      11 Male   White FALSE   Demo…    45 300000 Fram… Grafton\n 5 Control       8       5 Male   White TRUE    Demo…    55 135000 Fram… Grafton\n 6 Treated      13      13 Female White FALSE   Demo…    37  87500 Fram… Grafton\n 7 Control      13      13 Female White FALSE   Repu…    53  87500 Fram… Grafton\n 8 Treated      10      11 Male   White FALSE   Demo…    36 135000 Fram… Grafton\n 9 Control      12      12 Female White FALSE   Demo…    54 105000 Fram… Grafton\n10 Treated       9      10 Male   White FALSE   Repu…    42 135000 Fram… Grafton\n# … with 105 more rows, 3 more variables: hisp_perc <dbl>,\n#   ideology_start <int>, ideology_end <int>, and abbreviated variable names\n#   ¹​treatment, ²​att_start\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nLet’s unpack this output:\n\nA tibble is a specific kind of data frame. This particular data frame has 115 rows corresponding to different units, meaning people in this case.\nThe tibble also has 14 columns corresponding to variables which describe each unit or observation.\nWe see, by default, the top 10 rows and some of the columns. You can see more (or fewer) rows and columns by using the print() command:\n\n\nprint(trains, n = 15, width = 100)\n\n# A tibble: 115 × 14\n   treatment att_start att_end gender race  liberal party        age income\n   <fct>         <dbl>   <dbl> <chr>  <chr> <lgl>   <chr>      <int>  <dbl>\n 1 Treated          11      11 Female White FALSE   Democrat      31 135000\n 2 Treated           9      10 Female White FALSE   Republican    34 105000\n 3 Treated           3       5 Male   White TRUE    Democrat      63 135000\n 4 Treated          11      11 Male   White FALSE   Democrat      45 300000\n 5 Control           8       5 Male   White TRUE    Democrat      55 135000\n 6 Treated          13      13 Female White FALSE   Democrat      37  87500\n 7 Control          13      13 Female White FALSE   Republican    53  87500\n 8 Treated          10      11 Male   White FALSE   Democrat      36 135000\n 9 Control          12      12 Female White FALSE   Democrat      54 105000\n10 Treated           9      10 Male   White FALSE   Republican    42 135000\n11 Control          10       9 Female White FALSE   Democrat      33 105000\n12 Treated          11       9 Male   White FALSE   Democrat      50 250000\n13 Treated          13      13 Male   White FALSE   Republican    24 105000\n14 Control           6       7 Male   White TRUE    Democrat      40  62500\n15 Control           8       8 Male   White TRUE    Democrat      53 300000\n   line       station      hisp_perc ideology_start ideology_end\n   <chr>      <chr>            <dbl>          <int>        <int>\n 1 Framingham Grafton         0.0264              3            3\n 2 Framingham Southborough    0.0154              4            4\n 3 Framingham Grafton         0.0191              1            2\n 4 Framingham Grafton         0.0191              4            4\n 5 Framingham Grafton         0.0191              2            2\n 6 Framingham Grafton         0.0231              5            5\n 7 Framingham Grafton         0.0304              5            5\n 8 Framingham Grafton         0.0247              4            4\n 9 Framingham Grafton         0.0247              4            3\n10 Framingham Grafton         0.0259              4            4\n11 Framingham Grafton         0.0259              3            3\n12 Framingham Grafton         0.0259              5            4\n13 Framingham Grafton         0.0159              4            4\n14 Framingham Grafton         0.0159              1            1\n15 Framingham Southborough    0.0392              2            2\n# … with 100 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe n argument to print() tells R the number of rows you want to see. width refers to the number of characters to print across the screen. Want to see every row and every column? Try:\n\nprint(trains, n = Inf, width = Inf)\n\nInf is an R object which means infinity.\n\n1.1.2 Exploring tibbles\nThere are many ways to get a feel for the data contained in a tibble.\n\n1.1.2.1 view()\n\nRun view(trains) in the Console in RStudio. Explore this tibble in the resulting pop up viewer.\nObserve that there are many different types of variables. Some of the variables are quantitative. These variables are numerical in nature. Other variables here, including gender and treatment, are categorical. Categorical variables take on one of a limited set of possible values.\n\n1.1.2.2 glimpse()\n\nWe can also explore a tibble by using glimpse().\n\nglimpse(trains)\n\nRows: 115\nColumns: 14\n$ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n$ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n$ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n$ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n$ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n$ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n$ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n$ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n$ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n$ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n$ hisp_perc      <dbl> 0.0264, 0.0154, 0.0191, 0.0191, 0.0191, 0.0231, 0.0304,…\n$ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n$ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…\n\n\nWe see the first few values for each variable in a row after the variable name. In addition, the data type of the variable is given immediately after each variable’s name, inside < >.\ndbl refers to “double”, which is computer terminology for quantitative/numerical variables. int is for “integer.” fct is refers to a “factor,” a variable use for catagorical or nominal data. chr is for character data.\n\n1.1.2.3 summary()\n\nUsesummary() to get a sense of the distribution of the variables in the tibble.\n\nsummary(trains)\n\n   treatment    att_start         att_end          gender         \n Treated:51   Min.   : 3.000   Min.   : 3.000   Length:115        \n Control:64   1st Qu.: 7.000   1st Qu.: 7.000   Class :character  \n              Median : 9.000   Median : 9.000   Mode  :character  \n              Mean   : 9.191   Mean   : 9.139                     \n              3rd Qu.:11.000   3rd Qu.:11.000                     \n              Max.   :15.000   Max.   :15.000                     \n                                                                  \n     race            liberal           party                age       \n Length:115         Mode :logical   Length:115         Min.   :20.00  \n Class :character   FALSE:64        Class :character   1st Qu.:33.00  \n Mode  :character   TRUE :51        Mode  :character   Median :43.00  \n                                                       Mean   :42.37  \n                                                       3rd Qu.:52.00  \n                                                       Max.   :68.00  \n                                                                      \n     income           line             station            hisp_perc      \n Min.   : 23500   Length:115         Length:115         Min.   :0.01210  \n 1st Qu.: 87500   Class :character   Class :character   1st Qu.:0.01960  \n Median :135000   Mode  :character   Mode  :character   Median :0.03000  \n Mean   :141813                                         Mean   :0.03821  \n 3rd Qu.:135000                                         3rd Qu.:0.04290  \n Max.   :300000                                         Max.   :0.25940  \n                                                        NA's   :1        \n ideology_start   ideology_end \n Min.   :1.000   Min.   :1.00  \n 1st Qu.:2.000   1st Qu.:2.00  \n Median :3.000   Median :3.00  \n Mean   :2.757   Mean   :2.73  \n 3rd Qu.:4.000   3rd Qu.:3.50  \n Max.   :5.000   Max.   :5.00  \n                               \n\n\n\n1.1.2.4 $ operator\nThe $ operator allows us to extract a single variable from a tibble and return it as a vector.\n\ntrains$age\n\n  [1] 31 34 63 45 55 37 53 36 54 42 33 50 24 40 53 50 33 33 32 57 41 36 43 25 41\n [26] 33 44 46 41 28 36 37 38 48 20 52 38 45 55 38 45 44 36 29 42 43 54 39 31 50\n [51] 60 67 54 44 50 20 57 25 60 44 35 54 52 47 60 47 22 56 50 21 29 45 46 42 23\n [76] 29 60 41 30 61 21 46 53 45 46 63 21 31 35 22 68 27 22 30 59 56 32 35 23 60\n[101] 50 31 43 30 54 52 52 50 37 27 55 42 68 52 50"
  },
  {
    "objectID": "01-visualization.html#basic-plots",
    "href": "01-visualization.html#basic-plots",
    "title": "\n1  Visualization\n",
    "section": "\n1.2 Basic Plots",
    "text": "1.2 Basic Plots\nThere are three essential components to a plot:\n\n\ndata: the dataset containing the variables of interest.\n\ngeom: the geometric object to display, e.g., scatterplot, line, bar.\n\naes: aesthetic attributes of the geometric object. The most important are the names of the variables that should be on the x and y axes. Additional attributes include color and size. Aesthetic attributes are mapped to variables in the dataset.\n\nConsider a basic scatterplot using data from @enos2014 for 115 Boston commuters.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\n\n\n\n\nNotice how data and aes are specified in the call to ggplot(), followed by our choice of geom.\nPlots are composed of layers, combined using the + sign. The most essential layer specifies which type of geometric object we want the plot to use. In our graph above, the geom we used is geom_point().\nThe + sign comes at the end of the code line and not at the beginning. When adding layers to a plot, start a new line after the + so that the code for each layer is on a new line.\n\n1.2.1 geom_point()\n\nScatterplots, also called bivariate plots, allow you to visualize the relationship between two numerical variables.\nRecall our scatterplot from above.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\n\n\n\n\nLet’s break down this code, piece-by-piece.\n\nThe data argument is set to trains via data = trains.\nThe aesthetic mapping is set via mapping = aes(x = age, y = income). Here, we map age to the x axis and income to the y axis.\nThe geometric object is specified using geom_point(), telling R we want a scatterplot. We added a layer using the + sign.\n\nIf we do not specify the geometric object, we have a blank plot:\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income))\n\n\n\n\nIn addition to mapping variables to the x and y axes, we can also map variables to color.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income,\n                     color = party)) + \n  geom_point()\n\n\n\n\n\nWe use the function labs() to add a plot title, axis labels, subtitles, and captions to our graph. By default, R simply uses the names of variables for axes and legends. Add better titles and labels.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point() +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nNote that, as with geom’s, we add a layer using + when creating labs()for our plot. In general, every plot should give a title and axes labels. You should also add a subtitle, the purpose of which is to give a short “main point” of the graphic. What do you want the viewer to notice? You should also provide the source for the data, usually via the caption argument.\nLet’s now take a tour of some of the more useful geoms.\n\n1.2.2 Color and Fill\n\nBefore we take a look at the other useful geoms, let’s first talk about color and fill. These are two arguments we can use to change the color of geoms we have.\nLet’s take a look at this plot below.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income,\n                     color = party)) + \n  geom_point() +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nNotice that the plot is the exact same as the plot in the previous section, with one key change: we’ve added colors that separate the commuters by party. How did we do this? You’ll notice in the mapping = aes() function, we’ve added another argument. This argument, color =, can change the color of the dots. In this case, we changed the color by party.\nYou can also change the dots to all being a different.\n\nggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point(color = \"steelblue\") +\n  labs(title = \"Age and Income Among Boston Commuters\",\n       subtitle = \"Older commuters don't seem to make more money\",\n       x = \"Age\",\n       y = \"Income\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nBecause we set the color argument in the call to geom_point() directly, all the points are steel blue. (There are hundreds of other colors we could have chosen.) Note the difference between setting color as an aesthetic (with aes()) and as an argument to geom_point().\nThere is another argument you can use, called fill. The key difference between fill and color is that fill defines the color with which a geom is filled, whereas color sets the outline of a geom.\n\nggplot(data = trains,\n       mapping = aes(x = race)) +\n  geom_bar(color = \"pink\")\n\n\n\n\nYou can see here that color has only changed the very outline of the bars. If we used fill:\n\nggplot(data = trains,\n       mapping = aes(x = race)) +\n  geom_bar(fill = \"pink\")\n\n\n\n\n\n1.2.3 geom_jitter()\n\nConsider a different scatter plot using the trains data.\n\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nThe problem with this display is “overplotting.” Because attitudes are measured as integers, we do not know if a given point represents just one person or a dozen. There are two methods we can use to address overplotting: transparency and jitter.\nMethod 1: Changing the transparency\nWe can change the transparency/opacity of the points by using the alpha argument within geom_point(). The alpha argument can be set to any value between 0 and 1, where 0 sets the points to be 100% transparent and 1 sets the points to be 100% opaque. By default, alpha is set to 1.\n\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_point(alpha = 0.2) +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nNote that there is no aes() surrounding alpha = 0.2. This is because we are not mapping a variable to an aesthetic attribute, but only changing the default setting of alpha.\nMethod 2: Jittering the points\nWe can also decide to jitter the points on the plot. We do this by replacing geom_point() with geom_jitter(). Keep in mind that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged.\nIn order to specify how much jitter to add, we use the width and height arguments to geom_jitter(). This corresponds to how hard you’d like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. It is important to add just enough jitter to break any overlap in the points, but not to the extent where you alter the original pattern.\n\nggplot(data = trains, \n       mapping = aes(x = att_start, \n                     y = att_end)) + \n  geom_jitter() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nWhen deciding whether to jitter a scatterplot or use the alpha argument to geom_point(), know that there is no single right answer. We suggest you play around with both methods to see which one better emphasizes the point you are trying to make.\n\n1.2.4 geom_line()\n\nLinegraphs show the relationship between two numerical variables when the variable on the x-axis, also called the explanatory, predictive, or independent variable, is of a sequential nature. In other words, there is an inherent ordering to the variable.\n\n\n\n\n\nThe most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called time series plots.\nLet’s plot the median duration of unemployment in the United States over the last 50 years.\n\nggplot(data = economics,\n       mapping = aes(x = date, y = uempmed)) +\n  geom_line() +\n  labs(title = \"Unemployment Duration in the United States: 1965 -- 2015\",\n       subtitle = \"Dramatic increase in duration after the Great Recesssion\",\n       x = \"Date\",\n       y = \"Median Duration in Weeks\",\n       caption = \"Source: FRED Economic Data\")\n\n\n\n\nAlmost every aspect of the code used to create this plot is identical to our scatter plots, except for the geom we used.\n\n1.2.5 geom_histogram()\n\n\nA histogram is a plot that visualizes the distribution of a numerical value.\n\nWe first cut up the x-axis into a series of bins, where each bin represents a range of values.\nFor each bin, we count the number of observations that fall in the range corresponding to that bin.\nWe draw a bar whose height indicates the corresponding count.\n\nLet’s consider the income variable from the the trains tibble. Pay attention to how we have changed the two arguments to ggplot(). We have removed data = and mapping =. The code still works because R functions allow for passing in arguments by position. The first argument to ggplot() is the data. We don’t need to tell R that trains is the value for data. R assumes that it is because we passed it in as the first argument. Similarly, the second argument to ggplot() is mapping, so R assumes that aes(x = income) is the value we want for mapping because it is the second item passed in.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNote the message printed above:\n\nstat_bin() using bins = 30. Pick better value with binwidth.\n\nYou would get the same message if you ran this code yourself. Try it!\nThe message is telling us that the histogram was constructed using bins = 30 for 30 equally spaced bins. This is the default value. Unless you override this default number of bins with a number you specify, R will choose 30 by default. Because this is an important aspect of making a histogram, R insists on informing you with this message. You make this message go away by specifying the bin number yourself, as you should always do.\nLet’s specify bins and also add some labels.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nUnlike scatterplots and linegraphs, there is now only one variable being mapped in aes(). Here, that variable is income. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a geom_histogram().\nWe can use the fill argument to change the color of the actual bins. Let’s set fill to “steelblue”.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50,\n                 fill = \"steelblue\") +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nWe can also adjust the number of bins in our histogram in one of two ways:\n\nBy adjusting the number of bins via the bins argument to geom_histogram().\nBy adjusting the width of the bins via the binwidth argument to geom_histogram().\n\nIn this data, however, there are not many unique values for income, so neither approach will have much effect. Replace income with age if you want to experiment with these options.\n\n1.2.6 geom_bar()\n\ngeom_bar() visualizes the distribution of a categorical variable. This is a simpler task than creating a histogram, as we are simply counting different categories within a categorical variable, also known as the levels of the categorical variable. Often the best way to visualize these different counts, also known as frequencies, is with a barplot.\n\nggplot(data = trains, \n       mapping = aes(x = race)) +\n  geom_bar()\n\n\n\n\n\n1.2.6.1 Two categorical variables\nAnother use of barplots is to visualize the joint distribution of two categorical variables. (See Chapter @ref(probability) for the definition of a joint distribution.) Let’s look at race, as well as treatment, in the trains data by using the fill argument inside the aes() aesthetic mapping. Recall the fill aesthetic corresponds to the color used to fill the bars.\n\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar()\n\n\n\n\nThis is an example of a stacked barplot. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of people of different races within each region.\nAn alternative to stacked barplots are side-by-side barplots, also known as dodged barplots. The code to create a side-by-side barplot includes a position = \"dodge\" argument added inside geom_bar(). In other words, we are overriding the default barplot type, which is a stacked barplot, and specifying it to be a side-by-side barplot instead.\n\nggplot(trains, \n       aes(x = race, fill = treatment)) +\n  geom_bar(position = \"dodge\")\n\n\n\n\nWhites are over-represented in the Control group even though the treatment was assigned at random.\n\n1.2.7 geom_col()\n\ngeom_col() is similar to geom_bar(), except that geom_col() requires you to calculate the number of observations in each category ahead of time. geom_bar() does the calculation for you. See an example below.\n\ntrains |> \n  group_by(race,treatment) |>\n  summarize(count = sum(n())) |> \n  ggplot(mapping = aes(x = race,\n                       y = count,\n                       fill = treatment)) +\n   geom_col(position = \"dodge\")\n\n`summarise()` has grouped output by 'race'. You can override using the\n`.groups` argument.\n\n\n\n\n\nYou will learn to filter the data later in this chapter. However, what is key here is there was a y-variable supplied in geom_col() but there was not in geom_bar(). geom_col() gives you more control over the data that is being presented compared to geom_bar(), which may come in useful in some circumstances.\n\n1.2.7.1 No pie charts!\nOne of the most common plots used to visualize the distribution of categorical data is the pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. @robbins2013 argues that we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another. Do not use pie charts.\n\n1.2.8 geom_smooth()\n\nWe can add trend lines to the plots we create using the geom_smooth() function.\nRecall the following scatterplot from our previous work.\n\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nWe can add a trend line to our graph by adding the layer geom_smooth(). Including trend lines allow us to visualize the relationship between att_start and att_end.\n\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nNote the message. R is telling us that we need to specify the method and formula argument, just the way it told us to provide the bins argument when we used geom_histogram() before.\nLet’s add the argument method = \"lm\", where “lm” stands for linear model. This causes the fitted line to be straight rather than curved. Let’s also add the argument formula = y ~ x. We make the argument y ~ x since R doesn’t know what model it is estimating, and gives us a warning that y is a function of x. By specifying this relationship, the warning disappears as R is sure of the model it is estimating. Again, R was not giving us an error before. It was simply telling us what options it was using since we did not specify the options ourselves.\nAlways include enough detail in your code to make those messages disappear.\n\nggplot(trains, \n       aes(x = att_start, \n           y = att_end)) + \n  geom_point() +\n  labs(title = \"Immigration Attitudes Among Boston Commuters\",\n       subtitle = \"Attitudes did not change much after the experiment\",\n       x = \"Attitude Before Experiment\",\n       y = \"Attitude After Experiment\",\n       caption = \"Data source: Enos (2014)\") +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x)\n\n\n\n\nNotice the gray section surrounding the line we plotted. This area is called the confidence interval, which is set to 95% by default. We will learn about confidence intervals in Chapter @ref(probability). You can make the shaded area disappear by adding se = FALSE as another argument to geom_smooth().\n\n1.2.9 geom_density()\n\nRecall our plot from the geom_histogram() section.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_histogram(bins = 50) +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = \"Count\",\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nChange geom_histogram() to geom_density() to make a density plot, which is a smoothed version of the histogram.\n\nggplot(trains, \n       aes(x = income)) +\n  geom_density() +\n  labs(title = \"Income Among Boston Commuter\",\n       subtitle = \"Why are there so few people with `middle' incomes?\",\n       x = \"Income\",\n       y = NULL,\n       caption = \"Data source: Enos (2014)\")\n\n\n\n\nThe values on the y-axis are scaled so that the total area under the curve equals one.\n\n1.2.10 Conclusion\nThe geoms we talked about are some of the most commonly used ones while using R. However, there are an endless list of options for you to pick from. We’ve listed a couple more here, however, feel free to check out this documentation here if you want to learn more!\n\n\ngeom_boxplot(): this creates a box and whiskers plot, which visualizes 5 summary statistics.\n\ngeom_dotplot(): this is similar to a bar graph, except with stacked dots on top of each other rather than bars.\n\ngeom_map(): this is a geom option you have to turn polygons to an actual map. However, there is another method that we will teach you in Maps that also allows you to create maps.\n\ngeom_text(): this geom allows you to add text onto your plots. We will use an example of this in the Visualization Case Studies tutorial!"
  },
  {
    "objectID": "01-visualization.html#tidyverse",
    "href": "01-visualization.html#tidyverse",
    "title": "\n1  Visualization\n",
    "section": "\n1.3 Tidyverse",
    "text": "1.3 Tidyverse\nGoing forward, most ggplot() code will omit the data = and mapping = explicit naming of arguments while relying on the default ordering. Most of the time, we include argument names and, as a rule, you should to. But we create so many plots in The Primer that these omissions are unlikely to cause problems.\n\n1.3.1 Data wrangling\nWe can’t use all the beautiful plots that we learned in the previous chapter until we have “wrangled” the data into a convenient shape. Key wrangling functions include:\n\nfilter(): to pick out the rows we want to keep from a tibble.\nselect(): to pick out the columns we want to keep from a tibble.\narrange(): to sort the rows in a tibble, in either ascending or descending order.\nmutate(): to create new columns.\ngroup_by(): to assign each row in a tibble to a “group.” This allows statistics to be calculated for each group separately. You will usually use group_by() with summarize().\nsummarize(): to create a new tibble comprised of summary statistics for one (or more) rows for each grouped variable, or for the tibble as a whole if it is ungrouped.\n\n1.3.2 The pipe operator: |>\n\nThe pipe operator (|>) allows us to combine multiple operations in R into a single sequential chain of actions. Much like how the + sign has to come at the end of the line when constructing plots — because we are building the plot layer-by-layer — the pipe operator |> has to come at the end of the line because we are building a data wrangling pipeline step-by-step. If you do not include the pipe operator, R assumes the next line of code is unrelated to the layers you built and you will get an error.\n\n1.3.3 filter() rows\n\n\n\n\nfilter() reduces the rows in a tibble.\n\n\n\n\nThe filter() function works much like the “Filter” option in Microsoft Excel. It allows you to specify criteria about the values of a variable in your dataset and then selects only the rows that match that criteria.\n\ntrains |> \n  filter(gender == \"Male\")\n\n# A tibble: 64 × 14\n   treat…¹ att_s…² att_end gender race  liberal party   age income line  station\n   <fct>     <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n 1 Treated       3       5 Male   White TRUE    Demo…    63 135000 Fram… Grafton\n 2 Treated      11      11 Male   White FALSE   Demo…    45 300000 Fram… Grafton\n 3 Control       8       5 Male   White TRUE    Demo…    55 135000 Fram… Grafton\n 4 Treated      10      11 Male   White FALSE   Demo…    36 135000 Fram… Grafton\n 5 Treated       9      10 Male   White FALSE   Repu…    42 135000 Fram… Grafton\n 6 Treated      11       9 Male   White FALSE   Demo…    50 250000 Fram… Grafton\n 7 Treated      13      13 Male   White FALSE   Repu…    24 105000 Fram… Grafton\n 8 Control       6       7 Male   White TRUE    Demo…    40  62500 Fram… Grafton\n 9 Control       8       8 Male   White TRUE    Demo…    53 300000 Fram… Southb…\n10 Treated      13      13 Male   Asian FALSE   Repu…    33 250000 Fram… Southb…\n# … with 54 more rows, 3 more variables: hisp_perc <dbl>, ideology_start <int>,\n#   ideology_end <int>, and abbreviated variable names ¹​treatment, ²​att_start\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe result of using filter() will be a tibble with just the rows that you want. When we alter our data, it can be a good idea to save the result in a new data frame by using the <- assignment operator.\n\ntrains_men <- trains |> \n  filter(gender == \"Male\")\n\nLet’s break down the code. We assigned our new data to an object named trains_men via trains_men <-. Because we assigned this modified data frame to trains_men, it is a separate entity from the initial trains data frame. If, however, we had written the code as trains <- trains we would have overwritten the already-existing tibble.\nWe start with the trains tibble and then filter() so that only those observations where the gender equals “Male” are included. We test for equality using the double equal sign == and not a single equal sign =. In other words, filter(gender = \"Male\") will produce an error. This is a convention across many programming languages.\nYou can use other operators beyond just the == operator.\n\n\n> for “greater than”\n\n< for “less than”\n\n>= for “greater than or equal to”\n\n<= for “less than or equal to”\n\n!= for “not equal to.” The ! indicates “not.”\n\nFurthermore, you can combine multiple criteria using operators that make comparisons:\n\n\n| for “or”\n\n& for “and”\n\nFor example, let’s filter() the trains tibble to include only women who are Republicans and younger than 40.\n\ntrains |> \n  filter(gender == \"Female\" & \n           party == \"Republican\" &\n           age < 40)\n\n# A tibble: 3 × 14\n  treatm…¹ att_s…² att_end gender race  liberal party   age income line  station\n  <fct>      <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n1 Treated        9      10 Female White FALSE   Repu…    34 105000 Fram… Southb…\n2 Control       11      10 Female White FALSE   Repu…    21 135000 Fran… Norfolk\n3 Control       15      12 Female White FALSE   Repu…    21 250000 Fran… Norfolk\n# … with 3 more variables: hisp_perc <dbl>, ideology_start <int>,\n#   ideology_end <int>, and abbreviated variable names ¹​treatment, ²​att_start\n# ℹ Use `colnames()` to see all variable names\n\n\nInstead of creating a single criterion with many parts, like with an &, you can just separate the parts with a comma. The resulting tibble is the same.\n\ntrains |> \n  filter(gender == \"Female\",\n         party == \"Republican\",\n         age < 40)\n\n# A tibble: 3 × 14\n  treatm…¹ att_s…² att_end gender race  liberal party   age income line  station\n  <fct>      <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n1 Treated        9      10 Female White FALSE   Repu…    34 105000 Fram… Southb…\n2 Control       11      10 Female White FALSE   Repu…    21 135000 Fran… Norfolk\n3 Control       15      12 Female White FALSE   Repu…    21 250000 Fran… Norfolk\n# … with 3 more variables: hisp_perc <dbl>, ideology_start <int>,\n#   ideology_end <int>, and abbreviated variable names ¹​treatment, ²​att_start\n# ℹ Use `colnames()` to see all variable names\n\n\n\n1.3.4 select variables\n\n\n\n\nselect() reduces the number of columns in a tibble.\n\n\n\n\nUsing the filter() function we were able to pick out specific rows (observations) from the tibble. The select() function allows us to pick specific columns (variables) instead.\nUse glimpse() to see the names of the variables in trains:\n\nglimpse(trains)\n\nRows: 115\nColumns: 14\n$ treatment      <fct> Treated, Treated, Treated, Treated, Control, Treated, C…\n$ att_start      <dbl> 11, 9, 3, 11, 8, 13, 13, 10, 12, 9, 10, 11, 13, 6, 8, 1…\n$ att_end        <dbl> 11, 10, 5, 11, 5, 13, 13, 11, 12, 10, 9, 9, 13, 7, 8, 1…\n$ gender         <chr> \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"…\n$ race           <chr> \"White\", \"White\", \"White\", \"White\", \"White\", \"White\", \"…\n$ liberal        <lgl> FALSE, FALSE, TRUE, FALSE, TRUE, FALSE, FALSE, FALSE, F…\n$ party          <chr> \"Democrat\", \"Republican\", \"Democrat\", \"Democrat\", \"Demo…\n$ age            <int> 31, 34, 63, 45, 55, 37, 53, 36, 54, 42, 33, 50, 24, 40,…\n$ income         <dbl> 135000, 105000, 135000, 300000, 135000, 87500, 87500, 1…\n$ line           <chr> \"Framingham\", \"Framingham\", \"Framingham\", \"Framingham\",…\n$ station        <chr> \"Grafton\", \"Southborough\", \"Grafton\", \"Grafton\", \"Graft…\n$ hisp_perc      <dbl> 0.0264, 0.0154, 0.0191, 0.0191, 0.0191, 0.0231, 0.0304,…\n$ ideology_start <int> 3, 4, 1, 4, 2, 5, 5, 4, 4, 4, 3, 5, 4, 1, 2, 2, 3, 4, 2…\n$ ideology_end   <int> 3, 4, 2, 4, 2, 5, 5, 4, 3, 4, 3, 4, 4, 1, 2, 3, 3, 1, 2…\n\n\nHowever, if you only need two of these variables, say gender and treatment. You can select() just these two:\n\ntrains |> \n  select(gender, treatment)\n\n# A tibble: 115 × 2\n   gender treatment\n   <chr>  <fct>    \n 1 Female Treated  \n 2 Female Treated  \n 3 Male   Treated  \n 4 Male   Treated  \n 5 Male   Control  \n 6 Female Treated  \n 7 Female Control  \n 8 Male   Treated  \n 9 Female Control  \n10 Male   Treated  \n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nYou can drop, or “de-select,” certain variables by using the minus (-) sign:\n\ntrains |> \n  select(-gender, -liberal, -party, -age)\n\n# A tibble: 115 × 10\n   treatment att_st…¹ att_end race  income line  station hisp_…² ideol…³ ideol…⁴\n   <fct>        <dbl>   <dbl> <chr>  <dbl> <chr> <chr>     <dbl>   <int>   <int>\n 1 Treated         11      11 White 135000 Fram… Grafton  0.0264       3       3\n 2 Treated          9      10 White 105000 Fram… Southb…  0.0154       4       4\n 3 Treated          3       5 White 135000 Fram… Grafton  0.0191       1       2\n 4 Treated         11      11 White 300000 Fram… Grafton  0.0191       4       4\n 5 Control          8       5 White 135000 Fram… Grafton  0.0191       2       2\n 6 Treated         13      13 White  87500 Fram… Grafton  0.0231       5       5\n 7 Control         13      13 White  87500 Fram… Grafton  0.0304       5       5\n 8 Treated         10      11 White 135000 Fram… Grafton  0.0247       4       4\n 9 Control         12      12 White 105000 Fram… Grafton  0.0247       4       3\n10 Treated          9      10 White 135000 Fram… Grafton  0.0259       4       4\n# … with 105 more rows, and abbreviated variable names ¹​att_start, ²​hisp_perc,\n#   ³​ideology_start, ⁴​ideology_end\n# ℹ Use `print(n = ...)` to see more rows\n\n\nYou can specify a range of columns by using the : operator.\n\ntrains |> \n  select(gender:age)\n\n# A tibble: 115 × 5\n   gender race  liberal party        age\n   <chr>  <chr> <lgl>   <chr>      <int>\n 1 Female White FALSE   Democrat      31\n 2 Female White FALSE   Republican    34\n 3 Male   White TRUE    Democrat      63\n 4 Male   White FALSE   Democrat      45\n 5 Male   White TRUE    Democrat      55\n 6 Female White FALSE   Democrat      37\n 7 Female White FALSE   Republican    53\n 8 Male   White FALSE   Democrat      36\n 9 Female White FALSE   Democrat      54\n10 Male   White FALSE   Republican    42\n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nThis will select() all columns between the two specified variables. The select() function can also be used to rearrange columns when used with the everything() helper function. We can put the treatment and gender variables first with:\n\ntrains |> \n  select(treatment, gender, everything())\n\n# A tibble: 115 × 14\n   treat…¹ gender att_s…² att_end race  liberal party   age income line  station\n   <fct>   <chr>    <dbl>   <dbl> <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n 1 Treated Female      11      11 White FALSE   Demo…    31 135000 Fram… Grafton\n 2 Treated Female       9      10 White FALSE   Repu…    34 105000 Fram… Southb…\n 3 Treated Male         3       5 White TRUE    Demo…    63 135000 Fram… Grafton\n 4 Treated Male        11      11 White FALSE   Demo…    45 300000 Fram… Grafton\n 5 Control Male         8       5 White TRUE    Demo…    55 135000 Fram… Grafton\n 6 Treated Female      13      13 White FALSE   Demo…    37  87500 Fram… Grafton\n 7 Control Female      13      13 White FALSE   Repu…    53  87500 Fram… Grafton\n 8 Treated Male        10      11 White FALSE   Demo…    36 135000 Fram… Grafton\n 9 Control Female      12      12 White FALSE   Demo…    54 105000 Fram… Grafton\n10 Treated Male         9      10 White FALSE   Repu…    42 135000 Fram… Grafton\n# … with 105 more rows, 3 more variables: hisp_perc <dbl>,\n#   ideology_start <int>, ideology_end <int>, and abbreviated variable names\n#   ¹​treatment, ²​att_start\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe helper functions starts_with(), ends_with(), and contains() can be used to select variables/columns that match those conditions. Examples:\n\ntrains |> \n  select(starts_with(\"a\"))\n\n# A tibble: 115 × 3\n   att_start att_end   age\n       <dbl>   <dbl> <int>\n 1        11      11    31\n 2         9      10    34\n 3         3       5    63\n 4        11      11    45\n 5         8       5    55\n 6        13      13    37\n 7        13      13    53\n 8        10      11    36\n 9        12      12    54\n10         9      10    42\n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n1.3.5 slice() and pull() and []\n\nslice() and pull() are additional functions that you can use to pick out specific rows or columns within a data frame.\nUsing slice() gives us specific rows from the trains tibble:\n\ntrains |> \n  slice(2:5)\n\n# A tibble: 4 × 14\n  treatm…¹ att_s…² att_end gender race  liberal party   age income line  station\n  <fct>      <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n1 Treated        9      10 Female White FALSE   Repu…    34 105000 Fram… Southb…\n2 Treated        3       5 Male   White TRUE    Demo…    63 135000 Fram… Grafton\n3 Treated       11      11 Male   White FALSE   Demo…    45 300000 Fram… Grafton\n4 Control        8       5 Male   White TRUE    Demo…    55 135000 Fram… Grafton\n# … with 3 more variables: hisp_perc <dbl>, ideology_start <int>,\n#   ideology_end <int>, and abbreviated variable names ¹​treatment, ²​att_start\n# ℹ Use `colnames()` to see all variable names\n\n\nUnlike filter(), slice() relies on numeric order of the data.\npull() grabs out a variable as a vector, rather than leaving it within a tibble, as select() does:\n\ntrains |> \n  slice(2:5) |> \n  pull(age)\n\n[1] 34 63 45 55\n\n\n\n1.3.6 arrange()\n\narrange() allows us to sort/reorder a tibble’s rows according to the values of a specific variable. Unlike filter() or select(), arrange() does not remove any rows or columns from the tibble. Example:\n\ntrains |> \n  select(treatment, gender, age) |> \n  arrange(age)\n\n# A tibble: 115 × 3\n   treatment gender   age\n   <fct>     <chr>  <int>\n 1 Treated   Female    20\n 2 Control   Male      20\n 3 Control   Male      21\n 4 Control   Female    21\n 5 Control   Female    21\n 6 Control   Male      22\n 7 Control   Female    22\n 8 Treated   Male      22\n 9 Treated   Male      23\n10 Control   Male      23\n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\narrange() always returns rows sorted in ascending order by default. To switch the ordering to descending order instead, use the desc() function:\n\ntrains |> \n  select(treatment, gender, age) |> \n  arrange(desc(age))\n\n# A tibble: 115 × 3\n   treatment gender   age\n   <fct>     <chr>  <int>\n 1 Control   Female    68\n 2 Control   Male      68\n 3 Control   Male      67\n 4 Treated   Male      63\n 5 Control   Male      63\n 6 Control   Male      61\n 7 Control   Female    60\n 8 Treated   Male      60\n 9 Control   Male      60\n10 Control   Female    60\n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThis is the first of many “pipes” which we will create in The Primer. First, we have the trains tibble. Second, we pipe that to the select() function. Third, we pipe the results of select() to the arrange() function. Each step in the pipe starts with a tibble and then, once it is done, produces a tibble. It is tibbles all the way down!\n\n1.3.7 mutate()\n\n\n\n\n\n`mutate() adds a column to a tibble.\n\n\n\n\nmutate() takes existing columns and creates a new column. Recall that the income variable in the trains tibble is in dollars. Let’s use mutate() to create a new variable which is income in thousands of dollars. (We use select() at the start of the pipe so that it is easier to see the new and old variables at the same time.)\n\ntrains |> \n  select(gender, income) |> \n  mutate(income_in_thousands = income / 1000)\n\n# A tibble: 115 × 3\n   gender income income_in_thousands\n   <chr>   <dbl>               <dbl>\n 1 Female 135000               135  \n 2 Female 105000               105  \n 3 Male   135000               135  \n 4 Male   300000               300  \n 5 Male   135000               135  \n 6 Female  87500                87.5\n 7 Female  87500                87.5\n 8 Male   135000               135  \n 9 Female 105000               105  \n10 Male   135000               135  \n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNotice that we have a newly created column at the right-hand side of our tibble named income_in_thousands.\nWhen creating new variables we can also overwrite the original tibble:\n\ntrains <- trains |> \n  mutate(income_in_thousands = (income) / 1000)\n\n\nWhenever we create a new tibble, or a new variable within a tibble, we face a dilemma: Should we overwrite the existing tibble/variable or create a new one? There is no right answer.\nFor example, instead of overwriting trains in the code above, we could have created a new tibble trains_new. Similarly, instead of creating a new variable, income_in_thousands, we could have overwritten the current value of income. Use your best judgment and be careful.\n\n1.3.7.1 if_else()\n\nif_else() is often used within calls to mutate(). It has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE.\nImagine that we want to create a new variable old, which is TRUE when age > 50 and FALSE otherwise.\n\ntrains |> \n  select(age) |> \n  mutate(old = if_else(age > 50, TRUE, FALSE))\n\n# A tibble: 115 × 2\n     age old  \n   <int> <lgl>\n 1    31 FALSE\n 2    34 FALSE\n 3    63 TRUE \n 4    45 FALSE\n 5    55 TRUE \n 6    37 FALSE\n 7    53 TRUE \n 8    36 FALSE\n 9    54 TRUE \n10    42 FALSE\n# … with 105 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAnother function similar to if_else(), is dplyr::case_when(). case_when() is particularly useful inside mutate when you want to create a new variable that relies on a complex combination of existing variables. Note that there is a different version of if_else() in base R: ifelse(). This works exactly the same as the dplyr version but is somewhat less robust. For this reason, we prefer the if_else() version.\n\n1.3.8 summarize()\n\nWe often need to calculate summary statistics, things like the mean (also called the average) and the median (the middle value). Other examples of summary statistics include the sum, the minimum, the maximum, and the standard deviation.\nThe function summarize() allows us to calculate these statistics on individual columns from a tibble. Example:\n\ntrains |> \n  summarize(mn_age = mean(age), \n            sd_age = sd(age))\n\n# A tibble: 1 × 2\n  mn_age sd_age\n   <dbl>  <dbl>\n1   42.4   12.2\n\n\nThe mean() and sd() summary functions go inside the summarize() function. The summarize() function takes in a tibble and returns a tibble with only one row corresponding to the summary statistics. Remember: Tibbles go in and tibbles come out.\nmean()\nThe mean, or average, is the most commonly reported measure of the center of a distribution. The mean is the sum of all of the data elements divided by the number of elements. If we have \\(N\\) data points, the mean is given by:\n\\[\\bar{x} = \\frac{x_1 + x_2 + \\cdots + x_N}{N}\\]\nmedian()\nThe median is another commonly reported measure of the center of a distribution, calculated by first sorting the vector of values from smallest to largest. The middle element in the sorted list is the median. If the middle falls between two values, then the median is the mean of those two middle values. The median and the mean are the two most common measures of the center of a distribution. The median is more stable, less affected by outliers. There is no widely accepted symbol for the median, although \\(\\tilde{x}\\) is not uncommon. If we have \\(n\\) data points, and \\(n\\) is even, the median is given by:\n\\[m(x) = {\\frac{1}{2}}{(x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1})}\\]\nIf \\(n\\) is odd, then the median is given by:\nsd()\nThe standard deviation (sd) of a distribution is a measure of its variation around the mean.\n\\[\\text{sd} = \\sqrt{\\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + \\cdots + (x_n - \\bar{x})^2}{n - 1}}\\]\nmad()\nThe scaled median absolute deviation (mad) is a measure of variation around the median. It is not as popular as the standard deviation. The formula for calculating mad is a bit mysterious.\n\\[\\text{mad} = 1.4826 \\times \\text{median}(abs(x - \\tilde{x}))\\]\nThe basic idea for both sd and mad is that we need a measure of variation around the center of the distribution of the variable. sd uses the mean, \\(\\bar{x}\\), as its estimate of the center while mad uses the median, \\(\\tilde{x}\\). Because mad uses the absolute difference, as opposed to the squared difference, it is more robust to outliers. The 1.4826 multiplier causes the mad and the sd to be identical in the (important) case of standard normal distributions, a topic we will introduce in Chapter @ref(wrangling).\nquantile()\nThe quantile of a distribution is the value of that distribution which occupies a specific percentile location in the sorted list of values.\nI.e., the 5th percentile distribution is the point below which 5% of the data falls. The 95th percentile is, similarly, the point below which 95% of the data falls. The 50th percentile, the median, splits the data into two separate, and equal, parts. The minimum is at the 0th percentile. The maximum is at the 100th percentile.\nTherefore, the value of the 5th percentile would be the quantile of the 5th percentile. Say the dataset consisted of numbers from 0-100. Therefore, the 5th percentile would at 5, so 5 would be the quantile.\nLet’s take a look at the poverty variable in the kenya tibble from the primer.data package. poverty is the percentage of residents in each community with incomes below the poverty line. Let’s first confirm that quantile() works by comparing its output with that from simpler functions.\n\nc(min(kenya$poverty), median(kenya$poverty), max(kenya$poverty))\n\n[1] 0.1810048 0.4315779 0.8989494\n\nquantile(kenya$poverty, probs = c(0, 0.5, 1))\n\n       0%       50%      100% \n0.1810048 0.4315779 0.8989494 \n\n\nThe probs argument allows us to specify the percentile(s) we want. Two of the most important percentiles are the 2.5th and 97.5th because they define the 95% interval, a central range which includes 95% of the values.\n\nquantile(kenya$poverty, probs = c(0.025, 0.975))\n\n     2.5%     97.5% \n0.2225586 0.6588897 \n\n\nThe interval between these two percentiles includes 95% of all the values in the distribution. Depending on the context, this interval is sometimes called a “confidence interval” or “uncertainty interval” or “compatibility interval.” Different percentile ranges create intervals of different widths.\nIf there is an NA value in the variable, any statistical function like mean() will return NA. You can fix this by using na.rm = TRUE within the statistical function.\n\n1.3.9 group_by()\n\n\nWe can the use mean() with summarize() to calculate the average age for all the people in trains, as we did above.\n\ntrains |> \n  summarize(avg = mean(age))\n\n# A tibble: 1 × 1\n    avg\n  <dbl>\n1  42.4\n\n\nWhat if we want the mean age for each gender? Consider:\n\ntrains |> \n  group_by(gender)\n\n# A tibble: 115 × 15\n# Groups:   gender [2]\n   treat…¹ att_s…² att_end gender race  liberal party   age income line  station\n   <fct>     <dbl>   <dbl> <chr>  <chr> <lgl>   <chr> <int>  <dbl> <chr> <chr>  \n 1 Treated      11      11 Female White FALSE   Demo…    31 135000 Fram… Grafton\n 2 Treated       9      10 Female White FALSE   Repu…    34 105000 Fram… Southb…\n 3 Treated       3       5 Male   White TRUE    Demo…    63 135000 Fram… Grafton\n 4 Treated      11      11 Male   White FALSE   Demo…    45 300000 Fram… Grafton\n 5 Control       8       5 Male   White TRUE    Demo…    55 135000 Fram… Grafton\n 6 Treated      13      13 Female White FALSE   Demo…    37  87500 Fram… Grafton\n 7 Control      13      13 Female White FALSE   Repu…    53  87500 Fram… Grafton\n 8 Treated      10      11 Male   White FALSE   Demo…    36 135000 Fram… Grafton\n 9 Control      12      12 Female White FALSE   Demo…    54 105000 Fram… Grafton\n10 Treated       9      10 Male   White FALSE   Repu…    42 135000 Fram… Grafton\n# … with 105 more rows, 4 more variables: hisp_perc <dbl>,\n#   ideology_start <int>, ideology_end <int>, income_in_thousands <dbl>, and\n#   abbreviated variable names ¹​treatment, ²​att_start\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\nThe data is the same as before, but note the “Groups” message at the top. R is informing you that this tibble has been grouped so that any operation you perform now will be done for each gender.\n\ntrains |> \n  group_by(gender) |> \n  summarize(avg = mean(age))\n\n# A tibble: 2 × 2\n  gender   avg\n  <chr>  <dbl>\n1 Female  41.0\n2 Male    43.5\n\n\nNotice the message R sends us. The warning means that the tibble which issues forth from the end of the pipe has been “ungrouped”. This means the group attribute we applied with group_by() has been removed. This behavior is the (sensible) default.\nThe proper way to handle the situation, here and everywhere else that we use group_by() and summarize(), is to specify the .groups argument.\n\ntrains |> \n  group_by(gender) |> \n  summarize(mean = mean(age),\n            .groups = \"drop\")\n\n# A tibble: 2 × 2\n  gender  mean\n  <chr>  <dbl>\n1 Female  41.0\n2 Male    43.5\n\n\nThis code does the same thing as the first version, but does not issue a message, since we have made an affirmative decision to drop any grouping variables.\nThe group_by() function doesn’t change data frames by itself. Rather it changes the meta-data, or data about the data, specifically the grouping structure. It is only after we apply the summarize() function that the tibble changes.\nIf you have a tibble which has been grouped, you can remove the grouping variable by using ungroup().\nWhen your R code is behaving in a weird way, especially when it is “losing” rows, the problem is often solved by using ungroup() in the pipeline."
  },
  {
    "objectID": "01-visualization.html#advanced-plots",
    "href": "01-visualization.html#advanced-plots",
    "title": "\n1  Visualization\n",
    "section": "\n1.4 Advanced Plots",
    "text": "1.4 Advanced Plots\n\n\n\n\nGood visualizations teach. When you construct a plot, decide what message you want to convey. Here are some functions which may be helpful.\n\n1.4.1 Plot objects\nPlots are R objects, just like tibbles. We can create them, print them and save them. Up until now, we have just “spat” them out in an R code chunk. Nothing wrong with that! Indeed, this is the most common approach to plotting in R. Sometimes, however, it is handy to work with a plot object. Consider:\n\ntrain_plot <- ggplot(data = trains, \n       mapping = aes(x = age, \n                     y = income)) + \n  geom_point()\n\nThis is the same code as our first example with geom_point(). train_plot is an R object. This code does not print anything out. In order to make this plot appear, we need to print it out explicitly:\n\ntrain_plot\n\n\n\n\nRecall that typing the name of an object is the same thing as using print(). Now that we have this object, we can display it whenever we want.\nBut, sometimes, we want a permanent copy of the plot, saved to our computer. That is the purpose of ggsave():\n\nggsave(filename = \"enos_trains.jpg\", \n       plot = train_plot)\n\nggsave() uses the suffix of the provided filename to determine the type of image to save. Because we use “enos_trains.jpg”, the file is saved in JPEG format. If we had used “enos_trains.png”, the file would have been saved as a PNG. We can display a saved file by using knitr::include_graphics(). For example:\n\nknitr::include_graphics(\"enos_trains.jpg\")\n\nThis code displays the image in an Rmd, assuming that the file “enos_trains.jpg” is located in the current working directory. A common scenario is that we create an image and store it in a directory named figures/ and then use that figure in more than one Rmd.\n\n1.4.2 Faceting\nFaceting splits a visualization into parts, one for each value of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose contents will differ.\nBefore we proceed, let’s create a subset of the tibble gapminder from the gapminder package. (You may need to install the gapminder package for this code to work. Refer back to the introduction if you need a refresher on how to do so.)\n\nlibrary(gapminder)\ngapminder_filt <- gapminder |> \n      filter(year == 2007, continent != \"Oceania\")\n\nLet’s plot our filtered data using geom_point()\n\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point()\n\n\n\n\nIt is difficult to compare the continents despite the colors. It would be much easier if we could “split” this scatterplot by the 4 continents. In other words, we would create plots of gdpPercap and lifeExp for each continent separately. We do this by using the function facet_wrap() with the argument ~ continent. In facet_wrap, you must always put the tilde (~) in front of the variable you wish to wrap it by.\n\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent)\n\n\n\n\nThis is much better! We can specify the number of rows and columns in the grid by using the nrow argument inside of facet_wrap(). Let’s get all continents in a row by setting nrow to 1. Let’s also add a trend line geom_smooth() to our faceted plot.\n\nggplot(data = gapminder_filt, \n       mapping = aes(x = gdpPercap, \n                     y = lifeExp, \n                     color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ x, \n              se = FALSE)\n\n\n\n\nAs expected, we can see a positive correlation between economic development and life expectancy on all continents.\n\n1.4.3 Stats\nConsider the following histogram.\n\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp))+ \n  geom_histogram(bins = 20, \n                 color = \"white\")\n\n\n\n\nRecall that the y-aesthetic of a histogram — the count of the observations in each bin — gets computed automatically. We can use the after_stat() argument within geom_histogram() to generate percent values as our y-aesthetic. after_stat() allows us to control the values of the variables calculated specifically for this specific aesthetic layer.\n\nggplot(data = gapminder, \n       mapping = aes(x = lifeExp)) + \n  geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 20) +\n  labs(y = \"Percentage\")\n\n\n\n\n\n1.4.4 Axis Limits and Scales\n\n1.4.4.1 coord_cartesian()\n\nWe can also manipulate the limits of the axes by using xlim() and ylim() within a call to coord_cartesian(). For example, assume that we are only interested in countries with a GDP per capita from 0 to 30,000. Recall that, because data is the first argument and mapping is the second to ggplot(), we don’t actually have to name the arguments. We can just provide them, as long as they are in the correct order.\n\nggplot(gapminder_filt, \n       aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  coord_cartesian(xlim = c(0, 30000))\n\n\n\n\nWe can see that the GDP per capita on the x-axis is now only shown from 0 to 30,000.\n\n1.4.4.2 scale_x and scale_y\n\nWe can also change the scaling of the axes. For example, it might be useful to display the axes on a logarithmic scale by using scale_x_log10() or scale_y_log10(). Also, note that we can (lazily!) not provide the explicit x and y argument names to aes() as long as we provide the values in the right order: x comes before y.\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10()\n\n\n\n\n\nBeyond scale_x_log10(), there are other ways to change the scales. We will cover scale_y_continuous and scale_x_continuous in this section.\nThere are two major uses for scale_x_continuous, and that is to change the breaks and the labels. Take the graph below.\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\n\n\n\n\nWe only want there to be breaks on the y-axis every 20 years instead of every 10. We also want to add dollar signs to the x-axis, and to have breaks every 20,000 dollars. Let’s fix the graph!\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~continent) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) +\n  scale_y_continuous(breaks = c(40, 60, 80)) +\n  scale_x_continuous(labels = scales::dollar_format(),\n                     breaks = c(0, 20000, 40000))\n\n\n\n\nLet’s break this down. We used the breaks argument to create the breaks on the scale. We used the c() function to specify what breaks we wanted. Then, we used labels to modify the labels on the x-axis. The :: allows us to extract a function from a specific package, if the function exists in multiple packages. So, we specifically extract the dollar_format function from the scales package to change the labels of the x-axis.\nThere is another function called scale_x_discrete/scale_y_discrete. This function is similar enough to scale_y/x_continuous that we will not give it its own section. The only difference in the usage of the discrete vs. continuous function is that the discrete function is applied to discrete variables. Discrete variables are those that are countable (i.e. the number of tables in a room) with nothing in between, whereas continuous variables have infinite possibilities (i.e. height has an infinite number of possible values).\n\n1.4.5 Text\nRecall we use labs() to add labels and titles to our plots. We can also change labels inside the plots using geom_text().\n\nggplot(gapminder_filt, \n       aes(gdpPercap, lifeExp, color = continent)) +\n  geom_point() +\n  facet_wrap(~ continent, nrow = 1) +\n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) + \n  scale_x_log10() +\n  labs(title = \"Life Expectancy and GDP per Capita (2007)\",\n       subtitle = \"Selected Nations by Continent\",\n       x = \"GDP per Capita, USD\",\n       y = \"Life Expectancy, Years\",\n       caption = \"Source: Gapminder\") +\n  geom_text(aes(label = country), \n            size = 2, \n            color = \"black\", \n            check_overlap = TRUE)\n\n\n\n\nLet’s breakdown the code within geom_text(). We included a new aesthetic called label. This defines the character variable which will be used as the basis for the labels. We set label to country so each point corresponds to the country it represents. We set the text font by setting size to 2, and we set the text color by using color. Finally, we included the argument check_overlap = TRUE to make sure the names of the countries were legible.\n\n1.4.6 Themes\nThemes can be used to change the overall appearance of a plot without much effort. We add themes as layers to our plots. You can find an overview of the different themes in ggplot here.\nConsider the following faceted scatterplot.\n\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) \n\n\n\n\nNote the use of the breaks argument to scale_x_log10(). This specifies the location of labels on the x-axis. We can also use the labels argument if we want to change their appearence. These tricks work in the entire family of scale_* functions.\nLet’s now add a theme to our faceted scatterplot. We will use the theme theme_economist(), from the ggthemes package, to make our plot look like the plots in the The Economist.\n\nlibrary(ggthemes)\n\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000)) +\n  theme_economist()\n\n\n\n\nThis looks pretty good. However, notice the legend on the top of our graph. It crowds our graph and takes away from the most important part: the data. We can use theme() to customize the non-data parts of our plots such as background, gridlines, and legends. Let’s de-clutter the graph by removing our legend. We can do this by using the legend.position argument and setting it to “none”.\n\ngapminder |>\n  filter(continent != \"Oceania\") |>\n  filter(year == max(year)) |> \n  ggplot(aes(gdpPercap, lifeExp, color = continent)) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(method = \"lm\", \n                formula = y ~ x,\n                se = FALSE) + \n    facet_wrap(~continent, nrow = 2) +\n    labs(title = \"Life Expectancy and GDP per Capita\",\n         subtitle = \"Connection between GDP and life expectancy is weakest in Africa\",\n         x = \"GDP per Capita in USD\",\n         y = \"Life Expectancy\") +\n    scale_x_log10(breaks = c(500, 5000, 50000),\n                  labels = scales::dollar_format(accuracy = 1)) + \n    theme_economist() +\n    theme(legend.position = \"none\")\n\n\n\n\nGreat. Now our graph is easier to visualize.\n\n1.4.6.1 theme()\nThe theme() function also offers a wide selection of functions for manually changing individual elements. We will cover the most widely used ones here, but the vast majority will be listed in that link.\nThere are two key elements in the theme() function.\n\n\nTheme elements: these specify the non-data elements you can control. For example, panel.border is the element that controls the border of the grid area.\n\nElement function: this describes the visual properties of the element. There are four main element functions, element_blank(), element_rect(), element_line(), element_text().\n\n\nelement_blank(): this hides the element from the theme.\n\nelement_line(): this modifies elements that are plot lines, grid lines, axes, etc.\n\nelement_text(): this changes the text elements of the plot, like titles, captions, etc.\n\nelement_rect(): rectangle elements control the background of plots, legends, etc.\n\n\n\nLet’s look at an example of how we might want to modify a graph. Take this graph below.\n\nggplot(data = economics,\n       mapping = aes(x = date,\n                     y = unemploy)) +\n  geom_line() +\n  labs(title = \"Unemployed Population in the United States: 1965 - 2015\",\n       subtitle = \"Dramatic spike during the Great Recesssion\",\n       x = \"Year\",\n       y = \"Number of Unemployed (in thousands)\",\n       caption = \"Source: FRED Economic Data\")\n\n\n\n\nYou have a couple of problems with this graph. Firstly, you want the title to be bold. You want there to be a light blue background. You also don’t particularly care for there to be so many grid lines from the x-axis. Let’s change this!\n\nggplot(data = economics,\n       mapping = aes(x = date,\n                     y = unemploy)) +\n  geom_line() +\n  labs(title = \"Unemployed Population in the United States: 1965 - 2015\",\n       subtitle = \"Dramatic spike during the Great Recesssion\",\n       x = \"Year\",\n       y = \"Number of Unemployed (in thousands)\",\n       caption = \"Source: FRED Economic Data\") +\n  theme(plot.title = element_text(face = \"bold\"),\n        panel.background = element_rect(fill = \"lightblue\"),\n        panel.grid.minor.x = element_blank())\n\n\n\n\nThere! Within the theme() function, we can change the plot title to bold, the background to blue, and get rid of the “minor” grid lines. theme() is a very versatile function. See the full list of elements here."
  },
  {
    "objectID": "01-visualization.html#going-further",
    "href": "01-visualization.html#going-further",
    "title": "\n1  Visualization\n",
    "section": "\n1.5 Going further",
    "text": "1.5 Going further\nThere are so many more plots you can make with R that we have not shown you yet. For example, you can create cool animations with the gganimate package, like this plot below.\n\n\n\n\n\nYou can make your plots interactive, using the plotly package. Remember, you need to load packages before you use them, so load library(plotly) if you want to make your own interactive plots. Example:\n\n\n\n\n\n\nYou can also create maps from census data, like this plot below.\n\n\nWarning: package 'tidycensus' was built under R version 4.2.1\n\n\nWarning: package 'tigris' was built under R version 4.2.1\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile."
  },
  {
    "objectID": "01-visualization.html#summary-1",
    "href": "01-visualization.html#summary-1",
    "title": "\n1  Visualization\n",
    "section": "\n1.6 Summary",
    "text": "1.6 Summary\nTibbles are rectangular stores of data. They are a specific type of data frame, so we will use both terms interchangeably.\nYou need to practice every day.\nDo not use pie charts.\nShield my eyes from your ugly messages and warning.\nEach step in the pipe starts with a tibble and then, once it is done, produces a tibble. It is tibbles all the way down!\nWhen your R code is behaving in a weird way, especially when it is “losing” rows, the problem is often solved by using ungroup() in the pipeline.\nThe two most important attributes of a distribution are its center and its variation around that center.\nIn this chapter, we first looked at basic coding terminology and concepts that we deal with when programming with R. We then learned about the three basic components that make up each plot: data, mapping, and one or more geoms. The ggplot2 package offers a wide range of geoms that we can use to create different types of plots. Next, we examined the “super package” tidyverse, which includes helpful tools for visualization. It also offers features for importing and manipulating data, which is the main topic of Chapter @(wrangling). Lastly, we explored advanced plotting features such as axis scaling, faceting, and themes.\nRecall the plot we began the chapter with:\n\n\n\n\n\nYou now know enough to make plots like this by yourself.\nA beautiful plot is just a collection of steps, each simple enough on its own. We have taught you (some of) these steps. Time to start walking on your own."
  },
  {
    "objectID": "07-two-parameters.html",
    "href": "07-two-parameters.html",
    "title": "7  Two Parameters",
    "section": "",
    "text": "In Chapter @ref(one-parameter), we learned how to do inference. We created a joint distribution of the models under consideration and the data which might be observed. Once we observed the data, we went from the joint distribution to the conditional distribution of possible models given the data which we did, in fact, observe. That conditional distribution, suitably normalized, was our posterior probability distribution over the space of possible models. With that distribution, we can answer any question we might (reasonably) ask.\nBut what a pain in the ass that whole process was! Do professionals actually go through all those steps every time they work on a data science problem? No! That would be absurd. Instead, professionals use standard tools which, in an automated fashion, take care of those steps, taking us directly from assumptions and data to the posterior:\n\\[\\text{Prob}(\\text{models} | \\text{data} = \\text{data we observed})\\]\nEven then, however, the relative likelihood of different models is not that important. Models are invisible, mental entities with no more physical presence than unicorns or leprechauns. In the world itself, we make and test predictions. People with better models make better predictions. That is what matters.\nIn addition to a change in tools, there are two key differences in this chapter. First, Chapter @ref(one-parameter) used models with just one parameter: the number of red beads, which we can also transform into a parameter, \\(p\\), the number of red beads divided by 1,000. The model in Chapter @ref(one-parameter) was binomial, and there is only one unknown parameter \\(p\\) in such models. In this chapter, we have two unknown parameters: the mean \\(\\mu\\) height in the US and the standard deviation, \\(\\sigma\\), of the normally distributed error term.\nSecond, Chapter @ref(one-parameter) dealt with a limited set of specific models: 1,001 of them, to be precise. The procedure used was just what we saw in Chapter @ref(probability). In this chapter, on the other hand, we have continuous parameters.\nBy this point, we should understand that the reason for making models is not, primarily, that making models is fun (although it is)! The reason is that, when confronted with a question, we face decisions. We must decide between variables X or Y. We must choose from datasets A, B and C. Confronted by a decision, we need to make a model of the world to help us.\nThe real world is complex. Any substantive decision problem includes a great deal of complexity and requires even more context. We do not have the time to get into that level of detail now. So, we simplify. We are going to create a model of height for adult men. We will then use that model to answer four questions:\nThe hope for this chapter is that, by answering these four questions, we’ll gain a better and more thorough understanding of how professionals do data science.\nData science is ultimately a moral act, so we will use the four Cardinal Virtues — Wisdom, Justice, Courage and Temperance — to organize our approach."
  },
  {
    "objectID": "07-two-parameters.html#wisdom",
    "href": "07-two-parameters.html#wisdom",
    "title": "7  Two Parameters",
    "section": "\n7.1 Wisdom",
    "text": "7.1 Wisdom\n\n7.1.1 Preceptor Table\n\n\n\n\n\nWhat rows and columns of data do you need such that, if you had them all, the calculation of the number of interest would be trivial? If you want to know the average height of an adult in India, then the Preceptor Table would include a row for each adult in India and a column for their height. In this scenario, we want to know the average height for men, where “men” includes all males on Earth that are over the age of 18.\nOne key aspect of this Preceptor Table is whether or not we need more than one potential outcome in order to calculate our estimand. Mainly: are we are modeling (just) for prediction or are we (also) modeling for causation? Do we need a causal model, one which estimates that attitude under both treatment and control? In a causal model, the Preceptor Table would require two columns for the outcome. In this case, we are not modeling for causation; thus, we do not need two outcome columns.\nPredictive models care nothing about causation. Causal models are often also concerned with prediction, if only as a means of measuring the quality of the model. Here, we are looking at prediction.\nSo, what would our ideal table look like? Assuming we are predicting height for every male on planet Earth at this moment in time, we would have height data for every male person above 18 years of age. This means that we would have almost 4 billion rows, one for each male person, which includes a column for height.\nHere is a sample from our Preceptor Table:\n\n\n\n\n\n\n\nID\n      Sex\n      Height (cm)\n    \n\n\nPerson 1\nMale\n150\n\n\nPerson 2\nMale\n172\n\n\n...\n...\n...\n\n\nPerson 45000\nMale\n160\n\n\nPerson 45001\nMale\n142\n\n\n...\n...\n...\n\n\n\n\n\n\nThis table would extend all the way until person 4 billion-and-something. If we had this table, all of our questions could be answered with basic math. No inference necessary. Now that we’ve seen our ideal dataset, what does our actual data look like?\n\n7.1.2 EDA for nhanes\n\nIn our quest to find suitable data, we find the nhanes dataset from the National Health and Nutrition Examination Survey conducted from 2009 to 2011 by the Centers for Disease Control and Prevention, which examines health and other pieces of data for children and adults in the United States.\n\nlibrary(tidyverse)\nlibrary(primer.data)\nlibrary(skimr)\nglimpse(nhanes)\n\nRows: 10,000\nColumns: 15\n$ survey         <int> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2…\n$ gender         <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male…\n$ age            <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58…\n$ race           <chr> \"White\", \"White\", \"White\", \"Other\", \"White\", \"White\", \"…\n$ education      <fct> High School, High School, High School, NA, Some College…\n$ hh_income      <fct> 25000-34999, 25000-34999, 25000-34999, 20000-24999, 350…\n$ weight         <dbl> 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7, 7…\n$ height         <dbl> 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.7,…\n$ bmi            <dbl> 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.24,…\n$ pulse          <int> 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 80,…\n$ diabetes       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ general_health <int> 3, 3, 3, NA, 3, NA, NA, 4, 4, 4, 4, 4, 2, NA, NA, 3, NA…\n$ depressed      <fct> Several, Several, Several, NA, Several, NA, NA, None, N…\n$ pregnancies    <int> NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, NA,…\n$ sleep          <int> 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, NA,…\n\n\nnhanes includes 15 variables, including physical attributes like weight and height. Let’s restrict our attention to a subset, focusing on age, gender and height.\n\nnhanes |> \n  select(age, gender, height)\n\n# A tibble: 10,000 × 3\n     age gender height\n   <int> <chr>   <dbl>\n 1    34 Male     165.\n 2    34 Male     165.\n 3    34 Male     165.\n 4     4 Male     105.\n 5    49 Female   168.\n 6     9 Male     133.\n 7     8 Male     131.\n 8    45 Female   167.\n 9    45 Female   167.\n10    45 Female   167.\n# … with 9,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nNow, let’s examine a random sample of our data:\n\nnhanes |> \n  select(age, gender, height) |> \n  slice_sample(n = 5)\n\n# A tibble: 5 × 3\n    age gender height\n  <int> <chr>   <dbl>\n1    15 Male     163.\n2    44 Female   170.\n3    39 Female   160.\n4     3 Female   102.\n5    16 Male     175.\n\n\nNotice how there is a decimal in the height column of ch7. This is because height is a <dbl> and not an <int>.\nLet’s also run glimpse() on our new data.\n\nnhanes |> \n  select(age, gender, height) |> \n  glimpse()\n\nRows: 10,000\nColumns: 3\n$ age    <int> 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, 58, 50, 9,…\n$ gender <chr> \"Male\", \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Fema…\n$ height <dbl> 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.7, 166.7, …\n\n\nBe on the lookout for anything suspicious. Are there any NA’s in your data set? What types of data are the columns, i.e. why is age characterized as integer instead of double? Are there more females than males?\nYou can never look at your data too closely.\nIn addition to glimpse(), we can run skim(), from the skimr package, to calculate some summary statistics.\n\nnhanes |> \n  select(age, gender, height) |> \n  skim()\n\n\nData summary\n\n\nName\nselect(nhanes, age, gende…\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ngender\n0\n1\n4\n6\n0\n2\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nage\n0\n1.00\n36.74\n22.40\n0.0\n17.0\n36\n54.0\n80.0\n▇▇▇▆▅\n\n\nheight\n353\n0.96\n161.88\n20.19\n83.6\n156.8\n166\n174.5\n200.4\n▁▁▁▇▂\n\n\n\n\n\nInteresting! There are 353 missing values of height in our subset of data. Just using glimpse() does not show us that. Let’s filter out the NA’s using drop_na(). This will delete the rows in which the value of any variable is missing. Because we want to examine height in men (not boys, nor females), let’s limit our data to only include adult males.\n\nch7 <- nhanes |> \n  filter(gender == \"Male\", age >= 18) |> \n  select(height) |> \n  drop_na()\n\nLet’s plot this data using geom_density() and geom_histogram().\n\n\nch7 |>\n  ggplot(aes(x = height)) + \n  geom_density(color = \"black\",\n               fill = \"red\",\n               alpha = .2) +\n  \n  # You can have multiple geom layers in a plot by simply adding them one after\n  # the other.\n  \n  # Be careful though! Often times you'll need to give each additional layer its\n  # own mapping argument in order for it to show up properly.\n  \n  geom_histogram(mapping = aes(y = ..density..),\n                 bins = 15,\n                 color = \"black\",\n                 fill = \"blue\",\n                 alpha = .2) +\n  \n  # In order to use Greek letters and other mathematical expressions in plot\n  # labels and titles, you'll need to use the expression() function, with\n  # appropriate arguments.\n  \n  labs(x = expression(mu),\n       y = \"Density\",\n       title = \"Height (cm) in NHANES Dataset\",\n       color = \"Sex\") +\n  theme_classic()\n\n\n\n\nWe’ll be focusing on a subset of the nhanes data, designed to answer our questions. And, instead of using all the data, we’ll just use 50 randomly selected observations. (The set.seed() function ensures that the same 50 observations are selected every time this code is run.)\n\n\nch7_all <- nhanes |>\n  filter(gender == \"Male\", age >= 18) |>\n  select(height) |>\n  drop_na() \n\nset.seed(9)\n\nch7 <- ch7_all |> \n  slice_sample(n = 50)\n\nWill the data we have — which is only for a sample of adult American men more than a decade ago — allow us to answer our questions, however roughly? This is where the notion of population comes into play.\n\n7.1.3 Population\nOne of the most important components of Wisdom is the concept of the “population.”\nThe population is not the set of people for which we have data — the participants in the CDC’s Health and Nutrition Examination Survey conducted from 2009 to 2011. This is the dataset. Nor is it the set of voters about whom we would like to have data. Those are the rows in the Preceptor Table. The population is the larger — potentially much larger — set of individuals which include both the data we have and the data we want. Generally, the population will be much larger than either the data we have and the data we want.\nIn this case, we want to predict average height for males today, not for people in 2009-2011! We also want to predict height for males outside the United States, a group that is excluded from our dataset. Is it reasonable to generate conclusions for the world from this group? Most likely, no. However, we have limited data to work with and we have to determine how far we are willing to generalize to other groups.\nIt is a judgment call, a matter of Wisdom, as to whether or not we may assume that the data we have and the data we want to have (i.e., the Preceptor Table) are drawn from the same population.\nNote that this judgement call differs from the discussion of representativeness in Justice. In Wisdom, we ask questions about how the data was collected, and what this says about our data. In Justice, we ask questions (already knowing how the data was collected) about whether or not inferences made using this data can apply to a broader scope of the population. Suppose, for instance, that the men sampled in NHANES were all NBA players. Wisdom would tell us that these heights are likely to be much greater than those of the average person. Justice asks whether or not this data is representative of the average person, and what modifications could imposed to fix a potential bias.\nIn the social sciences, there is never a perfect relationship between the data you have and the question you are trying to answer. Data for American males in the past is not the same thing as data for American males today. Nor is it the same as the data for men in France or Mexico.Yet, this data is relevant. Right? It is certainly better than nothing.\nUsing not-perfect data is generally better than using no data at all.\nIs not-perfect data always better? No! If your problem is estimating the median height of 5th grade girls in Tokyo, we doubt that our data is at all relevant. Wisdom recognizes the danger of using non-relevant data to build a model and then mistakenly using that model in a way which will only make the situation worse. If the data won’t help, don’t use the data, don’t build a model. Better to just use your common sense and experience. Or find better data.\nFor now, we will accept that our data works."
  },
  {
    "objectID": "07-two-parameters.html#justice",
    "href": "07-two-parameters.html#justice",
    "title": "7  Two Parameters",
    "section": "\n7.2 Justice",
    "text": "7.2 Justice\n\n\n\n\n\nHaving looked at our data and decided that it is “close enough” to our questions that creating a model will help us come up with better answers, we move on to Justice.\nJustice emphasizes a few key concepts:\n\nThe Population Table, a structure which includes a row for every unit in the population. We generally break the rows in the Population Table into three categories: the data for units we want to have (the Preceptor Table), the data for units which we actually have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).\nIs our data representative of the population?\nIs the meaning of the columns consistent, i.e., can we assume validity? We then make an assumption about the data generating mechanism.\n\nWe inspect both representativeness and validity through our Population Table. Representativeness focuses on the rows of the table, while validity focuses on the columns. Let’s explore what that means in more depth.\n\n7.2.1 The Population Table\nThe Population Table shows rows from three sources: the Preceptor Table, the actual data, and the population (outside of the data).\nOur Preceptor Table rows contain the information that we would want to know in order to answer our questions. These rows contain entries for our covariates (sex and year) but they do not contain any outcome results (height). We are trying to answer questions about the male population in 2021, so our sex entries for these rows will read “Male” and our year entries of these rows will read “2021”.\nOur actual data rows contain the information that we do know. These rows contain entries for both our covariates and the outcomes. In this case, the actual data comes from a study conducted on males in 2009-2011, so our sex entries for these rows will read “Male” and our year entries of these rows will either read “2009”, “2010”, or “2011”.\nOur population rows contain no data. These are subjects which fall under our desired population, but for which we have no data. As such, all rows are missing.\n\n\n\n\n\n\n\n\nSource\n      Sex\n      Year\n      Height\n    \n\n\n\nPreceptor Table\n\n\nMale\n\n\n2021\n\n\n?\n\n\n\n\nPreceptor Table\n\n\nMale\n\n\n2021\n\n\n?\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nActual Data\n\n\nMale\n\n\n2009\n\n\n180\n\n\n\n\nActual Data\n\n\nMale\n\n\n2011\n\n\n160\n\n\n\n\nActual Data\n\n\nMale\n\n\n2010\n\n\n168\n\n\n\n\n...\n\n\n...\n\n\n...\n\n\n...\n\n\n\n\nPopulation\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\nPopulation\n\n\n?\n\n\n?\n\n\n?\n\n\n\n\n\n\n\n\n7.2.2 Representativeness\nAs we’ve stated above, representativeness involves our rows. More specifically, are the rows that we do have data for representative of the rows for which we do not have data? Ideally, the data we have is a random, unbiased selection from our population, and so the answer to our question is yes.\nFor our nhanes data, is this the case? It is time to investigate.\nAccording to the CDC, individuals are invited to participate in NHANES based on a randomized process. First, the United States is divided into a number of geographical groups (to ensure counties from all areas). From each of these groups, counties are randomly selected to participate. After a county has been randomly selected, members of the households in that county are notified of the upcoming survey, and must volunteer their time to participate. It is clear that this process goes through several layers of randomization (promising!). That being said, many counties are excluded by the end of the process. It is also possible for certain groups or communities to be less representative of the greater population, though we cannot know that for certain.\nThere is also the fact that participation is voluntary. Perhaps certain individuals (immobile, elderly, anxious) are less likely to participate. Perhaps individuals that are hospitalized do not get the opportunity to participate. This impacts our data!\nRegardless, we can assume that this process ensures that all people in the United States have a somewhat equal chance of being surveyed. Thus, our data is representative of our population.\n\n7.2.3 Validity\nValidity involves our columns. More specifically, whether our columns mean the same thing. Consider the following example: we are predicting height with two different datasets. Because both datasets measure height in centimeters, we may assume validity — that our columns are identical in meaning. However, validity is much more complex than it appears at first glance.\nConsider the method of measurements. In our first dataset, were participants asked to remove their shoes? Were they allowed to keep on thick socks? If this is not the case in our second dataset, our columns do not technically represent the same truth.\nThere are even smaller differences that could impact validity. For instance, it is known that — over the course of a day — the average human’s spine compresses about an inch from the time we wake up until the time we go to sleep! If the first set of data was collected in the morning, while the second set of data was collected in the evening, our predictions may be off.\nAs best we can, we need to investigate any possible reasons that we may not be able to say that our columns mean the same thing. If we find an issue, we may need to adjust one set of data to match the other. For instance, if we knew that height was taken in the evening for one sample (with the other measurements taken in the morning), we may add an inch of height to those findings. This would ensure validity.\n\n\n7.2.4 Functional form\nHowever, a little mathematical notation will make our modeling assumptions clear, will bring some precision to our approach. In this case:\n\\[ y_i =  \\mu + \\epsilon_i \\]\nwith \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). \\(y_i\\) is the height of male \\(i\\). \\(\\mu\\) is the average height of all males in the population. \\(\\epsilon_i\\) is the “error term,” the difference between the height of male \\(i\\) and the average height of all males.\n\\(\\epsilon_i\\) is normally distributed with a mean of 0 and a standard deviation of \\(\\sigma\\). The mean being 0 relates to our concept of accuracy; we are assuming that our data is representative enough to be accurate, and so we can expect our average error to be 0. The standard deviation, on the other hand, relates to our concept of precision; the smaller \\(\\sigma\\) is, the more precise our data is, and the larger \\(\\sigma\\) is, the less precise our data is.\nThis is the simplest model we can construct. Note:\n\nThe model has two unknown parameters: \\(\\mu\\) and \\(\\sigma\\). Before we can do anything else we need to estimate the values of these parameters. Can we ever know their exact value? No! Perfection lies only in God’s own R code. But, by using a Bayesian approach similar to what we used in Chapters @ref(probability) and @ref(one-parameter), we will be able to create a posterior probability distribution for each parameter.\n\n\nThe model is wrong, as are all models.\nThe parameter we most care about is \\(\\mu\\). That is the parameter with a substantively meaningful interpretation. Not only is the meaning of \\(\\sigma\\) difficult to describe, we also don’t particular care about its value. Parameters like \\(\\sigma\\) in this context are nuisance or auxiliary parameters. We still estimate their posterior distributions, but we don’t really care what those posteriors look like.\n\\(\\mu\\) is not the average height of the men in the sample. We can calculate that directly. It is 175.606. No estimation required! Instead, \\(\\mu\\) is the average height of men in the population. Recall from the discussions in Chapter @ref(one-parameter) that the population is the universe of people/units/whatever about which we seek to draw conclusions. On some level, this seems simple. On a deeper level, it is very subtle. For example, if we are walking around Copenhagen, then the population we really care about, in order to answer our three questions, is the set of adult men which we might meet today. This is not the same as the population of adult men in the US in 2010. But is it close enough? Is it better than nothing? We want to assume that both men from nhanes (the data we have) and men we meet in Copenhagen today (the data we want to have) are drawn from the same population. Each case is a different and the details matter.\n\\(\\sigma\\) is an estimate for the standard deviation of the errors, i.e., variability in height after accounting for the mean.\n\nConsider:\n\\[\\text{outcome} = \\text{model} + \\text{what is not in the model}\\]\nIn this case, the outcome is the height of an individual male. This variable, also called the “response,” is what we are trying to understand and/or explain and/or predict. The model is our creation, a mixture of data and parameters, an attempt to capture the underlying structure in the world which generates the outcome.\n\nWhat is the difference between the outcome and the model? By definition, it is what is not in the model, all the blooming and buzzing complexity of the real world. The model will always be incomplete in that it won’t capture everything. Whatever the model misses is thrown into the error term."
  },
  {
    "objectID": "07-two-parameters.html#courage",
    "href": "07-two-parameters.html#courage",
    "title": "7  Two Parameters",
    "section": "\n7.3 Courage",
    "text": "7.3 Courage\n\n\n\n\n\n\nIn data science, we deal with words, math, and code, but the most important of these is code. We need Courage to create the model, to take the leap of faith that we can make our ideas real.\n\n7.3.0.1 stan_glm\nBayesian models are not hard to create in R. The rstanarm package provides the tools we need, most importantly the function stan_glm().\n\nlibrary(rstanarm)\n\nWarning: package 'Rcpp' was built under R version 4.2.1\n\n\nThe first argument in stan_glm() is data, which in our case is the filtered ch7 tibble which contains 50 observations. The only other mandatory argument is the formula that we want to use to build the model. In this case, since we have no predictor variables, our formula is height ~ 1.\n\nfit_obj <- stan_glm(data = ch7, \n                    formula = height ~ 1, \n                    family = gaussian, \n                    refresh = 0,\n                    seed = 9)\n\nDetails:\n\nThis may take time. Bayesian models, especially ones with large amounts of data, can take longer than we might like. Indeed, computational limits were the main reason why Bayesian approaches were — and, to some extent, still are — little used. When creating your own models, you will often want to use the cache = TRUE code chunk option. This saves the result of the model so that you don’t recalculate it every time you knit your document.\nThe data argument, like all such usage in R, is for the input data.\nIf you don’t set refresh = 0, the model will puke out many lines of confusing output. You can learn more about that output by reading the help page for stan_glm(). The output provides details on the fitting process as it runs as well as diagnostics about the final result. All of those details are beyond the scope of this book.\nYou should always assign the result of the call of stan_glm() to an object, as we do above. By convention, the name of that object will often include the word “fit” to indicate that it is a fitted model object.\nThere is a direct connection between the mathematical form of the model created under Justice and the code we use to fit the model under Courage. height ~ 1 is the code equivalent of \\(y_i = \\mu\\).\nThe default value for family is gaussian, so we did not need to include it in the call above. From the Justice section, the assumption that \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) is equivalent to using gaussian. If \\(\\epsilon_i\\) had a different distribution, we would need to use a different family. We saw an example of such a situation at the end of Chapter @ref(one-parameter) when we performed the urn analysis using stan_glm() and setting family = binomial.\nAlthough you can use the standalone function set.seed() in order to make your code reproducible, it is more convenient to use the seed argument within stan_glm(). Even though the fitting process, unavoidably, contains a random component, you will get the exact same answer if we set seed = 9 and rerun this code. It does not matter what number you use for the seed.\n\n\n7.3.0.1.1 Printed model\nThere are several ways to examine the fitted model. The simplest is to print it. Recall that just typing x at the prompt is the same as writing print(x).\n\nfit_obj\n\nstan_glm\n family:       gaussian [identity]\n formula:      height ~ 1\n observations: 50\n predictors:   1\n------\n            Median MAD_SD\n(Intercept) 175.6    1.2 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.5    0.9   \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nThe first line is telling us which model we used, in our case a stan_glm().\nThe second line tells us this model is using a Gaussian, or normal, distribution. We discussed this distribution in Section @ref(normal). We typically use this default unless we are working with a lefthand variable that is extremely non-normal, e.g., something which only takes two values like 0/1 or TRUE/FALSE. Since height is (very roughly) normally distributed, the Gaussian distribution is a good choice.\nThe third line gives us back the formula we provided. We are creating a model predicting height with a constant — which is just about the simplest model you can create. Formulas in R are constructed in two parts. First, on the left side of the tilde (the “~” symbol) is the “response” or “dependent” variable, the thing which we are trying to explain. Since this is a model about height, height goes on the lefthand side. Second, we have the “explanatory” or “independent” or “predictor” variables on the righthand side of the tilde. There will often be many such variables but in this, the simplest possible model, there is only one, a single constant. (The number 1 indicates that constant. It does not mean that we think that everyone is height 1.)\nThe fourth and fifth lines of the output tell us that we have 50 observations and that we only have one predictor (the constant). Again, the terminology is a bit confusing. What does it mean to suggest that \\(\\mu\\) is “constant?” It means that, although \\(\\mu\\)’s value is unknown, it is fixed. It does not change from person to person. The 1 in the formula corresponds to the parameter \\(\\mu\\) in our mathematical definition of the model.\nWe knew all this information before we fit the model. R records it in the fit_obj because we don’t want to forget what we did. The second half of the display gives a summary of the parameter values. We can look at just the second half with the detail argument:\n\nprint(fit_obj, detail = FALSE)\n\n            Median MAD_SD\n(Intercept) 175.6    1.2 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.5    0.9   \n\n\nWe see the output for the two parameters of the model: “(Intercept)” and “sigma”. This can be confusing! Recall that the thing we care most about is \\(\\mu\\), the average height in the population. If we had the Preceptor Table — with a row for every adult male in the population we care about and no missing data — \\(\\mu\\) would be trivial to calculate, and with no uncertainty. But only we know that we named that parameter \\(\\mu\\). All that R sees is the 1 in the formula. In most fields of statistics, this constant term is called the “intercept.” So, now we have three things — \\(\\mu\\) (from the math), 1 (from the code), and “(Intercept)” (from the output) — all of which refer to the exact same concept. This will not be the last time that terminology will be confusing.\nAt this point, stan_glm() — or rather the print() method for objects created with stan_glm() — has a problem. We have full posteriors for both \\(\\mu\\) and \\(\\sigma\\). But this is a simple printed summary. We can’t show the entire distribution. So, what are the best few numbers to provide? There is no right answer to this question! Here, the choice is to provide the “Median” of the posterior and the “MAD_SD”.\n\nAnytime you have a distribution, whether posterior probability or otherwise, the most important single number associated with it is some measure of its location. Where is the data? The two most common choices for this measure are the mean and median. We use the median here because posterior distributions can often be quite skewed, making the mean a less stable measure.\nThe second most important number for summarizing a distribution concerns its spread. How far is the data spread around its center? The most common measure used for this is the standard deviation. MAD SD, the scaled median absolute deviation, is another. If the variable has a normal distribution, then the standard deviation and the MAD SD will be very similar. But the MAD SD is much more robust to outliers, which is why it is used here. (Note that MAD SD is the same measure as what we have referred to as mad up till now. The measure is calculated with the mad() command in R. Terminology is confusing, as usual.)\n\nWe can also change the number of digits shown:\n\nprint(fit_obj, detail = FALSE, digits = 1)\n\n            Median MAD_SD\n(Intercept) 175.6    1.2 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 8.5    0.9   \n\n\n\nNow that we understand the meaning of Median and MAD_SD in the above display, we can interpret the actual numbers. The median of the intercept, 175.6, is the median of our posterior distribution for \\(\\mu\\), the average height of all men in the population. The median of sigma, 8.5, is the median of our posterior distribution for the true \\(\\sigma\\), which can be roughly understood as the variability in the height of men, once we account for our estimate of \\(\\mu\\).\n\n\n\nThe MAD SD for each parameter is a measure of the variability of our posterior distributions for that parameter. How spread out are they? Speaking roughly, 95% of the mass of a posterior probability distribution is located within +/- 2 MAD SDs from the median. For example, we would be about 95% confident that the true value of \\(\\mu\\) is somewhere between 173.2 and 178.\n\n7.3.0.1.2 Plotting the posterior distributions\nInstead of doing this math in our heads, we can display both posterior probability distributions. Pictures speak where math mumbles. Fortunately, getting draws from those posteriors is easy:\n\n\nfit_obj |> \n  as_tibble()\n\n# A tibble: 4,000 × 2\n   `(Intercept)` sigma\n           <dbl> <dbl>\n 1          175.  8.34\n 2          175.  8.16\n 3          174.  8.26\n 4          174.  8.25\n 5          173.  9.43\n 6          175.  9.24\n 7          174.  8.47\n 8          174.  8.26\n 9          177.  8.71\n10          173.  9.06\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThese 4,000 rows are draws from the estimated posteriors, each in its own column. These are like the vectors which result from calling functions like rnorm() or rbinom(). We can create the plot in a similar way:\n\n\nfit_obj |> \n  as_tibble() |> \n  ggplot(aes(x = `(Intercept)`)) +\n  \n# Recall that after_stat() allows us to work with \"stat variables\" that have\n# been calculated by ggplot, such as \"count\" and \"density\".\n  \n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Average Male Height\",\n         subtitle = \"American men average around 176 cm in height\",\n         x = expression(mu), \n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nAlthough it is possible to have variable names like “(Intercept)”, it is not recommended. Avoid weird names! When you are stuck with them, place them in backticks. Even better, rename them, as we do above.\nNote that the title includes the word “Posterior” and not the complete term “Posterior Probability Distribution.” This will be our practice going forward. “Posterior” means “Posterior Distribution” and, any posterior distribution in which the sum of the range is 1 is a “posterior probability distribution.” In most or our plots, “posterior” implies “posterior probability distribution.”\nAlways go back to first principles. There is some truth, an unknown number, a fact about the world. If we knew everything, if we had the Preceptor Table, inference would not be necessary. Algebra would suffice. Alas, in this imperfect world, we have no choice but to be data scientist. We are always uncertain. We summarize our knowledge of unknown numbers with posterior probability distributions, or “posteriors” for short.\n\n\n# The same plot as above, but for sigma instead of mu.\n\nfit_obj |> \n  as_tibble() |> \n  ggplot(aes(x = sigma)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))), \n                   bins = 100) +\n    labs(title = \"Posterior for Standard Deviation of Male Height\",\n         subtitle = \"The standard deviation of height is around 7 to 11 cm\",\n         x = expression(sigma),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nAgain, \\(\\sigma\\) is usually a nuisance parameter. We don’t really care what its value is, so we rarely plot it.\n\n\n\n\n\n7.3.1 Testing"
  },
  {
    "objectID": "07-two-parameters.html#temperance",
    "href": "07-two-parameters.html#temperance",
    "title": "7  Two Parameters",
    "section": "\n7.4 Temperance",
    "text": "7.4 Temperance\n\n\n\n\n\nWe have a model. What can we do with it? Let’s answer the four questions with which we started.\n\n7.4.1 Question 1\n\n\nWhat is the average height of men?\n\nIf we had the Preceptor Table, one with a row for every man alive, and with their actual height, we could calculate this number easily. Just take the average of those 3 billion or so rows! Alas, in our actual Preceptor Table, the vast majority of heights are missing. Question marks make simple algebra impossible. So, as with any unknown number, we need to estimate a posterior probability distribution. Objects created by stan_glm() make this easy to do.\n\nnewobs <- tibble(constant = 1)\n\npe <- posterior_epred(object = fit_obj,\n                      newdata = newobs) |> \n        as_tibble()\n\npe\n\n# A tibble: 4,000 × 1\n     `1`\n   <dbl>\n 1  175.\n 2  175.\n 3  174.\n 4  174.\n 5  173.\n 6  175.\n 7  174.\n 8  174.\n 9  177.\n10  173.\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nWe will use posterior_epred() many times. The two key arguments are object, the fitted model object returned by stan_glm(), and newdata, which is the tibble which contains the covariate values associated with the unit (or units) for which we want to make a forecast. (In this case, newdata can be any tibble because an intercept-only model does not make use of covariates. We don’t really need a variable named constant, but including it does no harm.) The epred in posterior_epred() stands for expected prediction. In other words, if we pick a random adult male what do we “expect” his height to be. We also call this the expected value.\n\nWe use as_tibble() to convert the matrix which is returned by posterior_epred(). We have a tibble with 1 column and 4,000 rows. The column, unhelpfully named 1, is 4,000 draws from the posterior probability distribution for the expected height of a random male. Recall from earlier chapters how a posterior probability distribution and the draws from a posterior probability distribution are, more or less, the same thing. Or, rather, a posterior probability distribution, its ownself, is hard to work with. Draws from that distribution, on the other hand, are easy to manipulate. We use draws to answer our questions.\nConverting these 4,000 draws into a posterior probability distribution is straightforward.\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Average Adult Male Height\",\n         subtitle = \"Note that the plot is very similar to the one created with the parameters\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\n\nThe rest of the Primer will be filled with graphics like this one. You will make dozens of them yourself. The fundamental structure for doing algebra is the real number. The fundamental structure for data science is the posterior probability distribution. You need to be able to create and interpret them.\n\n7.4.2 Question 2\n\nWhat is the probability that the next adult male we meet will be taller than 180 centimeters?\n\nThere are two fundamentally different kinds of unknowns which we care about: expected values (as in the previous question) and predicted values. With the former, we are not interested in any specific individual. The individual value is irrelevant. With predicted values, we care, not about the average, but about this specific person. With the former, we use posterior_epred(). With the latter, the relevant function is posterior_predict(). Both functions return draws from a posterior probability distribution, but the unknown number which underlies the posterior is very different.\nRecall the mathematics:\n\\[ y_i =  \\mu + \\epsilon_i \\]\nWith expected values or averages, we can ignore the \\(\\epsilon_i\\) term in this formula. The expected value of \\(\\epsilon_i\\) is zero since, by assumption, \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). However, we can’t ignore \\(\\epsilon_i\\) when predicting the height for a single individual.\n\nnewobs <- tibble(constant = 1)\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble()\n\npp\n\n# A tibble: 4,000 × 1\n     `1`\n   <dbl>\n 1  179.\n 2  164.\n 3  172.\n 4  172.\n 5  167.\n 6  184.\n 7  184.\n 8  159.\n 9  182.\n10  168.\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nAs before, it is straightforward to turn draws from the posterior probability distribution into a graphic:\n\npp |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Height of Random Male\",\n         subtitle = \"Uncertainty for a single individual is much greater than for the expected value\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n    theme_classic()\n\n\n\n\nNote:\n\nThe posterior for an individual is much wider than the posterior for the expected value.\nEyeballing, seems like there is a 1 out of 3 chance that the next man we meet, or any randomly chosen man, is taller than 180 cm.\nWe can calculate the exact probability by manipulating the tibble of draws directly.\n\n\nsum(pp$`1` > 180)/length(pp$`1`)\n\n[1] 0.309\n\n\nIf 30% or so of the draws from the posterior probability distribution are greater than 180 cm, then there is about a 30% chance that the next individual will be taller than 180 cm.\nAgain, the key conceptual difficulty is the population. The problem we actually have involves walking around London, or wherever, today. The data we have involve America in 2010. Those are not the same things! But they are not totally different. Knowing whether the data we have is “close enough” to the problem we want to solve is at the heart of Wisdom. Yet that was the decision we made at the start of the process, the decision to create a model in the first place. Now that we have created a model, we look to the virtue of Temperance for guidance in using that model. The data we have is never a perfect match for the world we face. We need to temper our confidence and act with humility. Our forecasts will never be as good as a naive use of the model might suggest. Reality will surprise us. We need to take the model’s claims with a family-sized portion of salt.\n\n\n7.4.3 Question 3\n\n\nWhat is the probability that, among the next 4 men we meet, the tallest is at least 10 cm taller than the shortest?\n\nBayesian models are beautiful because, via the magic of simulation, we can answer (almost!) any question. Because the question is about four random individuals, we need posterior_predict() to give us four sets of draws from four identical posterior probability distributions. Start with a new newobs:\n\nnewobs <- tibble(constant = rep(1, 4))\n\nnewobs\n\n# A tibble: 4 × 1\n  constant\n     <dbl>\n1        1\n2        1\n3        1\n4        1\n\n\nIf you need to predict X individuals, then you need a tibble with X rows, regardless of whether or not those rows are otherwise identical.\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble() |> \n        mutate_all(as.numeric)\n\nNote we need to add mutate_all(as.numeric) to the end of the pipe. This is caused by a bug — or at least an awkwardness — whereby the variable type provided by posterior_predict() is ppd rather than dbl. Using mutate_all(as.numeric) makes each column of type double. Avoid working with columns of type ppd. Doing so will only lead to heartache.\n\npp\n\n# A tibble: 4,000 × 4\n     `1`   `2`   `3`   `4`\n   <dbl> <dbl> <dbl> <dbl>\n 1  177.  170.  169.  164.\n 2  178.  167.  164.  185.\n 3  177.  180.  181.  170.\n 4  174.  169.  166.  169.\n 5  176.  191.  186.  165.\n 6  173.  164.  160.  183.\n 7  181.  199.  172.  157.\n 8  169.  181.  174.  174.\n 9  185.  180.  179.  170.\n10  177.  184.  164.  164.\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThe next step is to calculate the number of interest. We can not, directly, draw the height of the tallest or shortest out of 4 random men. However, having drawn 4 random men, we can calculate those numbers, and the difference between them.\n\n# First part of the code is the same as we did above.\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n        as_tibble() |> \n        mutate_all(as.numeric) |> \n  \n        # Second part of the code requires some trickery.\n  \n        rowwise() |> \n        mutate(tallest = max(c_across())) |> \n        mutate(shortest = min(c_across())) |> \n        mutate(diff = tallest - shortest) \n        \npp        \n\n# A tibble: 4,000 × 7\n# Rowwise: \n     `1`   `2`   `3`   `4` tallest shortest  diff\n   <dbl> <dbl> <dbl> <dbl>   <dbl>    <dbl> <dbl>\n 1  167.  169.  159.  177.    177.     159.  17.6\n 2  179.  187.  178.  170.    187.     170.  16.8\n 3  169.  183.  168.  172.    183.     168.  15.2\n 4  158.  179.  175.  161.    179.     158.  20.9\n 5  182.  182.  158.  170.    182.     158.  24.5\n 6  166.  153.  189.  165.    189.     153.  36.0\n 7  169.  172.  190.  175.    190.     169.  20.3\n 8  172.  164.  178.  197.    197.     164.  32.1\n 9  190.  184.  177.  171.    190.     171.  18.7\n10  161.  192.  165.  193.    193.     161.  32.4\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nThese steps serve as a template for much of the analysis we do later. It is often very hard to create a model directly of the thing we want to know. There is no easy way to create a model which estimates this height difference directly. It is easy, however, to create a model which allows for random draws.\nGive us enough random draws, and a tibble in which to store them, and we can estimate the world.\nOnce we have random draws from the posterior distribution we care about, graphing the posterior probability distribution is the same-old, same-old.\n\npp |> \n  ggplot(aes(x = diff)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Max Height Difference Among Four Men\",\n         subtitle = \"The expected value for this difference would be much more narrow\",\n         x = \"Height Difference in Centimeters\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(breaks = seq(0, 50, 10),\n                       labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format()) \n\n\n\n\nThere is about an 85% chance that, when meeting 4 random men, the tallest will be at least 10 cm taller than the shortest. Exact calculation:\n\nsum(pp$diff > 10) / length(pp$diff)\n\n[1] 0.847\n\n\n\n\n7.4.4 Question 4\n\nWhat is our posterior probability distribution of the height of the 3rd tallest man out of the next 100 we meet?\n\nThe same approach will work for almost any question.\n\nnewobs <- tibble(constant = rep(1, 100))\n\npp <- posterior_predict(object = fit_obj,\n                        newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) |> \n  rowwise() |> \n  mutate(third_tallest = sort(c_across(), \n                              decreasing = TRUE)[3])\n\nExplore the pp object. It has 101 columns: one hundred for the 100 individual heights and one column for the 3rd tallest among them. Having done the hard work, plotting is easy:\n\npp |> \n  ggplot(aes(x = third_tallest, y = after_stat(count / sum(count)))) +\n    geom_histogram(bins = 100) +\n    labs(title = \"Posterior for Height of 3rd Tallest Man from Next 100\",\n         subtitle = \"Should we have more or less certainty about behavior in the tails?\",\n         x = expression(mu),\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format()) +\n    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) \n\n\n\n\n\n7.4.5 Three Levels of Knowledge\nWhen answering questions as we have been, it can be easy to falsely believe that we are delivering the truth. This is not the case. In fact, there are three primary levels of knowledge which we need to understand in order to account for our uncertainty.\nThe three primary levels of knowledge possible knowledge in our scenario include: the Truth (the Preceptor Table), the DGM Posterior, and Our Posterior.\n\n7.4.5.1 The Truth\nIf we know the Truth (with a capital “T”), then we know the Preceptor Table. With that knowledge, we can directly answer our question precisely. We can calculate each individual’s height, and any summary measure we might be interested in, like the average height for different ages or countries.\nThis level of knowledge is possible only under an omniscient power, one who can see every outcome in every individual under every treatment. The Truth would show, for any given individual, their actions under control, their actions under treatment, and each little factor that impacted those decisions.\nThe Truth represents the highest level of knowledge one can have — with it, our questions merely require algebra. There is no need to estimate a treatment effect, or the different treatment effects for different groups of people. We would not need to predict at all — we would know.\n\n7.4.5.2 DGM posterior\nThe DGM posterior is the next level of knowledge, which lacks the omniscient quality of The Truth. This posterior is the posterior we would calculate if we had perfect knowledge of the data generating mechanism, meaning we have the correct model structure and exact parameter values. This is often falsely conflated with “our posterior,” which is subject to error in model structure and parameter value estimations.\nWhat we do with the DGM posterior is the same as our posterior — we estimate parameters based on data and predict the future with the latest and most relevant information possible. The difference is that, when we calculate posteriors for an unknown value in the DGM posterior, we expect those posteriors to be perfect.\n\n7.4.5.3 Our posterior\nUnfortunately, our posterior possesses even less certainty! In the real world, we don’t have perfect knowledge of the DGM: the model structure and the exact parameter values. What does this mean?\nWhen we go to our boss, we tell them that this is our best guess. It is an informed estimate based on the most relevant data possible. From that data, we have created a posterior for the average height of males.\nDoes this mean we are certain that the average height lies is the most probable outcome in our posterior? Of course not! As we would tell our boss, it would not be shocking to find out that the actual average height was less or more than our estimate.\nThis is because a lot of the assumptions we make during the process of building a model, the processes in Wisdom, are subject to error. Perhaps our data did not match the future as well as we had hoped. Ultimately, we try to account for our uncertainty in our estimates. Even with this safeguard, we aren’t surprised if we are a bit off."
  },
  {
    "objectID": "07-two-parameters.html#zero-one-outcomes",
    "href": "07-two-parameters.html#zero-one-outcomes",
    "title": "7  Two Parameters",
    "section": "\n7.5 0/1 Outcomes",
    "text": "7.5 0/1 Outcomes\nVariables with well-behaved, continuous ranges are the easiest to handle. We started with height because it was simple. Sadly, however, many variables are not like height. Consider gender, a variable in nhanes which takes on two possible values: “Male” and “Female”. In the same way that we would like to construct a model which explains or predicts height, we would like to build a model which explains or predicts gender. We want to answer questions like:\nWhat is the probability that a random person who is 180 cm tall is female?\nWisdom suggests we start by looking at the data. Because models use numbers, we need to create a new variable, female, which is 1 for Females and 0 for Males.\n\nch7_b <- nhanes |> \n  select(age, gender, height) |>\n  mutate(female = ifelse(gender == \"Female\", 1, 0)) |> \n  filter(age >= 18) |> \n  select(female, height) |> \n  drop_na()\n\nch7_b\n\n# A tibble: 7,424 × 2\n   female height\n    <dbl>  <dbl>\n 1      0   165.\n 2      0   165.\n 3      0   165.\n 4      1   168.\n 5      1   167.\n 6      1   167.\n 7      1   167.\n 8      0   170.\n 9      0   182.\n10      0   169.\n# … with 7,414 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nch7_b |> \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Men are taller than women\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(0, 1),\n                     labels = c(\"Male\", \"Female\"))\n\n\n\n\nWhy not just fit a linear model, as we did above? Consider:\n\nfit_gender_linear <- stan_glm(data = ch7_b,\n                              formula = female ~ height,\n                              family = gaussian,\n                              refresh = 0,\n                              seed = 82) \n\nRecall that the default value for family is gaussian, so we did not need to include it here. Initially, the fitted model seems OK.\n\nprint(fit_gender_linear, digits = 2)\n\nstan_glm\n family:       gaussian [identity]\n formula:      female ~ height\n observations: 7424\n predictors:   2\n------\n            Median MAD_SD\n(Intercept)  6.22   0.07 \nheight      -0.03   0.00 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.36   0.00  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nComparing two individuals who differ in height by 1 cm, we expect the taller individual to have a 3% lower probability of being female. That is not unreasonable. The problems show up at the extremes. Consider the fitted values across the range of our data.\n\nch7_b |> \n  ggplot(aes(x = height, y = female)) +\n  geom_jitter(height = 0.1, alpha = 0.05) +\n  geom_smooth(aes(y = fitted(fit_gender_linear)),\n              method = \"lm\",\n              formula = y ~ x,\n              se = FALSE) +\n  labs(title = \"Gender and Height\",\n       subtitle = \"Some fitted values are impossible\",\n       x = \"Height (cm)\",\n       y = NULL,\n       caption = \"Data from NHANES\") +\n  scale_y_continuous(breaks = c(-0.5, 0, 0.5, 1, 1.5),\n                     labels = c(\"-50%\", \"0% (Male)\", \n                                \"50%\", \"100% (Female)\",\n                                \"150%\"))\n\n\n\n\nUsing 1 for Female and 0 for Male allows us to interpret fitted values as the probability that a person is female or male. That is a handy and natural interpretation. The problem with a linear model arises when, as in this case, the model suggests values outside 0 to 1. Such values are, by definition, impossible. People who are 190 cm tall do not have a -25% chance of being female.\nJustice suggests a different functional form, one which restricts fitted values to the acceptable range. Look closely at the math:\n\\[  p(\\text{Female} = 1) = \\frac{\\text{exp}(\\beta_0 + \\beta_1 \\text{height})}{1 + \\text{exp}(\\beta_0 + \\beta_1 \\text{height})} \\]\nThis is an inverse logistic function, but don’t worry about the details. Mathematical formulas are never more than a Google search away. Instead, note how the range is restricted. Even if \\(\\beta_0 + \\beta_1 \\text{height}\\) is a very large number, the ratio is bound below 1. Similarly, no matter how negative \\(\\beta_0 + \\beta_1 \\text{height}\\) is, the ratio can never be smaller than 0. The model can not, ever, produce impossible values.\nWhenever you have two categories as the outcome, you should use family = binomial.\n\nCourage allows us to use the same tools for fitting this logistic regression as we did above in fitting linear models.\n\nfit_2 <- stan_glm(data = ch7_b,\n                  formula = female ~ height,\n                  family = binomial,\n                  refresh = 0,\n                  seed = 27)\n\n\nprint(fit_2, digits = 3)\n\nstan_glm\n family:       binomial [logit]\n formula:      female ~ height\n observations: 7424\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 43.389  0.999\nheight      -0.257  0.006\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nOne major difference between linear and logistic models is that parameters in the latter are much harder to interpret. What does it mean, substantively, that \\(\\beta_1\\) is -0.26? That is a topic for a more advanced course.\n\nFortunately, parameters are not what we care about. They are epiphenomenon, unicorns of our imagination. Instead, we want answers to our questions, for which Temperance — and the functions in rstanarm — is our guide. Recall our question:\nWhat is the probability that a random person who is 180 cm tall is female?\n\nnewobs <- tibble(height = 180)\n\npe <- posterior_epred(fit_2, newdata = newobs) |> \n  as_tibble()\n\npe |> \n  ggplot(aes(x = `1`)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for p(female | height = 180 cm)\",\n         subtitle = \"There is a 5-6% chance that a person this tall is female\",\n         x = \"Probability\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) \n\n\n\n\nThere is only about a 1 in 20 chance that a 180 centimeter tall person is female.\nNote that both the x and y axes are probabilities. Whenever we create a posterior probability distribution then, by definition, the y-axis is a probability. The x-axis is the unknown number we do not know. That unknown number can be anything — the weight of the average male, the height of 3rd tallest out of 100 men, the probability that a 180 cm tall person is female. A probability is just another number. The interpretation is the same as always.\nAnother major difference with logistic models is that posterior_epred() and posterior_predict() return different types of objects. posterior_epred() returns probabilities, as above. posterior_predict(), on the other hand, returns predictions, as its name suggests. In other words, it returns zeros and ones. Consider another question:\nIn a group of 100 people who are 180 centimeters tall, how many will be women?\n\nnewobs <- tibble(height = rep(180, 100))\n\npp <- posterior_predict(fit_2, newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) \n\npp[, 1:4]\n\n# A tibble: 4,000 × 4\n     `1`   `2`   `3`   `4`\n   <dbl> <dbl> <dbl> <dbl>\n 1     0     0     0     0\n 2     0     0     0     0\n 3     0     0     0     0\n 4     0     0     0     0\n 5     0     0     0     0\n 6     0     0     0     0\n 7     0     0     0     0\n 8     0     0     0     0\n 9     0     0     0     0\n10     1     0     0     0\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\nWe show just the first 4 columns for convenience. Each column is 4,000 draws from the posterior predictive distribution for the gender of a person who is 180 cm tall. (Since all 100 people have the same height, all the columns are draws from the same distribution.)\nWe can manipulate this object on a row-by-row basis.\n\npp <- posterior_predict(fit_2, newdata = newobs) |> \n  as_tibble() |> \n  mutate_all(as.numeric) |> \n  rowwise() |> \n  mutate(total = sum(c_across()))\n\npp[, c(\"1\", \"2\", \"100\", \"total\")]\n\n# A tibble: 4,000 × 4\n# Rowwise: \n     `1`   `2` `100` total\n   <dbl> <dbl> <dbl> <dbl>\n 1     0     0     0     5\n 2     0     0     0     3\n 3     0     0     0     7\n 4     0     0     0     8\n 5     0     0     0     6\n 6     0     0     0     5\n 7     0     0     0     5\n 8     0     0     0     2\n 9     0     0     0     6\n10     0     0     0     4\n# … with 3,990 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\ntotal is the number of women in each row. Manipulating draws on a row-by-row basis is very common.\n\n\npp |> \n  ggplot(aes(x = total)) +\n    geom_histogram(aes(y = after_stat(count/sum(count))),\n                   bins = 100) +\n    labs(title = \"Posterior for Number of Women among 100 People 180 cm Tall\",\n         subtitle = \"Consistent with probability estimate above\",\n         x = \"Number of Women\",\n         y = \"Probability\",\n         caption = \"Data source: NHANES\") + \n    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +\n    scale_y_continuous(labels = scales::percent_format()) \n\n\n\n\nThat 5 or 6 women is the most likely number is very consistent with the answer to the first question. There we found that a random person who is 180 cm tall has a 5% or 6% chance of being female. So, with 100 such people, 5 or 6 seems a reasonable total. But the expected value from posterior_epred(), although it does provide a sense of where the center of the predictive distribution will be, does not tell us much about the range of possible outcomes. For that, we need posterior_predict()."
  },
  {
    "objectID": "07-two-parameters.html#summary",
    "href": "07-two-parameters.html#summary",
    "title": "7  Two Parameters",
    "section": "\n7.6 Summary",
    "text": "7.6 Summary\n\nThe next five chapters will follow the same process we have just completed here. We start with a decision we have to make. With luck, we will have some data to guide us. (Without data, even the best data scientist will struggle to make progress.) Wisdom asks us: “Is the data we have close enough to the decision we face to make using that data helpful?” Often times, the answer is “No.”\nOnce we start to build the model, Justice will guide us. Is the model descriptive or causal? What is the mathematical relationship between the dependent variable we are trying to explain and the independent variables we can use to explain it? What assumptions are we making about distribution of the error term?\nHaving set up the model framework, we need Courage to implement the model in code. Without code, all the math in the world is useless. Once we have created the model, we need to understand it. What are the posterior distributions of the unknown parameters? Do they seem sensible? How should we interpret them?\nTemperance guides the final step. With a model, we can finally get back to the decision which motivated the exercise in the first place. We can use the model to make statements about the world, both to confirm that the model is consistent with the world and to use the model to make predictions about numbers which we do not know.\nLet’s practice this process another dozen or so times."
  }
]